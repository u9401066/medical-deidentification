# Ollama æœ¬åœ° LLM æ•´åˆèªªæ˜

## ğŸ“¦ ç‚ºä»€éº¼ä½¿ç”¨ Ollamaï¼Ÿ

OpenAI API åœ¨æ¸¬è©¦ä¸­å›æ‡‰æ¥µåº¦ç·©æ…¢ï¼ˆ>5åˆ†é˜/è«‹æ±‚ï¼‰ï¼Œå› æ­¤æ•´åˆæœ¬åœ° LLM æ–¹æ¡ˆï¼š

**å„ªå‹¢**:
- ğŸš€ **å¿«é€Ÿ**: æœ¬åœ°æ¨ç†ï¼Œç„¡ç¶²è·¯å»¶é²
- ğŸ’° **å…è²»**: é–‹æºæ¨¡å‹ï¼Œç„¡ API è²»ç”¨
- ğŸ”’ **éš±ç§**: é†«ç™‚æ•æ„Ÿæ•¸æ“šä¸ä¸Šå‚³é›²ç«¯
- ğŸ¯ **æº–ç¢º**: Qwen 2.5, Llama 3.1 ç­‰é«˜å“è³ªæ¨¡å‹

---

## ğŸ› ï¸ å®‰è£ Ollama

### 1. ä¸‹è¼‰ä¸¦å®‰è£
è¨ªå• [https://ollama.ai/download](https://ollama.ai/download) ä¸‹è¼‰é©åˆä½ ç³»çµ±çš„ç‰ˆæœ¬ã€‚

### 2. å•Ÿå‹• Ollama æœå‹™
```bash
ollama serve
```
ï¼ˆåœ¨ Windows ä¸Šï¼Œå®‰è£å¾Œæœƒè‡ªå‹•å•Ÿå‹•æœå‹™ï¼‰

### 3. ä¸‹è¼‰æ¨è–¦æ¨¡å‹

```bash
# Qwen 2.5 7B (æ¨è–¦) - ä¸­è‹±æ–‡å„ªç§€ï¼ŒPHI è­˜åˆ¥æº–ç¢º
ollama pull qwen2.5:7b

# Llama 3.1 8B - é€šç”¨æ€§å¼·
ollama pull llama3.1:8b

# Mistral 7B - é€Ÿåº¦å¿«
ollama pull mistral:7b
```

### 4. é©—è­‰å®‰è£
```bash
ollama list
```
æ‡‰è©²èƒ½çœ‹åˆ°ä½ ä¸‹è¼‰çš„æ¨¡å‹ã€‚

---

## ğŸ’» ä½¿ç”¨æ–¹å¼

### æ–¹æ³• 1: ä½¿ç”¨é è¨­é…ç½®

```python
from medical_deidentification.infrastructure.llm.config import LLMPresets
from medical_deidentification.infrastructure.llm.factory import create_llm

# ä½¿ç”¨ Qwen 2.5 7B
config = LLMPresets.local_qwen()
llm = create_llm(config)

response = llm.invoke("ä½ çš„ prompt")
```

### æ–¹æ³• 2: è‡ªè¨‚é…ç½®

```python
from medical_deidentification.infrastructure.llm.config import LLMConfig
from medical_deidentification.infrastructure.llm.factory import create_llm

config = LLMConfig(
    provider="ollama",
    model_name="qwen2.5:7b",
    temperature=0.0,
    max_tokens=2048,
)

llm = create_llm(config)
```

### æ–¹æ³• 3: åœ¨ PHI è­˜åˆ¥ä¸­ä½¿ç”¨

```python
from medical_deidentification.infrastructure.rag.phi_identification_chain import (
    PHIIdentificationConfig,
    PHIIdentificationChain
)
from medical_deidentification.infrastructure.llm.config import LLMConfig

# é…ç½®ä½¿ç”¨ Ollama
llm_config = LLMConfig(
    provider="ollama",
    model_name="qwen2.5:7b",
    temperature=0.0,
)

phi_config = PHIIdentificationConfig(
    llm_config=llm_config,
    retrieve_regulation_context=False,
    use_structured_output=False,  # âš ï¸ Ollama å¯èƒ½ä¸æ”¯æ´ structured output
)

phi_chain = PHIIdentificationChain(regulation_chain, phi_config)
```

---

## ğŸ§ª æ¸¬è©¦æ•´åˆ

åŸ·è¡Œæ¸¬è©¦è…³æœ¬ï¼š

```bash
python test_ollama_llm.py
```

æ¸¬è©¦åŒ…å«ï¼š
1. âœ… åŸºæœ¬é€£æ¥æ¸¬è©¦
2. âœ… PHI è­˜åˆ¥æ¸¬è©¦
3. âœ… æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦

---

## ğŸ“Š æ”¯æ´çš„æ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç‰¹é» | æ¨è–¦ç”¨é€” |
|------|------|------|---------|
| `qwen2.5:7b` | 7B | ä¸­è‹±æ–‡å„ªç§€ | â­ PHI è­˜åˆ¥ |
| `qwen2.5:14b` | 14B | æ›´é«˜æº–ç¢ºç‡ | è¤‡é›œé†«ç™‚æ–‡æœ¬ |
| `llama3.1:8b` | 8B | é€šç”¨æ€§å¼· | é€šç”¨ NLP |
| `mistral:7b` | 7B | é€Ÿåº¦å¿« | å¿«é€Ÿæ¨ç† |
| `gemma2:9b` | 9B | Google å‡ºå“ | å¯¦é©—æ€§ |

---

## âš™ï¸ é…ç½®é¸é …

```python
LLMConfig(
    provider="ollama",
    model_name="qwen2.5:7b",       # æ¨¡å‹åç¨±
    temperature=0.0,                # 0.0 = ç¢ºå®šæ€§, 1.0 = å‰µé€ æ€§
    max_tokens=2048,                # æœ€å¤§è¼¸å‡ºé•·åº¦
    timeout=30.0,                   # è«‹æ±‚è¶…æ™‚ï¼ˆç§’ï¼‰
    api_base="http://localhost:11434",  # Ollama API åœ°å€ï¼ˆå¯é¸ï¼‰
)
```

---

## âš ï¸ é™åˆ¶èˆ‡æ³¨æ„äº‹é …

### Structured Output æ”¯æ´
Ollama ç›®å‰**å¯èƒ½ä¸å®Œå…¨æ”¯æ´** LangChain çš„ `with_structured_output()`ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
1. ä½¿ç”¨ `use_structured_output=False` é…ç½®
2. æ”¹ç”¨ JSON mode æˆ–æ‰‹å‹•è§£æ
3. åœ¨ prompt ä¸­æ˜ç¢ºè¦æ±‚ JSON æ ¼å¼è¼¸å‡º

### è¨˜æ†¶é«”éœ€æ±‚
- 7B æ¨¡å‹: ~4-6 GB RAM
- 14B æ¨¡å‹: ~8-10 GB RAM
- 32B æ¨¡å‹: ~20+ GB RAM

### GPU åŠ é€Ÿ
æœ‰ GPU çš„è©±æœƒè‡ªå‹•ä½¿ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¯æå‡ 10-50 å€ã€‚

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: "Connection refused"
**åŸå› **: Ollama æœå‹™æœªå•Ÿå‹•  
**è§£æ±º**: åŸ·è¡Œ `ollama serve` æˆ–æª¢æŸ¥æœå‹™ç‹€æ…‹

### å•é¡Œ 2: "Model not found"
**åŸå› **: æ¨¡å‹æœªä¸‹è¼‰  
**è§£æ±º**: `ollama pull qwen2.5:7b`

### å•é¡Œ 3: æ¨ç†å¤ªæ…¢
**åŸå› **: 
- CPU æ¨ç†è¼ƒæ…¢ï¼ˆæ­£å¸¸ï¼‰
- æ¨¡å‹å¤ªå¤§

**è§£æ±º**:
- ä½¿ç”¨è¼ƒå°æ¨¡å‹ (7B è€Œé 14B)
- ä½¿ç”¨ GPU
- æ¸›å°‘ `max_tokens`

### å•é¡Œ 4: "langchain_community not installed"
**åŸå› **: ç¼ºå°‘ä¾è³´  
**è§£æ±º**: `pip install langchain-community`

---

## ğŸ“ˆ æ•ˆèƒ½æ¯”è¼ƒ

åŸºæ–¼å…§éƒ¨æ¸¬è©¦ï¼ˆéæ­£å¼ï¼‰:

| Provider | æ¨¡å‹ | å›æ‡‰æ™‚é–“ | æˆæœ¬ | éš±ç§ |
|----------|------|---------|------|------|
| OpenAI | gpt-4o-mini | 3-5ç§’* | $$ | âš ï¸ é›²ç«¯ |
| OpenAI | gpt-4o-mini | >5åˆ†é˜** | $$ | âš ï¸ é›²ç«¯ |
| Ollama | qwen2.5:7b | 2-10ç§’ | å…è²» | âœ… æœ¬åœ° |
| Ollama | llama3.1:8b | 2-10ç§’ | å…è²» | âœ… æœ¬åœ° |

\* æ­£å¸¸æƒ…æ³  
\*\* æ¸¬è©¦æ™‚é‡åˆ°çš„ç•°å¸¸ç·©æ…¢

---

## ğŸ¯ æœ€ä½³å¯¦è¸

### PHI è­˜åˆ¥æ¨è–¦é…ç½®

```python
# å¿«é€Ÿæ¸¬è©¦ç”¨
config_fast = LLMConfig(
    provider="ollama",
    model_name="mistral:7b",
    temperature=0.0,
    max_tokens=1024,
)

# æº–ç¢ºåº¦å„ªå…ˆ
config_accurate = LLMConfig(
    provider="ollama",
    model_name="qwen2.5:14b",
    temperature=0.0,
    max_tokens=2048,
)

# å¹³è¡¡æ€§ï¼ˆæ¨è–¦ï¼‰
config_balanced = LLMPresets.local_qwen()  # qwen2.5:7b
```

---

## ğŸ“š åƒè€ƒè³‡æº

- [Ollama å®˜ç¶²](https://ollama.ai/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [æ”¯æ´çš„æ¨¡å‹åˆ—è¡¨](https://ollama.ai/library)
- [LangChain Ollama æ–‡æª”](https://python.langchain.com/docs/integrations/chat/ollama)

---

## ğŸ†˜ éœ€è¦å¹«åŠ©ï¼Ÿ

1. æŸ¥çœ‹ `test_ollama_llm.py` æ¸¬è©¦è…³æœ¬
2. åŸ·è¡Œ `ollama --help` æŸ¥çœ‹å‘½ä»¤
3. è¨ªå• Ollama Discord ç¤¾ç¾¤

---

**Last Updated**: 2024-11-22  
**Version**: 1.0.0
