"""
Batch PHI Identification Processor
æ‰¹æ¬¡ PHI è­˜åˆ¥è™•ç†å™¨

Processes multiple documents/rows from Excel files for PHI identification.
è™•ç†ä¾†è‡ª Excel æ–‡ä»¶çš„å¤šå€‹æ–‡æª”/è¡Œä»¥é€²è¡Œ PHI è­˜åˆ¥ã€‚
"""

import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime

import pandas as pd
from loguru import logger

from ...domain import PHIEntity
from ...infrastructure.rag.phi_identification_chain import PHIIdentificationChain
from .output_manager import OutputManager, get_default_output_manager
from .report_generator import ReportGenerator


@dataclass
class BatchProcessingConfig:
    """æ‰¹æ¬¡è™•ç†é…ç½®"""
    max_rows: Optional[int] = None  # é™åˆ¶è™•ç†è¡Œæ•¸ï¼ˆNone=å…¨éƒ¨ï¼‰
    language: str = "zh-TW"  # é è¨­èªè¨€
    skip_empty_rows: bool = True  # è·³éç©ºè¡Œ
    combine_columns: bool = True  # åˆä½µæ‰€æœ‰æ¬„ä½ç‚ºå–®ä¸€æ–‡æœ¬
    log_progress_interval: int = 10  # æ¯Nè¡Œè¨˜éŒ„ä¸€æ¬¡é€²åº¦


@dataclass
class RowProcessingResult:
    """å–®è¡Œè™•ç†çµæœ"""
    row_number: int
    case_id: str
    text_length: int
    entities: List[PHIEntity] = field(default_factory=list)
    processing_time: float = 0.0
    success: bool = True
    error_message: Optional[str] = None


@dataclass
class BatchProcessingResult:
    """æ‰¹æ¬¡è™•ç†çµæœ"""
    file_name: str
    total_rows: int
    processed_rows: int
    total_entities: int
    total_time: float
    average_time_per_row: float
    row_results: List[RowProcessingResult] = field(default_factory=list)
    started_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    def mark_completed(self) -> None:
        """æ¨™è¨˜ç‚ºå®Œæˆ"""
        self.completed_at = datetime.now()
    
    def get_phi_type_distribution(self) -> Dict[str, int]:
        """ç²å–PHIé¡å‹åˆ†å¸ƒ"""
        distribution: Dict[str, int] = {}
        for row_result in self.row_results:
            for entity in row_result.entities:
                phi_type = entity.type.value
                distribution[phi_type] = distribution.get(phi_type, 0) + 1
        return distribution
    
    def get_confidence_statistics(self) -> Dict[str, float]:
        """ç²å–ä¿¡å¿ƒåº¦çµ±è¨ˆ"""
        all_confidences = [
            entity.confidence
            for row_result in self.row_results
            for entity in row_result.entities
        ]
        
        if not all_confidences:
            return {"mean": 0.0, "min": 0.0, "max": 0.0}
        
        return {
            "mean": sum(all_confidences) / len(all_confidences),
            "min": min(all_confidences),
            "max": max(all_confidences)
        }
    
    def to_dataframe(self) -> pd.DataFrame:
        """è½‰æ›ç‚ºDataFrameä»¥ä¾¿è¼¸å‡º"""
        records = []
        for row_result in self.row_results:
            for entity in row_result.entities:
                records.append({
                    "file": self.file_name,
                    "row": row_result.row_number,
                    "case_id": row_result.case_id,
                    "phi_type": entity.type.value,
                    "phi_text": entity.text,
                    "start_pos": entity.start_pos,
                    "end_pos": entity.end_pos,
                    "confidence": entity.confidence,
                    "reason": entity.reason,
                    "regulation_source": entity.regulation_source or ""
                })
        return pd.DataFrame(records)


class BatchPHIProcessor:
    """
    æ‰¹æ¬¡ PHI è­˜åˆ¥è™•ç†å™¨
    
    ç”¨æ–¼è™•ç† Excel æ–‡ä»¶ä¸­çš„å¤šè¡Œæ•¸æ“šï¼Œè­˜åˆ¥æ¯è¡Œä¸­çš„ PHI å¯¦é«”ã€‚
    
    Examples:
        >>> processor = BatchPHIProcessor(phi_chain)
        >>> result = processor.process_excel_file("data.xlsx")
        >>> result.to_dataframe().to_excel("results.xlsx")
    """
    
    def __init__(
        self,
        phi_chain: PHIIdentificationChain,
        config: Optional[BatchProcessingConfig] = None
    ):
        """
        åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨
        
        Args:
            phi_chain: PHI identification chain
            config: æ‰¹æ¬¡è™•ç†é…ç½®
        """
        self.phi_chain = phi_chain
        self.config = config or BatchProcessingConfig()
    
    def process_excel_file(
        self,
        file_path: str,
        case_id_column: Optional[str] = None
    ) -> BatchProcessingResult:
        """
        è™•ç† Excel æ–‡ä»¶
        
        Args:
            file_path: Excel æ–‡ä»¶è·¯å¾‘
            case_id_column: Case ID æ¬„ä½åç¨±ï¼ˆNone=ä½¿ç”¨ç¬¬ä¸€æ¬„ï¼‰
            
        Returns:
            æ‰¹æ¬¡è™•ç†çµæœ
        """
        logger.info(f"{'='*80}")
        logger.info(f"Processing Excel file: {file_path}")
        logger.info(f"{'='*80}")
        
        # è¼‰å…¥æ•¸æ“š
        df = self._load_excel(file_path)
        
        # é™åˆ¶è¡Œæ•¸
        if self.config.max_rows:
            df = df.head(self.config.max_rows)
            logger.info(f"Limited to first {self.config.max_rows} rows")
        
        # åˆå§‹åŒ–çµæœ
        result = BatchProcessingResult(
            file_name=Path(file_path).name,
            total_rows=len(df),
            processed_rows=0,
            total_entities=0,
            total_time=0.0,
            average_time_per_row=0.0
        )
        
        start_time = time.time()
        
        # è™•ç†æ¯ä¸€è¡Œ
        for idx, row in df.iterrows():
            row_number = idx + 1 if isinstance(idx, int) else int(str(idx)) + 1
            
            # ç²å– Case ID
            if case_id_column and case_id_column in df.columns:
                case_id = str(row[case_id_column])
            else:
                case_id = str(row.iloc[0]) if len(row) > 0 else f"Row-{row_number}"
            
            # è™•ç†è¡Œ
            row_result = self._process_row(row, df, row_number, case_id)
            result.row_results.append(row_result)
            
            if row_result.success:
                result.processed_rows += 1
                result.total_entities += len(row_result.entities)
            
            # è¨˜éŒ„é€²åº¦
            if row_number % self.config.log_progress_interval == 0:
                logger.info(
                    f"Progress: {row_number}/{len(df)} rows processed, "
                    f"{result.total_entities} PHI entities found"
                )
        
        # å®Œæˆçµ±è¨ˆ
        result.total_time = time.time() - start_time
        result.average_time_per_row = (
            result.total_time / result.processed_rows 
            if result.processed_rows > 0 else 0.0
        )
        result.mark_completed()
        
        self._log_summary(result)
        return result
    
    def process_multiple_files(
        self,
        file_paths: List[str],
        case_id_column: Optional[str] = None
    ) -> List[BatchProcessingResult]:
        """
        è™•ç†å¤šå€‹ Excel æ–‡ä»¶
        
        Args:
            file_paths: Excel æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
            case_id_column: Case ID æ¬„ä½åç¨±
            
        Returns:
            æ‰¹æ¬¡è™•ç†çµæœåˆ—è¡¨
        """
        results = []
        
        for file_path in file_paths:
            if not Path(file_path).exists():
                logger.warning(f"File not found: {file_path}")
                continue
            
            result = self.process_excel_file(file_path, case_id_column)
            results.append(result)
        
        self._log_overall_summary(results)
        return results
    
    def _load_excel(self, file_path: str) -> pd.DataFrame:
        """è¼‰å…¥ Excel æ–‡ä»¶"""
        logger.info(f"Loading Excel file: {file_path}")
        df = pd.read_excel(file_path)
        logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
        return df
    
    def _process_row(
        self,
        row: pd.Series,
        df: pd.DataFrame,
        row_number: int,
        case_id: str
    ) -> RowProcessingResult:
        """è™•ç†å–®ä¸€è¡Œ"""
        start_time = time.time()
        
        try:
            # åˆä½µæ¬„ä½æ–‡æœ¬
            if self.config.combine_columns:
                row_text = self._combine_row_text(row, df)
            else:
                row_text = " ".join(str(v) for v in row if pd.notna(v))
            
            # è·³éç©ºè¡Œ
            if self.config.skip_empty_rows and not row_text.strip():
                logger.debug(f"Row {row_number} is empty, skipping")
                return RowProcessingResult(
                    row_number=row_number,
                    case_id=case_id,
                    text_length=0,
                    processing_time=time.time() - start_time
                )
            
            # è­˜åˆ¥ PHI
            result = self.phi_chain.identify_phi(
                text=row_text,
                language=self.config.language,
                return_entities=True
            )
            
            entities = result.get("entities", [])
            processing_time = time.time() - start_time
            
            if entities:
                logger.debug(
                    f"Row {row_number} ({case_id}): Found {len(entities)} PHI entities "
                    f"in {processing_time:.2f}s"
                )
            
            return RowProcessingResult(
                row_number=row_number,
                case_id=case_id,
                text_length=len(row_text),
                entities=entities,
                processing_time=processing_time,
                success=True
            )
        
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Error processing row {row_number}: {e}")
            
            return RowProcessingResult(
                row_number=row_number,
                case_id=case_id,
                text_length=0,
                processing_time=processing_time,
                success=False,
                error_message=str(e)
            )
    
    def _combine_row_text(self, row: pd.Series, df: pd.DataFrame) -> str:
        """åˆä½µè¡Œçš„æ‰€æœ‰æ¬„ä½ç‚ºå–®ä¸€æ–‡æœ¬ï¼Œä¿ç•™æ¬„ä½åç¨±"""
        parts = []
        for col in df.columns:
            value = row[col]
            if pd.notna(value) and str(value).strip():
                parts.append(f"[{col}] {str(value)}")
        return "\n\n".join(parts)
    
    def _log_summary(self, result: BatchProcessingResult) -> None:
        """è¨˜éŒ„è™•ç†æ‘˜è¦"""
        logger.info(f"\n{'='*80}")
        logger.info("Batch Processing Complete")
        logger.info(f"{'='*80}")
        logger.info(f"File: {result.file_name}")
        logger.info(f"Total rows: {result.total_rows}")
        logger.info(f"Processed rows: {result.processed_rows}")
        logger.info(f"Total PHI entities: {result.total_entities}")
        logger.info(f"Total time: {result.total_time:.2f}s")
        logger.info(f"Average time per row: {result.average_time_per_row:.2f}s")
        
        # PHI é¡å‹åˆ†å¸ƒ
        distribution = result.get_phi_type_distribution()
        if distribution:
            logger.info("\nPHI Type Distribution:")
            for phi_type, count in sorted(distribution.items(), key=lambda x: -x[1]):
                logger.info(f"  - {phi_type}: {count}")
        
        # ä¿¡å¿ƒåº¦çµ±è¨ˆ
        confidence_stats = result.get_confidence_statistics()
        if confidence_stats["mean"] > 0:
            logger.info("\nConfidence Statistics:")
            logger.info(f"  - Mean: {confidence_stats['mean']:.2%}")
            logger.info(f"  - Min: {confidence_stats['min']:.2%}")
            logger.info(f"  - Max: {confidence_stats['max']:.2%}")
        
        logger.info(f"{'='*80}\n")
    
    def _log_overall_summary(self, results: List[BatchProcessingResult]) -> None:
        """è¨˜éŒ„å¤šæ–‡ä»¶è™•ç†ç¸½æ‘˜è¦"""
        total_rows = sum(r.total_rows for r in results)
        total_entities = sum(r.total_entities for r in results)
        total_time = sum(r.total_time for r in results)
        
        logger.info(f"\n{'='*80}")
        logger.info("Overall Summary")
        logger.info(f"{'='*80}")
        logger.info(f"Total files: {len(results)}")
        logger.info(f"Total rows: {total_rows}")
        logger.info(f"Total PHI entities: {total_entities}")
        logger.info(f"Total time: {total_time:.2f}s ({total_time/60:.1f} minutes)")
        logger.info(f"Average time per row: {total_time/total_rows:.2f}s")
        logger.info(f"{'='*80}\n")


def save_batch_results(
    results: List[BatchProcessingResult],
    output_file: Optional[str] = None,
    output_manager: Optional[OutputManager] = None,
    generate_report: bool = True
) -> Dict[str, Path]:
    """
    å„²å­˜æ‰¹æ¬¡è™•ç†çµæœåˆ° Excel å’Œç”Ÿæˆå ±å‘Š
    Save batch processing results to Excel and generate report
    
    Args:
        results: æ‰¹æ¬¡è™•ç†çµæœåˆ—è¡¨ | List of batch processing results
        output_file: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼Œä½¿ç”¨ output_manager è‡ªå‹•ç”Ÿæˆï¼‰
                     Output file path (optional, auto-generated by output_manager)
        output_manager: OutputManager å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œä½¿ç”¨é è¨­å€¼ï¼‰
                       OutputManager instance (optional, uses default)
        generate_report: æ˜¯å¦ç”Ÿæˆ JSON å ±å‘Š | Whether to generate JSON report
    
    Returns:
        Dictionary with paths to saved files {"result": Path, "report": Path}
    """
    # Get output manager
    om = output_manager or get_default_output_manager()
    
    # Generate output file path if not provided
    if output_file is None:
        # Use first result's filename as prefix
        if results:
            file_stem = Path(results[0].file_name).stem
            result_path = om.get_result_path(f"batch_{file_stem}", "xlsx")
        else:
            result_path = om.get_result_path("batch_result", "xlsx")
    else:
        result_path = Path(output_file)
        result_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åˆä½µæ‰€æœ‰çµæœ
    all_dfs = [result.to_dataframe() for result in results]
    combined_df = pd.concat(all_dfs, ignore_index=True)
    
    # å„²å­˜ Excel
    combined_df.to_excel(result_path, index=False)
    logger.success(f"âœ“ Batch results saved: {result_path}")
    
    # é¡¯ç¤ºé è¦½
    logger.info("\nğŸ“Š Sample results (first 10 rows):")
    print(combined_df.head(10).to_string())
    
    saved_paths = {"result": result_path}
    
    # ç”Ÿæˆå ±å‘Š
    if generate_report and results:
        generator = ReportGenerator(output_manager=om)
        
        # Generate report for each result
        for result in results:
            try:
                file_stem = Path(result.file_name).stem
                report_path = generator.save_batch_report(
                    result,
                    filename_prefix=f"report_{file_stem}",
                    include_details=True
                )
                logger.success(f"âœ“ Report generated: {report_path}")
                saved_paths["report"] = report_path
            except Exception as e:
                logger.warning(f"âš  Failed to generate report: {e}")
    
    return saved_paths
