#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniMind PHI Detection Evaluation | MiniMind PHI æª¢æ¸¬è©•ä¼°

ä½¿ç”¨å¸¶æ¨™è¨˜çš„æ¸¬è©¦è³‡æ–™è©•ä¼° MiniMind çš„ PHI æª¢æ¸¬æ•ˆèƒ½
Evaluate MiniMind's PHI detection performance using tagged test data

Metrics:
- Precision: æª¢æ¸¬æ­£ç¢ºç‡ï¼ˆé¿å…éåº¦æª¢æ¸¬ï¼‰
- Recall: å¬å›ç‡ï¼ˆé¿å…æ¼æª¢ï¼‰
- F1 Score: ç¶œåˆè©•åˆ†
- Over-detection: éåº¦æª¢æ¸¬åˆ†æ
"""

import re
import sys
import time
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
from dataclasses import dataclass, field
from collections import defaultdict

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger

# Configure logging
logger.remove()
logger.add(sys.stderr, level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>")


@dataclass
class PHIInstance:
    """å–®å€‹ PHI å¯¦ä¾‹"""
    phi_type: str
    content: str
    phi_id: Optional[str] = None
    start: int = 0
    end: int = 0
    
    def __hash__(self):
        return hash((self.phi_type, self.content.strip()))
    
    def __eq__(self, other):
        if not isinstance(other, PHIInstance):
            return False
        return self.phi_type == other.phi_type and self.content.strip() == other.content.strip()


@dataclass
class EvaluationResult:
    """è©•ä¼°çµæœ"""
    case_id: str
    ground_truth: List[PHIInstance]
    detected: List[PHIInstance]
    
    # Metrics
    true_positives: List[PHIInstance] = field(default_factory=list)
    false_positives: List[PHIInstance] = field(default_factory=list)  # éåº¦æª¢æ¸¬
    false_negatives: List[PHIInstance] = field(default_factory=list)  # æ¼æª¢
    
    processing_time: float = 0.0
    
    @property
    def precision(self) -> float:
        tp = len(self.true_positives)
        fp = len(self.false_positives)
        return tp / (tp + fp) if (tp + fp) > 0 else 0.0
    
    @property
    def recall(self) -> float:
        tp = len(self.true_positives)
        fn = len(self.false_negatives)
        return tp / (tp + fn) if (tp + fn) > 0 else 0.0
    
    @property
    def f1_score(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


def parse_phi_tags(text: str) -> List[PHIInstance]:
    """å¾å¸¶æ¨™è¨˜çš„æ–‡æœ¬ä¸­è§£æ PHI æ¨™æº–ç­”æ¡ˆ"""
    pattern = r'ã€PHI:(\w+):?(\w*)ã€‘([^ã€]+?)ã€/PHIã€‘'
    instances = []
    
    for match in re.finditer(pattern, text):
        phi_type = match.group(1)
        phi_id = match.group(2) if match.group(2) else None
        content = match.group(3).strip()
        
        instances.append(PHIInstance(
            phi_type=phi_type,
            phi_id=phi_id,
            content=content,
            start=match.start(),
            end=match.end()
        ))
    
    return instances


def remove_phi_tags(text: str) -> str:
    """ç§»é™¤ PHI æ¨™è¨˜ï¼Œä¿ç•™å…§å®¹ï¼ˆç”¨æ–¼é€çµ¦ LLM åˆ†æï¼‰"""
    pattern = r'ã€PHI:\w+:?\w*ã€‘([^ã€]+?)ã€/PHIã€‘'
    return re.sub(pattern, r'\1', text)


def normalize_phi_type(phi_type: str) -> str:
    """æ¨™æº–åŒ– PHI é¡å‹åç¨±"""
    type_mapping = {
        # å§“åç›¸é—œ
        'NAME': 'NAME',
        'PATIENT_NAME': 'NAME',
        'DOCTOR_NAME': 'NAME',
        'PHYSICIAN': 'NAME',
        
        # å¹´é½¡ç›¸é—œ
        'AGE': 'AGE',
        'AGE_OVER_89': 'AGE',
        
        # æ—¥æœŸç›¸é—œ
        'DATE': 'DATE',
        'BIRTHDATE': 'DATE',
        'DOB': 'DATE',
        'ADMISSION_DATE': 'DATE',
        
        # ID ç›¸é—œ
        'ID': 'ID',
        'ID_NUMBER': 'ID',
        'NATIONAL_ID': 'ID',
        'MRN': 'MEDICAL_RECORD_NUMBER',
        'MEDICAL_RECORD': 'MEDICAL_RECORD_NUMBER',
        
        # è¯çµ¡è³‡è¨Š
        'PHONE': 'PHONE',
        'TELEPHONE': 'PHONE',
        'MOBILE': 'PHONE',
        'EMAIL': 'EMAIL',
        'ADDRESS': 'ADDRESS',
        'LOCATION': 'LOCATION',
        
        # å…¶ä»–
        'FACILITY': 'FACILITY',
        'HOSPITAL': 'FACILITY',
        'DEVICE_ID': 'DEVICE_ID',
        'ACCOUNT': 'ACCOUNT',
    }
    return type_mapping.get(phi_type.upper(), phi_type.upper())


def parse_llm_response(response_text: str) -> List[PHIInstance]:
    """è§£æ LLM çš„ PHI æª¢æ¸¬å›æ‡‰"""
    detected = []
    
    # å˜—è©¦è§£æ JSON æ ¼å¼
    try:
        # å°‹æ‰¾ JSON å€å¡Š
        json_match = re.search(r'\[[\s\S]*?\]', response_text)
        if json_match:
            data = json.loads(json_match.group())
            for item in data:
                if isinstance(item, dict):
                    phi_type = item.get('type', item.get('phi_type', 'UNKNOWN'))
                    content = item.get('text', item.get('content', item.get('value', '')))
                    if content:
                        detected.append(PHIInstance(
                            phi_type=normalize_phi_type(phi_type),
                            content=str(content).strip()
                        ))
            return detected
    except (json.JSONDecodeError, TypeError):
        pass
    
    # å˜—è©¦è§£æåˆ—è¡¨æ ¼å¼
    # ä¾‹å¦‚: "- NAME: ç‹å¤§æ˜" æˆ– "1. å§“å: ç‹å¤§æ˜"
    patterns = [
        r'[-â€¢*]\s*(\w+)\s*[:ï¼š]\s*(.+?)(?=\n|$)',
        r'\d+[.ã€]\s*(\w+)\s*[:ï¼š]\s*(.+?)(?=\n|$)',
        r'(\w+)\s*[:ï¼š]\s*[ã€Œã€""]?(.+?)[ã€ã€""]?(?=\n|$)',
    ]
    
    for pattern in patterns:
        for match in re.finditer(pattern, response_text):
            phi_type = match.group(1).strip()
            content = match.group(2).strip()
            
            # éæ¿¾æ‰é PHI çš„é …ç›®
            if phi_type.upper() in ['PHI', 'TYPE', 'VALUE', 'TEXT', 'é¡å‹', 'å…§å®¹']:
                continue
            
            if content and len(content) < 100:  # é¿å…éé•·çš„èª¤åˆ¤
                detected.append(PHIInstance(
                    phi_type=normalize_phi_type(phi_type),
                    content=content
                ))
    
    return detected


def evaluate_case(
    case_id: str,
    ground_truth: List[PHIInstance],
    detected: List[PHIInstance]
) -> EvaluationResult:
    """è©•ä¼°å–®å€‹æ¡ˆä¾‹"""
    result = EvaluationResult(
        case_id=case_id,
        ground_truth=ground_truth,
        detected=detected
    )
    
    # æ¨™æº–åŒ–é¡å‹
    gt_normalized = {
        PHIInstance(normalize_phi_type(phi.phi_type), phi.content.strip())
        for phi in ground_truth
    }
    det_normalized = {
        PHIInstance(normalize_phi_type(phi.phi_type), phi.content.strip())
        for phi in detected
    }
    
    # ä¹Ÿç”¨å…§å®¹æ¨¡ç³ŠåŒ¹é…
    gt_contents = {phi.content.strip().lower() for phi in ground_truth}
    det_contents = {phi.content.strip().lower() for phi in detected}
    
    # è¨ˆç®— TP, FP, FN
    for phi in detected:
        content_lower = phi.content.strip().lower()
        if phi in gt_normalized or content_lower in gt_contents:
            result.true_positives.append(phi)
        else:
            result.false_positives.append(phi)
    
    for phi in ground_truth:
        content_lower = phi.content.strip().lower()
        matched = any(
            d.content.strip().lower() == content_lower
            for d in detected
        )
        if not matched:
            result.false_negatives.append(phi)
    
    return result


def run_minimind_detection(text: str, llm, timeout: int = 120) -> Tuple[List[PHIInstance], float]:
    """ä½¿ç”¨ LLM åŸ·è¡Œ PHI æª¢æ¸¬"""
    prompt = f"""è«‹å¾ä»¥ä¸‹é†«ç™‚æ–‡æœ¬ä¸­è­˜åˆ¥æ‰€æœ‰å€‹äººå¥åº·è³‡è¨Š (PHI)ã€‚

PHI é¡å‹åŒ…æ‹¬ï¼š
- NAME: å§“åï¼ˆç—…æ‚£ã€é†«å¸«ã€å®¶å±¬ç­‰ï¼‰
- AGE: å¹´é½¡
- DATE: æ—¥æœŸï¼ˆå‡ºç”Ÿæ—¥æœŸã€å°±è¨ºæ—¥æœŸç­‰ï¼‰
- ID: èº«åˆ†è­‰å­—è™Ÿã€ç—…æ­·è™Ÿ
- PHONE: é›»è©±è™Ÿç¢¼
- EMAIL: é›»å­éƒµä»¶
- ADDRESS: åœ°å€
- LOCATION: åœ°é»ã€æ©Ÿæ§‹åç¨±
- FACILITY: é†«ç™‚æ©Ÿæ§‹

è«‹ä»¥ JSON æ ¼å¼åˆ—å‡ºæ‰€æœ‰ PHIï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"type": "NAME", "text": "å¯¦éš›å…§å®¹"}},
  ...
]

é†«ç™‚æ–‡æœ¬ï¼š
{text[:2000]}

è«‹åˆ—å‡ºæ‰€æœ‰ PHIï¼š"""
    
    start_time = time.time()
    try:
        response = llm.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        elapsed = time.time() - start_time
        
        detected = parse_llm_response(content)
        return detected, elapsed
    except Exception as e:
        logger.warning(f"LLM error (will retry): {e}")
        elapsed = time.time() - start_time
        return [], elapsed


def print_evaluation_report(results: List[EvaluationResult], model_name: str):
    """åˆ—å°è©•ä¼°å ±å‘Š"""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š MiniMind PHI Detection Evaluation Report")
    print(f"   Model: {model_name}")
    print("=" * 80)
    
    total_gt = sum(len(r.ground_truth) for r in results)
    total_detected = sum(len(r.detected) for r in results)
    total_tp = sum(len(r.true_positives) for r in results)
    total_fp = sum(len(r.false_positives) for r in results)
    total_fn = sum(len(r.false_negatives) for r in results)
    total_time = sum(r.processing_time for r in results)
    
    # ç¸½é«”æŒ‡æ¨™
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nğŸ“ˆ Overall Metrics | ç¸½é«”æŒ‡æ¨™")
    print("-" * 40)
    print(f"  Ground Truth PHI:     {total_gt:>5}")
    print(f"  Detected PHI:         {total_detected:>5}")
    print(f"  True Positives (TP):  {total_tp:>5}  âœ… æ­£ç¢ºæª¢æ¸¬")
    print(f"  False Positives (FP): {total_fp:>5}  âš ï¸  éåº¦æª¢æ¸¬")
    print(f"  False Negatives (FN): {total_fn:>5}  âŒ æ¼æª¢")
    print("-" * 40)
    print(f"  Precision:  {precision:.2%}  (é¿å…éåº¦æª¢æ¸¬)")
    print(f"  Recall:     {recall:.2%}  (é¿å…æ¼æª¢)")
    print(f"  F1 Score:   {f1:.2%}  (ç¶œåˆè©•åˆ†)")
    print(f"  Avg Time:   {total_time/len(results):.2f}s per case")
    
    # å„æ¡ˆä¾‹è©³æƒ…
    print(f"\nğŸ“‹ Per-Case Results | å„æ¡ˆä¾‹çµæœ")
    print("-" * 80)
    print(f"{'Case ID':<12} {'GT':>4} {'Det':>4} {'TP':>4} {'FP':>4} {'FN':>4} {'Prec':>7} {'Rec':>7} {'F1':>7} {'Time':>6}")
    print("-" * 80)
    
    for r in results:
        print(f"{r.case_id:<12} {len(r.ground_truth):>4} {len(r.detected):>4} "
              f"{len(r.true_positives):>4} {len(r.false_positives):>4} {len(r.false_negatives):>4} "
              f"{r.precision:>6.1%} {r.recall:>6.1%} {r.f1_score:>6.1%} {r.processing_time:>5.1f}s")
    
    # éåº¦æª¢æ¸¬åˆ†æ
    if total_fp > 0:
        print(f"\nâš ï¸  Over-Detection Analysis | éåº¦æª¢æ¸¬åˆ†æ")
        print("-" * 60)
        fp_by_type = defaultdict(list)
        for r in results:
            for fp in r.false_positives:
                fp_by_type[fp.phi_type].append((r.case_id, fp.content))
        
        for phi_type, items in sorted(fp_by_type.items(), key=lambda x: -len(x[1])):
            print(f"\n  {phi_type}: {len(items)} æ¬¡éåº¦æª¢æ¸¬")
            for case_id, content in items[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                print(f"    - [{case_id}] \"{content[:30]}{'...' if len(content) > 30 else ''}\"")
            if len(items) > 5:
                print(f"    ... é‚„æœ‰ {len(items)-5} å€‹")
    
    # æ¼æª¢åˆ†æ
    if total_fn > 0:
        print(f"\nâŒ Missed Detection Analysis | æ¼æª¢åˆ†æ")
        print("-" * 60)
        fn_by_type = defaultdict(list)
        for r in results:
            for fn in r.false_negatives:
                fn_by_type[fn.phi_type].append((r.case_id, fn.content))
        
        for phi_type, items in sorted(fn_by_type.items(), key=lambda x: -len(x[1])):
            print(f"\n  {phi_type}: {len(items)} æ¬¡æ¼æª¢")
            for case_id, content in items[:5]:
                print(f"    - [{case_id}] \"{content[:30]}{'...' if len(content) > 30 else ''}\"")
            if len(items) > 5:
                print(f"    ... é‚„æœ‰ {len(items)-5} å€‹")
    
    # è©•åˆ†ç­‰ç´š
    print(f"\nğŸ† Performance Grade | æ•ˆèƒ½ç­‰ç´š")
    print("-" * 40)
    if f1 >= 0.9:
        grade = "A+ (Excellent)"
    elif f1 >= 0.8:
        grade = "A (Very Good)"
    elif f1 >= 0.7:
        grade = "B (Good)"
    elif f1 >= 0.6:
        grade = "C (Acceptable)"
    elif f1 >= 0.5:
        grade = "D (Needs Improvement)"
    else:
        grade = "F (Poor)"
    
    print(f"  Overall Grade: {grade}")
    print(f"  Note: MiniMind is a 104M parameter model")
    print(f"        For production, consider Qwen 2.5 7B+")
    
    print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•¸"""
    # Fix Windows encoding
    import io
    import argparse
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    
    # Parse arguments
    parser = argparse.ArgumentParser(description='Evaluate PHI detection performance')
    parser.add_argument('--model', type=str, default='minimind', 
                        choices=['minimind', 'llama', 'qwen'],
                        help='Model to evaluate: minimind, llama, qwen')
    args = parser.parse_args()
    
    print(f"[Loading] {args.model} model...")
    
    # Import and create LLM
    from medical_deidentification.infrastructure.llm import LLMPresets, create_llm
    
    try:
        if args.model == 'minimind':
            llm = create_llm(LLMPresets.local_minimind())
            model_name = "jingyaogong/minimind2 (104M)"
        elif args.model == 'llama':
            llm = create_llm(LLMPresets.local_llama())
            model_name = "llama3.1:8b (8B)"
        elif args.model == 'qwen':
            llm = create_llm(LLMPresets.local_qwen())
            model_name = "qwen2.5:7b (7B)"
        else:
            llm = create_llm(LLMPresets.local_minimind())
            model_name = "jingyaogong/minimind2 (104M)"
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        logger.info("Make sure Ollama is running and model is installed")
        return
    
    # Load test data
    test_file = Path("data/test/test_phi_tagged_cases.xlsx")
    if not test_file.exists():
        logger.error(f"Test file not found: {test_file}")
        return
    
    print(f"[Data] Loading test data from {test_file}")
    df = pd.read_excel(test_file)
    
    print(f"[Info] Testing ALL {len(df)} cases for complete evaluation")
    
    results = []
    
    # Process each case
    for idx, row in df.iterrows():
        case_id = row['Case ID']
        
        # åˆä½µæ‰€æœ‰æ–‡æœ¬æ¬„ä½
        text_columns = [
            'Clinical Summary\n(å«æ¨™è¨˜çš„ PHI)',
            'Contact Info\n(å«æ¨™è¨˜çš„è¯çµ¡è³‡è¨Š)',
            'Medical History\n(å«æ¨™è¨˜çš„æ™‚é–“/åœ°é»)',
            'Treatment Notes\n(å«æ¨™è¨˜çš„é†«å¸«/æ—¥æœŸ)'
        ]
        
        full_text_with_tags = ""
        for col in text_columns:
            if col in df.columns and pd.notna(row[col]):
                full_text_with_tags += str(row[col]) + "\n"
        
        # è§£ææ¨™æº–ç­”æ¡ˆ
        ground_truth = parse_phi_tags(full_text_with_tags)
        
        # ç§»é™¤æ¨™è¨˜ï¼Œæº–å‚™é€çµ¦ LLM
        clean_text = remove_phi_tags(full_text_with_tags)
        
        print(f"\nğŸ” Processing {case_id} ({len(ground_truth)} PHI in ground truth)...")
        
        # åŸ·è¡Œ MiniMind æª¢æ¸¬
        detected, elapsed = run_minimind_detection(clean_text, llm)
        
        # è©•ä¼°çµæœ
        result = evaluate_case(case_id, ground_truth, detected)
        result.processing_time = elapsed
        results.append(result)
        
        print(f"   Detected: {len(detected)}, TP: {len(result.true_positives)}, "
              f"FP: {len(result.false_positives)}, FN: {len(result.false_negatives)}, "
              f"Time: {elapsed:.1f}s")
    
    # åˆ—å°å®Œæ•´å ±å‘Š
    print_evaluation_report(results, model_name)
    
    # å„²å­˜çµæœ
    output_file = Path(f"data/output/reports/{args.model}_evaluation_report.json")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    report_data = {
        "model": model_name,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "summary": {
            "total_cases": len(results),
            "total_ground_truth": sum(len(r.ground_truth) for r in results),
            "total_detected": sum(len(r.detected) for r in results),
            "total_tp": sum(len(r.true_positives) for r in results),
            "total_fp": sum(len(r.false_positives) for r in results),
            "total_fn": sum(len(r.false_negatives) for r in results),
            "precision": sum(len(r.true_positives) for r in results) / max(1, sum(len(r.detected) for r in results)),
            "recall": sum(len(r.true_positives) for r in results) / max(1, sum(len(r.ground_truth) for r in results)),
        },
        "cases": [
            {
                "case_id": r.case_id,
                "ground_truth_count": len(r.ground_truth),
                "detected_count": len(r.detected),
                "tp": len(r.true_positives),
                "fp": len(r.false_positives),
                "fn": len(r.false_negatives),
                "precision": r.precision,
                "recall": r.recall,
                "f1": r.f1_score,
                "time": r.processing_time,
                "false_positives": [{"type": p.phi_type, "content": p.content} for p in r.false_positives],
                "false_negatives": [{"type": p.phi_type, "content": p.content} for p in r.false_negatives],
            }
            for r in results
        ]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ Report saved to: {output_file}")


if __name__ == "__main__":
    main()
