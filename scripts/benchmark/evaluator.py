"""
PHI Evaluator | PHI è­˜åˆ¥è©•ä¼°å™¨

æ•´åˆ PHI è­˜åˆ¥ç³»çµ±èˆ‡ benchmark è³‡æ–™é€²è¡Œè©•ä¼°
æ”¯æ´ Presidio Evaluator é¢¨æ ¼çš„è©•ä¼°æµç¨‹
"""

import json
import logging
import time
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

from .data_loader import (
    BenchmarkSample,
    load_benchmark_data,
)
from .metrics import (
    ConfusionMatrix,
    EvaluationMetrics,
    calculate_metrics,
    calculate_metrics_by_type,
)

logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """
    å–®å€‹æ¨£æœ¬çš„è©•ä¼°çµæžœ
    
    Attributes:
        sample_id: æ¨£æœ¬ ID
        ground_truth: æ¨™æº–ç­”æ¡ˆ
        predictions: é æ¸¬çµæžœ
        confusion_matrix: æ··æ·†çŸ©é™£
        inference_time: æŽ¨è«–æ™‚é–“ (ç§’)
        error: éŒ¯èª¤è¨Šæ¯ (å¦‚æœ‰)
    """
    sample_id: str
    ground_truth: list[tuple]
    predictions: list[tuple]
    confusion_matrix: ConfusionMatrix
    inference_time: float = 0.0
    error: str | None = None

    @property
    def is_success(self) -> bool:
        return self.error is None

    def to_dict(self) -> dict:
        return {
            "sample_id": self.sample_id,
            "ground_truth": self.ground_truth,
            "predictions": self.predictions,
            "metrics": self.confusion_matrix.to_dict(),
            "inference_time": self.inference_time,
            "error": self.error,
        }


@dataclass
class EvaluationReport:
    """
    å®Œæ•´è©•ä¼°å ±å‘Š
    
    Attributes:
        metrics: è©•ä¼°æŒ‡æ¨™
        results: å„æ¨£æœ¬çµæžœ
        config: è©•ä¼°è¨­å®š
        timestamp: è©•ä¼°æ™‚é–“
    """
    metrics: EvaluationMetrics
    results: list[EvaluationResult]
    config: dict[str, Any]
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    @property
    def success_rate(self) -> float:
        if not self.results:
            return 0.0
        return sum(1 for r in self.results if r.is_success) / len(self.results)

    def to_dict(self) -> dict:
        return {
            "timestamp": self.timestamp,
            "config": self.config,
            "metrics": self.metrics.to_dict(),
            "success_rate": self.success_rate,
            "total_samples": len(self.results),
            "results": [r.to_dict() for r in self.results],
        }

    def save(self, path: str | Path):
        """å„²å­˜å ±å‘Šç‚º JSON"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)

    def print_summary(self):
        """å°å‡ºæ‘˜è¦"""
        m = self.metrics
        print("\n" + "=" * 60)
        print("ðŸ“Š PHI è­˜åˆ¥è©•ä¼°å ±å‘Š")
        print("=" * 60)
        print(f"â° æ™‚é–“: {self.timestamp}")
        print(f"ðŸ“ æ¨£æœ¬æ•¸: {m.total_samples}")
        print(f"âœ… æˆåŠŸçŽ‡: {self.success_rate:.1%}")
        print(f"â±ï¸  å¹³å‡æ™‚é–“: {m.avg_time_per_sample:.2f}s/sample")
        print("-" * 60)
        print("ðŸ“ˆ æ•´é«”æŒ‡æ¨™:")
        print(f"   Precision: {m.overall.precision:.3f}")
        print(f"   Recall:    {m.overall.recall:.3f}")
        print(f"   F1 Score:  {m.overall.f1:.3f}")
        print("-" * 60)
        print("ðŸ“‹ æŒ‰ PHI é¡žåž‹:")
        for phi_type, cm in sorted(m.by_type.items()):
            print(f"   {phi_type:12} | P={cm.precision:.3f} R={cm.recall:.3f} F1={cm.f1:.3f}")
        print("=" * 60)


# Type alias for PHI detector function
PHIDetector = Callable[[str], list[tuple]]


class PHIEvaluator:
    """
    PHI è­˜åˆ¥è©•ä¼°å™¨
    
    æ•´åˆ PHI è­˜åˆ¥ç³»çµ±èˆ‡ benchmark è³‡æ–™é€²è¡Œè©•ä¼°ã€‚
    æ”¯æ´ Presidio Evaluator é¢¨æ ¼çš„è©•ä¼°æµç¨‹ã€‚
    
    ä½¿ç”¨ç¯„ä¾‹:
    ```python
    # å®šç¾© PHI detector å‡½æ•¸
    def my_detector(text: str) -> List[tuple]:
        # å‘¼å«ä½ çš„ PHI è­˜åˆ¥ç³»çµ±
        result = your_phi_system.identify(text)
        return [(entity.text, entity.type) for entity in result]
    
    # å»ºç«‹è©•ä¼°å™¨
    evaluator = PHIEvaluator(detector=my_detector)
    
    # åŸ·è¡Œè©•ä¼°
    report = evaluator.evaluate("data/benchmark/test.jsonl")
    report.print_summary()
    report.save("results/evaluation.json")
    ```
    """

    def __init__(
        self,
        detector: PHIDetector | None = None,
        match_type: str = "partial",
        verbose: bool = True,
    ):
        """
        åˆå§‹åŒ–è©•ä¼°å™¨
        
        Args:
            detector: PHI è­˜åˆ¥å‡½æ•¸ï¼ŒæŽ¥æ”¶æ–‡å­—ï¼Œè¿”å›ž [(text, type), ...]
            match_type: åŒ¹é…æ–¹å¼ (exact, partial, overlap)
            verbose: æ˜¯å¦é¡¯ç¤ºé€²åº¦
        """
        self.detector = detector
        self.match_type = match_type
        self.verbose = verbose

    def set_detector(self, detector: PHIDetector):
        """è¨­å®š PHI è­˜åˆ¥å™¨"""
        self.detector = detector

    def evaluate(
        self,
        data_path: str | Path,
        format: str = "auto",
        limit: int | None = None,
        save_path: str | Path | None = None,
    ) -> EvaluationReport:
        """
        åŸ·è¡Œè©•ä¼°
        
        Args:
            data_path: benchmark è³‡æ–™è·¯å¾‘
            format: è³‡æ–™æ ¼å¼
            limit: é™åˆ¶æ¨£æœ¬æ•¸ (debug ç”¨)
            save_path: çµæžœå„²å­˜è·¯å¾‘
        
        Returns:
            EvaluationReport
        """
        if self.detector is None:
            raise ValueError("No detector set. Use set_detector() first.")

        # è¼‰å…¥è³‡æ–™
        samples = list(load_benchmark_data(data_path, format))
        if limit:
            samples = samples[:limit]

        if self.verbose:
            print(f"ðŸ“‚ è¼‰å…¥ {len(samples)} å€‹æ¨£æœ¬")

        # è©•ä¼°
        results = []
        metrics = EvaluationMetrics()
        total_time = 0.0

        for i, sample in enumerate(samples):
            if self.verbose and (i + 1) % 10 == 0:
                print(f"â³ é€²åº¦: {i + 1}/{len(samples)}")

            result = self._evaluate_sample(sample)
            results.append(result)

            if result.is_success:
                metrics.overall = metrics.overall + result.confusion_matrix
                total_time += result.inference_time

                # æŒ‰é¡žåž‹çµ±è¨ˆ
                type_metrics = calculate_metrics_by_type(
                    sample.ground_truth,
                    result.predictions,
                    self.match_type,
                )
                for phi_type, cm in type_metrics.items():
                    metrics.add_type_result(phi_type, cm)

        metrics.total_samples = len(samples)
        metrics.total_time = total_time

        # å»ºç«‹å ±å‘Š
        report = EvaluationReport(
            metrics=metrics,
            results=results,
            config={
                "data_path": str(data_path),
                "format": format,
                "match_type": self.match_type,
                "limit": limit,
            },
        )

        if save_path:
            report.save(save_path)
            if self.verbose:
                print(f"ðŸ’¾ å ±å‘Šå·²å„²å­˜è‡³ {save_path}")

        if self.verbose:
            report.print_summary()

        return report

    def _evaluate_sample(self, sample: BenchmarkSample) -> EvaluationResult:
        """è©•ä¼°å–®å€‹æ¨£æœ¬"""
        try:
            start_time = time.time()
            predictions = self.detector(sample.text)
            inference_time = time.time() - start_time

            cm = calculate_metrics(
                sample.ground_truth,
                predictions,
                self.match_type,
            )

            return EvaluationResult(
                sample_id=sample.id,
                ground_truth=sample.ground_truth,
                predictions=predictions,
                confusion_matrix=cm,
                inference_time=inference_time,
            )

        except Exception as e:
            logger.error(f"Error evaluating sample {sample.id}: {e}")
            return EvaluationResult(
                sample_id=sample.id,
                ground_truth=sample.ground_truth,
                predictions=[],
                confusion_matrix=ConfusionMatrix(),
                error=str(e),
            )

    def evaluate_samples(
        self,
        samples: list[BenchmarkSample],
        save_path: str | Path | None = None,
    ) -> EvaluationReport:
        """
        è©•ä¼°å·²è¼‰å…¥çš„æ¨£æœ¬åˆ—è¡¨
        
        Args:
            samples: BenchmarkSample åˆ—è¡¨
            save_path: çµæžœå„²å­˜è·¯å¾‘
        
        Returns:
            EvaluationReport
        """
        if self.detector is None:
            raise ValueError("No detector set. Use set_detector() first.")

        results = []
        metrics = EvaluationMetrics()
        total_time = 0.0

        for sample in samples:
            result = self._evaluate_sample(sample)
            results.append(result)

            if result.is_success:
                metrics.overall = metrics.overall + result.confusion_matrix
                total_time += result.inference_time

        metrics.total_samples = len(samples)
        metrics.total_time = total_time

        report = EvaluationReport(
            metrics=metrics,
            results=results,
            config={"source": "samples", "match_type": self.match_type},
        )

        if save_path:
            report.save(save_path)

        return report


def create_detector_from_engine(engine) -> PHIDetector:
    """
    å¾ž DeidentificationEngine å»ºç«‹ detector
    
    Args:
        engine: DeidentificationEngine å¯¦ä¾‹
    
    Returns:
        PHIDetector å‡½æ•¸
    """
    def detector(text: str) -> list[tuple]:
        result = engine.process(text)
        # å‡è¨­ engine.process è¿”å›žæœ‰ phi_entities å±¬æ€§çš„ç‰©ä»¶
        if hasattr(result, 'phi_entities'):
            return [(e.text, e.phi_type) for e in result.phi_entities]
        elif hasattr(result, 'entities'):
            return [(e.text, e.type) for e in result.entities]
        else:
            return []

    return detector


def quick_evaluate(
    detector: PHIDetector,
    data_path: str | Path,
    match_type: str = "partial",
) -> dict:
    """
    å¿«é€Ÿè©•ä¼°
    
    Returns:
        {"precision": float, "recall": float, "f1": float}
    """
    evaluator = PHIEvaluator(detector=detector, match_type=match_type, verbose=False)
    report = evaluator.evaluate(data_path)

    return {
        "precision": report.metrics.overall.precision,
        "recall": report.metrics.overall.recall,
        "f1": report.metrics.overall.f1,
        "total_samples": report.metrics.total_samples,
    }
