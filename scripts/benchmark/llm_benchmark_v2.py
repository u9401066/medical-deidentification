#!/usr/bin/env python3
"""
LLM PHI Detection Benchmark V2 - ä½¿ç”¨å°ˆæ¡ˆæ¶æ§‹
==============================================

èˆ‡ v1 çš„å·®ç•°ï¼š
- âœ… ä½¿ç”¨å°ˆæ¡ˆçš„ PHIDetectionResponse çµæ§‹åŒ–è¼¸å‡º
- âœ… ä½¿ç”¨å°ˆæ¡ˆçš„ LLM Factory (LangChain integration)
- âœ… æ”¯æ´ Ollama structured output
- âœ… è¼¸å‡ºå¯ç›´æ¥ç”¨æ–¼å¾ŒçºŒè™•ç† (PHIEntity)

ç”¨æ³•ï¼š
    # ç°¡å–®æ¡ˆä¾‹
    python scripts/benchmark/llm_benchmark_v2.py --models "llama3.1:8b,phi4:14b"
    
    # è¤‡é›œæ“¬çœŸæ¡ˆä¾‹
    python scripts/benchmark/llm_benchmark_v2.py --realistic --models "llama3.3:70b"
"""

import argparse
import json
import os
import re
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from core.domain.phi_identification_models import PHIDetectionResponse
from core.infrastructure.llm.config import LLMConfig
from core.infrastructure.llm.factory import create_structured_output_llm


# =============================================================================
# PHI æ¨™è¨˜è§£æå™¨ (æ”¯æ´è¤‡é›œæ“¬çœŸæ¡ˆä¾‹)
# =============================================================================

def parse_phi_tags(text: str) -> list[dict]:
    """å¾æ–‡æœ¬ä¸­è§£æ PHI æ¨™è¨˜"""
    pattern = r'ã€PHI:(\w+):?(\w*)ã€‘([^ã€]+?)ã€/PHIã€‘'
    matches = []
    for match in re.finditer(pattern, text):
        matches.append({
            'type': match.group(1),
            'text': match.group(3),
            'id': match.group(2) if match.group(2) else None,
        })
    return matches


def remove_phi_tags(text: str) -> str:
    """ç§»é™¤ PHI æ¨™è¨˜ï¼Œåªä¿ç•™å…§å®¹"""
    pattern = r'ã€PHI:\w+:?\w*ã€‘([^ã€]+?)ã€/PHIã€‘'
    return re.sub(pattern, r'\1', text)


def load_realistic_test_cases(excel_path: str) -> list[dict]:
    """å¾ PHI æ¨™è¨˜çš„ Excel æª”æ¡ˆè¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹"""
    try:
        import openpyxl
    except ImportError:
        print("âŒ éœ€è¦å®‰è£ openpyxl: pip install openpyxl")
        return []
    
    wb = openpyxl.load_workbook(excel_path)
    ws = wb.active
    
    test_cases = []
    for row in ws.iter_rows(min_row=2, values_only=True):
        if not row[0]:
            continue
        
        case_id = row[0]
        text_parts = []
        for col_idx in range(1, 5):
            if col_idx < len(row) and row[col_idx]:
                text_parts.append(str(row[col_idx]))
        
        full_text_with_tags = ' '.join(text_parts)
        expected_phi = parse_phi_tags(full_text_with_tags)
        clean_text = remove_phi_tags(full_text_with_tags)
        
        test_cases.append({
            "id": case_id,
            "text": clean_text,
            "expected_phi": expected_phi,
        })
    
    return test_cases


# =============================================================================
# ç°¡å–®æ¸¬è©¦æ¡ˆä¾‹
# =============================================================================

TEST_CASES = [
    {
        "id": "case_001",
        "text": "æ‚£è€…å¼µä¸‰ï¼Œç”·æ€§ï¼Œ45æ­²ï¼Œä½é™¢è™Ÿï¼šA123456ï¼Œæ–¼2024å¹´3æœˆ15æ—¥å…¥é™¢ã€‚",
        "expected_phi": [
            {"text": "å¼µä¸‰", "type": "NAME"},
            {"text": "45æ­²", "type": "AGE"},
            {"text": "A123456", "type": "MEDICAL_RECORD_NUMBER"},
            {"text": "2024å¹´3æœˆ15æ—¥", "type": "DATE"},
        ],
    },
    {
        "id": "case_002",
        "text": "ç—…äººæå°æ˜(èº«åˆ†è­‰å­—è™Ÿï¼šA123456789)ï¼Œé›»è©±0912-345-678ï¼Œemail: test@example.com",
        "expected_phi": [
            {"text": "æå°æ˜", "type": "NAME"},
            {"text": "A123456789", "type": "ID"},
            {"text": "0912-345-678", "type": "PHONE"},
            {"text": "test@example.com", "type": "EMAIL"},
        ],
    },
    {
        "id": "case_003",
        "text": "ç‹é†«å¸«è¨ºæ–·ï¼šæ‚£è€…é™³ç¾ç²ï¼Œå¥³ï¼Œå‡ºç”Ÿæ—¥æœŸ1965/08/20ï¼Œä½å€ï¼šå°åŒ—å¸‚ä¿¡ç¾©å€æ¾ä»è·¯100è™Ÿ5æ¨“",
        "expected_phi": [
            {"text": "ç‹é†«å¸«", "type": "NAME"},
            {"text": "é™³ç¾ç²", "type": "NAME"},
            {"text": "1965/08/20", "type": "DATE"},
            {"text": "å°åŒ—å¸‚ä¿¡ç¾©å€æ¾ä»è·¯100è™Ÿ5æ¨“", "type": "LOCATION"},
        ],
    },
    {
        "id": "case_004",
        "text": "è½‰è¨ºå–®ï¼šå°å¤§é†«é™¢ç¥ç¶“å…§ç§‘æ—å¿—æ˜ä¸»æ²»é†«å¸«ï¼Œç—…æ­·è™ŸM2024001234ï¼Œå¥ä¿å¡è™ŸA123456789",
        "expected_phi": [
            {"text": "å°å¤§é†«é™¢", "type": "HOSPITAL_NAME"},
            {"text": "æ—å¿—æ˜", "type": "NAME"},
            {"text": "M2024001234", "type": "MEDICAL_RECORD_NUMBER"},
            {"text": "A123456789", "type": "INSURANCE_NUMBER"},
        ],
    },
    {
        "id": "case_005",
        "text": "92æ­²é«˜é½¡æ‚£è€…é»ƒè€å…ˆç”Ÿï¼Œå› ç½•è¦‹ç–¾ç—…Hutchinson-Gilfordæ—©è¡°ç—‡å€™ç¾¤å°±è¨º",
        "expected_phi": [
            {"text": "92æ­²", "type": "AGE_OVER_89"},
            {"text": "é»ƒè€å…ˆç”Ÿ", "type": "NAME"},
            {"text": "Hutchinson-Gilfordæ—©è¡°ç—‡å€™ç¾¤", "type": "RARE_DISEASE"},
        ],
    },
]


# =============================================================================
# Benchmark é¡
# =============================================================================

@dataclass
class BenchmarkResult:
    """å–®ä¸€æ¨¡å‹çš„æ¸¬è©¦çµæœ"""
    model: str
    total_cases: int = 0
    passed_cases: int = 0
    
    # æ•ˆèƒ½æŒ‡æ¨™
    total_time_sec: float = 0.0
    avg_latency_sec: float = 0.0
    
    # æ­£ç¢ºç‡æŒ‡æ¨™
    true_positives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    
    # è©³ç´°çµæœ
    case_results: list = field(default_factory=list)
    errors: list = field(default_factory=list)
    
    # çµæ§‹åŒ–è¼¸å‡ºæˆåŠŸç‡
    structured_output_success: int = 0
    structured_output_failures: int = 0
    
    def calculate_metrics(self):
        """è¨ˆç®— Precision/Recall/F1"""
        if self.true_positives + self.false_positives > 0:
            self.precision = self.true_positives / (self.true_positives + self.false_positives)
        if self.true_positives + self.false_negatives > 0:
            self.recall = self.true_positives / (self.true_positives + self.false_negatives)
        if self.precision + self.recall > 0:
            self.f1_score = 2 * (self.precision * self.recall) / (self.precision + self.recall)


class StructuredBenchmark:
    """ä½¿ç”¨å°ˆæ¡ˆæ¶æ§‹çš„ Benchmark"""
    
    # PHI åµæ¸¬ Prompt (èˆ‡å°ˆæ¡ˆä¸€è‡´)
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½é†«ç™‚è³‡æ–™å»è­˜åˆ¥åŒ–å°ˆå®¶ã€‚
è«‹è­˜åˆ¥æ–‡æœ¬ä¸­æ‰€æœ‰çš„å€‹äººå¥åº·è³‡è¨Š(PHI)ï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼š

- NAME: å§“åã€æš±ç¨±
- AGE: å¹´é½¡ (89æ­²ä»¥ä¸‹)
- AGE_OVER_89: é«˜é½¡ (90æ­²ä»¥ä¸Š)
- DATE: æ—¥æœŸã€æ™‚é–“
- PHONE: é›»è©±è™Ÿç¢¼
- EMAIL: é›»å­éƒµä»¶
- ID: èº«åˆ†è­‰å­—è™Ÿã€è­·ç…§è™Ÿç¢¼
- MEDICAL_RECORD_NUMBER: ç—…æ­·è™Ÿ
- INSURANCE_NUMBER: å¥ä¿å¡è™Ÿ
- LOCATION: åœ°å€ã€åœ°é»
- HOSPITAL_NAME: é†«é™¢åç¨±
- RARE_DISEASE: ç½•è¦‹ç–¾ç—…åç¨±

è«‹ç²¾ç¢ºæ“·å– PHI æ–‡å­—ï¼Œä¸è¦æ“·å–æ•´æ®µå¥å­ã€‚"""

    def __init__(self, base_url: str, timeout: float = 120.0):
        self.base_url = base_url
        self.timeout = timeout
        
    def create_llm_config(self, model: str) -> LLMConfig:
        """ç‚ºæŒ‡å®šæ¨¡å‹å‰µå»º LLM é…ç½®"""
        return LLMConfig(
            provider="ollama",
            model_name=model,
            temperature=0.0,
            api_base=self.base_url,  # ä½¿ç”¨ api_base è€Œé base_url
            timeout=self.timeout,
        )
    
    def benchmark_model(
        self, 
        model: str, 
        test_cases: list[dict] | None = None
    ) -> BenchmarkResult:
        """æ¸¬è©¦å–®ä¸€æ¨¡å‹ (ä½¿ç”¨çµæ§‹åŒ–è¼¸å‡º)"""
        if test_cases is None:
            test_cases = TEST_CASES
        
        result = BenchmarkResult(model=model)
        result.total_cases = len(test_cases)
        
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹: {model} (Structured Output)")
        print(f"{'='*60}")
        
        # å‰µå»ºçµæ§‹åŒ–è¼¸å‡º LLM
        config = self.create_llm_config(model)
        try:
            structured_llm = create_structured_output_llm(
                config=config,
                schema=PHIDetectionResponse
            )
        except Exception as e:
            print(f"âŒ ç„¡æ³•å‰µå»ºçµæ§‹åŒ– LLM: {e}")
            result.errors.append({"error": str(e)})
            return result
        
        for case in test_cases:
            case_id = case["id"]
            text = case["text"]
            expected = case["expected_phi"]
            
            print(f"\n  ğŸ“ {case_id}: {text[:50]}...")
            
            # æ§‹å»º prompt
            prompt = f"{self.SYSTEM_PROMPT}\n\né†«ç™‚æ–‡æœ¬ï¼š\n{text}"
            
            # å‘¼å«çµæ§‹åŒ– LLM
            start = time.perf_counter()
            try:
                response: PHIDetectionResponse = structured_llm.invoke(prompt)
                latency = time.perf_counter() - start
                result.structured_output_success += 1
                
                # å¾çµæ§‹åŒ–è¼¸å‡ºæå– PHI
                detected = [
                    {"text": e.entity_text, "type": e.phi_type.value if e.phi_type else "UNKNOWN"}
                    for e in response.entities
                ]
                
            except Exception as e:
                latency = time.perf_counter() - start
                result.structured_output_failures += 1
                result.errors.append({"case_id": case_id, "error": str(e)})
                print(f"     âŒ çµæ§‹åŒ–è¼¸å‡ºå¤±æ•—: {e}")
                continue
            
            # è¨ˆç®— TP/FP/FN
            tp, fp, fn = self.compare_phi(expected, detected)
            result.true_positives += tp
            result.false_positives += fp
            result.false_negatives += fn
            result.total_time_sec += latency
            
            # è¨˜éŒ„çµæœ
            case_result = {
                "case_id": case_id,
                "latency_sec": latency,
                "expected_count": len(expected),
                "detected_count": len(detected),
                "tp": tp, "fp": fp, "fn": fn,
                "detected_entities": detected,  # ä¿ç•™çµæ§‹åŒ–è¼¸å‡º
            }
            result.case_results.append(case_result)
            
            # é¡¯ç¤ºçµæœ
            if fn == 0 and fp == 0:
                print(f"     âœ… TP={tp} FP={fp} FN={fn} | {latency:.2f}s")
            elif fn == 0:
                print(f"     âš ï¸ TP={tp} FP={fp} FN={fn} | {latency:.2f}s")
                fp_texts = {d["text"] for d in detected} - {e["text"].lower() for e in expected}
                print(f"        èª¤å ±: {fp_texts}")
            else:
                print(f"     âŒ TP={tp} FP={fp} FN={fn} | {latency:.2f}s")
                fn_texts = {e["text"] for e in expected} - {d["text"].lower() for d in detected}
                print(f"        æ¼åµ: {fn_texts}")
                if fp > 0:
                    fp_texts = {d["text"] for d in detected} - {e["text"].lower() for e in expected}
                    print(f"        èª¤å ±: {fp_texts}")
        
        # è¨ˆç®—æŒ‡æ¨™
        result.calculate_metrics()
        if result.total_cases > 0:
            result.avg_latency_sec = result.total_time_sec / result.total_cases
        
        return result
    
    def compare_phi(self, expected: list[dict], detected: list[dict]) -> tuple[int, int, int]:
        """æ¯”è¼ƒé æœŸå’Œåµæ¸¬åˆ°çš„ PHI"""
        expected_texts = {e["text"].lower() for e in expected}
        detected_texts = {d["text"].lower() for d in detected}
        
        tp = len(expected_texts & detected_texts)
        fp = len(detected_texts - expected_texts)
        fn = len(expected_texts - detected_texts)
        
        return tp, fp, fn
    
    def run_benchmark(
        self, 
        models: list[str],
        test_cases: list[dict] | None = None
    ) -> list[BenchmarkResult]:
        """åŸ·è¡Œå®Œæ•´ benchmark"""
        if test_cases is None:
            test_cases = TEST_CASES
        
        print(f"\nğŸš€ é–‹å§‹æ¸¬è©¦ {len(models)} å€‹æ¨¡å‹ (Structured Output Mode)...")
        print(f"ğŸ“Š æ¸¬è©¦æ¡ˆä¾‹æ•¸: {len(test_cases)}")
        
        results = []
        for model in models:
            try:
                result = self.benchmark_model(model, test_cases)
                results.append(result)
            except Exception as e:
                import traceback
                print(f"âŒ æ¸¬è©¦ {model} å¤±æ•—: {e}")
                traceback.print_exc()
        
        return results
    
    def print_summary(self, results: list[BenchmarkResult]):
        """å°å‡ºæ¸¬è©¦æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦ (Structured Output)")
        print("=" * 80)
        
        print(f"\n{'æ¨¡å‹':<25} {'F1':>8} {'Prec':>8} {'Recall':>8} {'å»¶é²(s)':>10} {'çµæ§‹åŒ–æˆåŠŸ':>12}")
        print("-" * 80)
        
        sorted_results = sorted(results, key=lambda r: r.f1_score, reverse=True)
        
        for r in sorted_results:
            success_rate = r.structured_output_success / max(r.total_cases, 1) * 100
            print(f"{r.model:<25} {r.f1_score:>8.1%} {r.precision:>8.1%} {r.recall:>8.1%} "
                  f"{r.avg_latency_sec:>10.2f} {success_rate:>11.0f}%")
        
        print("-" * 80)
        
        if sorted_results:
            best_f1 = sorted_results[0]
            print(f"\nğŸ† æœ€é«˜æ­£ç¢ºç‡: {best_f1.model} (F1={best_f1.f1_score:.1%})")
    
    def save_results(self, results: list[BenchmarkResult], output_path: str):
        """å„²å­˜æ¸¬è©¦çµæœ (åŒ…å«çµæ§‹åŒ–è¼¸å‡º)"""
        data = []
        for r in results:
            data.append({
                "model": r.model,
                "f1_score": r.f1_score,
                "precision": r.precision,
                "recall": r.recall,
                "avg_latency_sec": r.avg_latency_sec,
                "total_cases": r.total_cases,
                "true_positives": r.true_positives,
                "false_positives": r.false_positives,
                "false_negatives": r.false_negatives,
                "structured_output_success": r.structured_output_success,
                "structured_output_failures": r.structured_output_failures,
                "case_results": r.case_results,
                "errors": r.errors,
            })
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="LLM PHI Detection Benchmark V2 (Structured Output)")
    parser.add_argument("--base-url", default=os.getenv("OLLAMA_BASE_URL", "http://192.168.1.2:30133"),
                        help="Ollama API URL")
    parser.add_argument("--models", type=str, required=True,
                        help="è¦æ¸¬è©¦çš„æ¨¡å‹ï¼Œé€—è™Ÿåˆ†éš”")
    parser.add_argument("--output", type=str, default="data/output/benchmark_structured_results.json",
                        help="çµæœè¼¸å‡ºè·¯å¾‘")
    parser.add_argument("--timeout", type=float, default=120.0,
                        help="è«‹æ±‚è¶…æ™‚ç§’æ•¸")
    parser.add_argument("--realistic", action="store_true",
                        help="ä½¿ç”¨è¤‡é›œæ“¬çœŸæ¸¬è©¦æ¡ˆä¾‹")
    parser.add_argument("--test-file", type=str, 
                        default="data/test/test_phi_tagged_cases.xlsx",
                        help="PHI æ¨™è¨˜æ¸¬è©¦è³‡æ–™çš„ Excel è·¯å¾‘")
    
    args = parser.parse_args()
    
    print(f"ğŸ”— é€£æ¥ Ollama API: {args.base_url}")
    print(f"ğŸ“¦ ä½¿ç”¨å°ˆæ¡ˆæ¶æ§‹: PHIDetectionResponse (Pydantic)")
    
    benchmark = StructuredBenchmark(base_url=args.base_url, timeout=args.timeout)
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    models = [m.strip() for m in args.models.split(",")]
    
    # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
    test_cases = None
    if args.realistic:
        print(f"ğŸ“‚ è¼‰å…¥æ“¬çœŸæ¸¬è©¦è³‡æ–™: {args.test_file}")
        test_cases = load_realistic_test_cases(args.test_file)
        if test_cases:
            total_phi = sum(len(c["expected_phi"]) for c in test_cases)
            print(f"   âœ… è¼‰å…¥ {len(test_cases)} å€‹æ¡ˆä¾‹ï¼Œå…± {total_phi} å€‹ PHI æ¨™è¨˜")
    
    # åŸ·è¡Œæ¸¬è©¦
    results = benchmark.run_benchmark(models, test_cases)
    
    if results:
        benchmark.print_summary(results)
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        benchmark.save_results(results, args.output)


if __name__ == "__main__":
    main()
