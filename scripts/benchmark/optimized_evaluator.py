"""
Optimized PHI Evaluator | å„ªåŒ–ç‰ˆ PHI è©•ä¼°å™¨

é‡å° CPU-only ç’°å¢ƒå„ªåŒ–çš„è©•ä¼°å™¨ï¼š
- æ‰¹æ¬¡è™•ç†æ¸›å°‘ API å‘¼å«
- ç°¡åŒ– prompt æ¸›å°‘ token
- ä¸¦è¡Œè™•ç† (å¦‚æ”¯æ´)
- çµæœå¿«å–

é æœŸæ•ˆèƒ½: ~1000 tokens / 60s (CPU)
"""

import re
import json
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from langchain_ollama import ChatOllama
from pydantic import BaseModel, Field


@dataclass
class OptimizedConfig:
    """å„ªåŒ–è¨­å®š"""
    model: str = "granite4:1b"
    num_ctx: int = 512  # æœ€å° context
    num_predict: int = 150  # é™åˆ¶è¼¸å‡º
    batch_size: int = 5  # æ‰¹æ¬¡å¤§å°
    temperature: float = 0
    max_workers: int = 2  # ä¸¦è¡Œæ•¸


class FastPHI(BaseModel):
    """ç°¡åŒ– PHI çµæ§‹"""
    t: str = Field(description="text")  # çŸ­åç¨±ç¯€çœ token
    y: str = Field(description="type")  # NAME, DATE, PHONE, EMAIL, ID, LOCATION


class FastPHIList(BaseModel):
    """PHI åˆ—è¡¨"""
    p: List[FastPHI] = Field(default_factory=list)


# ç°¡åŒ–çš„ type mapping
TYPE_MAP = {
    'person': 'NAME', 'name': 'NAME', 'patient': 'NAME',
    'date': 'DATE', 'dob': 'DATE', 'birthday': 'DATE',
    'phone': 'PHONE', 'tel': 'PHONE', 'mobile': 'PHONE',
    'email': 'EMAIL', 'mail': 'EMAIL',
    'id': 'ID', 'ssn': 'ID', 'mrn': 'ID', 'account': 'ID',
    'address': 'LOCATION', 'location': 'LOCATION', 'city': 'LOCATION',
    'org': 'FACILITY', 'hospital': 'FACILITY', 'company': 'FACILITY',
}


def normalize_type(t: str) -> str:
    """å¿«é€Ÿ type æ¨™æº–åŒ–"""
    t_lower = t.lower().replace(' ', '').replace('_', '')
    for key, val in TYPE_MAP.items():
        if key in t_lower:
            return val
    return t.upper()


class OptimizedPHIDetector:
    """
    å„ªåŒ–ç‰ˆ PHI åµæ¸¬å™¨
    
    ä½¿ç”¨ç°¡åŒ– prompt å’Œæ‰¹æ¬¡è™•ç†åŠ é€Ÿ
    """
    
    # æ¥µç°¡ prompt (ç¯€çœ input tokens)
    PROMPT = "PHI(t=text,y=type NAME/DATE/PHONE/EMAIL/ID/LOCATION):"
    
    def __init__(self, config: Optional[OptimizedConfig] = None):
        self.config = config or OptimizedConfig()
        self.llm = ChatOllama(
            model=self.config.model,
            temperature=self.config.temperature,
            num_ctx=self.config.num_ctx,
            num_predict=self.config.num_predict,
        )
        self.structured_llm = self.llm.with_structured_output(
            FastPHIList, 
            method='json_schema'
        )
        
        # çµ±è¨ˆ
        self.total_calls = 0
        self.total_time = 0.0
    
    def detect(self, text: str) -> List[Tuple[str, str]]:
        """
        å–®ä¸€æ–‡æœ¬ PHI åµæ¸¬
        
        Returns:
            [(text, type), ...]
        """
        start = time.time()
        try:
            result = self.structured_llm.invoke(f"{self.PROMPT} {text[:500]}")
            self.total_calls += 1
            self.total_time += time.time() - start
            
            if result and result.p:
                return [(p.t, normalize_type(p.y)) for p in result.p]
        except Exception as e:
            pass
        
        return []
    
    def detect_batch(self, texts: List[str]) -> List[List[Tuple[str, str]]]:
        """
        æ‰¹æ¬¡ PHI åµæ¸¬
        
        å°‡å¤šå€‹æ–‡æœ¬åˆä½µè™•ç†ï¼Œæ¸›å°‘ API å‘¼å«
        """
        if not texts:
            return []
        
        # åˆä½µæ–‡æœ¬
        combined = "\n".join([
            f"[{i}]{t[:200]}" 
            for i, t in enumerate(texts)
        ])
        
        start = time.time()
        try:
            # ä½¿ç”¨éçµæ§‹åŒ–è¼¸å‡ºåŠ é€Ÿ
            result = self.llm.invoke(
                f"For each [N], list PHI as JSON {{t:text,y:type}}:\n{combined}"
            )
            self.total_calls += 1
            self.total_time += time.time() - start
            
            # è§£æçµæœ
            return self._parse_batch_result(result.content, len(texts))
            
        except Exception as e:
            return [[] for _ in texts]
    
    def _parse_batch_result(
        self, 
        content: str, 
        expected_count: int
    ) -> List[List[Tuple[str, str]]]:
        """è§£ææ‰¹æ¬¡çµæœ"""
        results = [[] for _ in range(expected_count)]
        
        # å˜—è©¦è§£æ JSON æ ¼å¼
        try:
            # æ‰¾æ‰€æœ‰ JSON objects
            pattern = r'\{[^}]+\}'
            matches = re.findall(pattern, content)
            
            current_idx = 0
            for match in matches:
                try:
                    obj = json.loads(match)
                    if 't' in obj and 'y' in obj:
                        results[current_idx].append(
                            (obj['t'], normalize_type(obj['y']))
                        )
                except:
                    pass
                    
        except Exception:
            pass
        
        return results
    
    def get_stats(self) -> Dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        return {
            "total_calls": self.total_calls,
            "total_time": self.total_time,
            "avg_time": self.total_time / self.total_calls if self.total_calls > 0 else 0,
        }


def run_optimized_benchmark(
    data_path: Path,
    limit: int = 20,
    batch_size: int = 1,
) -> Dict:
    """
    åŸ·è¡Œå„ªåŒ–ç‰ˆ benchmark
    
    Args:
        data_path: è³‡æ–™è·¯å¾‘
        limit: æ¨£æœ¬æ•¸é™åˆ¶
        batch_size: æ‰¹æ¬¡å¤§å°
    
    Returns:
        è©•ä¼°çµæœ
    """
    from scripts.benchmark import load_benchmark_data, calculate_metrics
    from scripts.benchmark.metrics import normalize_phi_type
    
    # è¼‰å…¥è³‡æ–™
    samples = list(load_benchmark_data(data_path, format='presidio'))[:limit]
    print(f"ğŸ“ è¼‰å…¥ {len(samples)} ç­†è³‡æ–™")
    
    # å»ºç«‹åµæ¸¬å™¨
    detector = OptimizedPHIDetector()
    print(f"ğŸ”§ ä½¿ç”¨ {detector.config.model}, ctx={detector.config.num_ctx}")
    
    # è©•ä¼°
    total_tp = 0
    total_fp = 0
    total_fn = 0
    
    start = time.time()
    
    for i, sample in enumerate(samples):
        if (i + 1) % 5 == 0:
            print(f"â³ {i+1}/{len(samples)}...")
        
        # åµæ¸¬
        predictions = detector.detect(sample.text)
        
        # æ¨™æº–åŒ– ground truth
        gt = [(ann.text, normalize_phi_type(ann.phi_type)) for ann in sample.annotations]
        pred_normalized = [(t, normalize_phi_type(y)) for t, y in predictions]
        
        # è¨ˆç®— metrics (ç°¡åŒ–ç‰ˆ)
        gt_set = set(t.lower() for t, _ in gt)
        pred_set = set(t.lower() for t, _ in pred_normalized)
        
        tp = len(gt_set & pred_set)
        fp = len(pred_set - gt_set)
        fn = len(gt_set - pred_set)
        
        total_tp += tp
        total_fp += fp
        total_fn += fn
    
    elapsed = time.time() - start
    
    # è¨ˆç®—æŒ‡æ¨™
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    stats = detector.get_stats()
    
    return {
        "samples": len(samples),
        "time": elapsed,
        "time_per_sample": elapsed / len(samples),
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "api_calls": stats["total_calls"],
        "avg_api_time": stats["avg_time"],
    }


if __name__ == "__main__":
    import sys
    
    data_path = Path("data/benchmark/presidio_synthetic.jsonl")
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 10
    
    print("=" * 60)
    print("ğŸ“Š Optimized PHI Benchmark")
    print("=" * 60)
    
    result = run_optimized_benchmark(data_path, limit=limit)
    
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Results")
    print("=" * 60)
    print(f"â±ï¸  Time: {result['time']:.1f}s ({result['time_per_sample']:.2f}s/sample)")
    print(f"ğŸ“Š Precision: {result['precision']:.3f}")
    print(f"ğŸ“Š Recall:    {result['recall']:.3f}")
    print(f"ğŸ“Š F1 Score:  {result['f1']:.3f}")
    print(f"ğŸ”¢ API Calls: {result['api_calls']} (avg {result['avg_api_time']:.2f}s)")
