#!/usr/bin/env python3
"""
LLM PHI Detection Benchmark | LLM PHI åµæ¸¬æ•ˆèƒ½æ¸¬è©¦

æ¸¬è©¦ä¸åŒæ¨¡å‹çš„ï¼š
1. æ¨ç†é€Ÿåº¦ (tokens/sec)
2. æ­£ç¢ºç‡ (Precision/Recall/F1)
3. é¦–æ¬¡å›æ‡‰å»¶é² (TTFT)

ç”¨æ³•ï¼š
    # è¨­å®šç’°å¢ƒè®Šæ•¸
    export OLLAMA_BASE_URL=http://172.18.11.101:30133
    
    # åŸ·è¡Œæ¸¬è©¦ï¼ˆç°¡å–®æ¡ˆä¾‹ï¼‰
    python scripts/benchmark/llm_benchmark.py
    
    # æ¸¬è©¦ç‰¹å®šæ¨¡å‹
    python scripts/benchmark/llm_benchmark.py --models "qwen2.5:7b,llama3.1:8b"
    
    # ä½¿ç”¨è¤‡é›œæ“¬çœŸæ¡ˆä¾‹ï¼ˆå¾ Excel è®€å– PHI æ¨™è¨˜è³‡æ–™ï¼‰
    python scripts/benchmark/llm_benchmark.py --realistic
    python scripts/benchmark/llm_benchmark.py --realistic --test-file data/test/test_phi_tagged_cases.xlsx
"""

import argparse
import json
import os
import re
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import httpx

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


# =============================================================================
# PHI æ¨™è¨˜è§£æå™¨ (æ”¯æ´è¤‡é›œæ“¬çœŸæ¡ˆä¾‹)
# =============================================================================

def parse_phi_tags(text: str) -> list[dict]:
    """
    å¾æ–‡æœ¬ä¸­è§£ææ‰€æœ‰ PHI æ¨™è¨˜
    
    æ ¼å¼: ã€PHI:TYPE:IDã€‘contentã€/PHIã€‘ or ã€PHI:TYPEã€‘contentã€/PHIã€‘
    
    Returns:
        List of dict with keys: type, text, id
    """
    pattern = r'ã€PHI:(\w+):?(\w*)ã€‘([^ã€]+?)ã€/PHIã€‘'
    matches = []
    
    for match in re.finditer(pattern, text):
        phi_type = match.group(1)
        phi_id = match.group(2) if match.group(2) else None
        content = match.group(3)
        
        matches.append({
            'type': phi_type,
            'text': content,
            'id': phi_id,
        })
    
    return matches


def remove_phi_tags(text: str) -> str:
    """ç§»é™¤ PHI æ¨™è¨˜ï¼Œåªä¿ç•™å…§å®¹"""
    # å°‡ ã€PHI:TYPE:IDã€‘contentã€/PHIã€‘ æ›¿æ›ç‚º content
    pattern = r'ã€PHI:\w+:?\w*ã€‘([^ã€]+?)ã€/PHIã€‘'
    return re.sub(pattern, r'\1', text)


def load_realistic_test_cases(excel_path: str) -> list[dict]:
    """
    å¾ PHI æ¨™è¨˜çš„ Excel æª”æ¡ˆè¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
    
    Args:
        excel_path: Excel æª”æ¡ˆè·¯å¾‘ (å¦‚ data/test/test_phi_tagged_cases.xlsx)
    
    Returns:
        æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨ï¼Œæ ¼å¼èˆ‡ TEST_CASES ç›¸åŒ
    """
    try:
        import openpyxl
    except ImportError:
        print("âŒ éœ€è¦å®‰è£ openpyxl: pip install openpyxl")
        return []
    
    wb = openpyxl.load_workbook(excel_path)
    ws = wb.active
    
    test_cases = []
    
    # è·³éè¡¨é ­ï¼Œå¾ç¬¬2è¡Œé–‹å§‹
    for row_idx, row in enumerate(ws.iter_rows(min_row=2, values_only=True), start=1):
        if not row[0]:  # è·³éç©ºè¡Œ
            continue
        
        case_id = row[0]
        
        # åˆä½µæ‰€æœ‰æ–‡æœ¬æ¬„ä½ (ç¬¬2-5åˆ—)
        text_parts = []
        for col_idx in range(1, 5):
            if col_idx < len(row) and row[col_idx]:
                text_parts.append(str(row[col_idx]))
        
        full_text_with_tags = ' '.join(text_parts)
        
        # è§£æ PHI æ¨™è¨˜ä½œç‚º ground truth
        expected_phi = parse_phi_tags(full_text_with_tags)
        
        # ç§»é™¤æ¨™è¨˜å¾—åˆ°ç´”æ–‡æœ¬ (é€™æ˜¯è¦é€çµ¦ LLM çš„)
        clean_text = remove_phi_tags(full_text_with_tags)
        
        test_cases.append({
            "id": case_id,
            "text": clean_text,
            "expected_phi": expected_phi,
            "original_with_tags": full_text_with_tags,
        })
    
    return test_cases


# =============================================================================
# æ¸¬è©¦è³‡æ–™
# =============================================================================

# æ¨™æº–æ¸¬è©¦æ¡ˆä¾‹ (å«æ¨™æº–ç­”æ¡ˆ)
TEST_CASES = [
    {
        "id": "case_001",
        "text": "æ‚£è€…å¼µä¸‰ï¼Œç”·æ€§ï¼Œ45æ­²ï¼Œä½é™¢è™Ÿï¼šA123456ï¼Œæ–¼2024å¹´3æœˆ15æ—¥å…¥é™¢ã€‚",
        "expected_phi": [
            {"text": "å¼µä¸‰", "type": "NAME"},
            {"text": "45æ­²", "type": "AGE"},
            {"text": "A123456", "type": "MEDICAL_RECORD_NUMBER"},
            {"text": "2024å¹´3æœˆ15æ—¥", "type": "DATE"},
        ],
    },
    {
        "id": "case_002",
        "text": "ç—…äººæå°æ˜(èº«åˆ†è­‰å­—è™Ÿï¼šA123456789)ï¼Œé›»è©±0912-345-678ï¼Œemail: test@example.com",
        "expected_phi": [
            {"text": "æå°æ˜", "type": "NAME"},
            {"text": "A123456789", "type": "ID"},
            {"text": "0912-345-678", "type": "PHONE"},
            {"text": "test@example.com", "type": "EMAIL"},
        ],
    },
    {
        "id": "case_003",
        "text": "ç‹é†«å¸«è¨ºæ–·ï¼šæ‚£è€…é™³ç¾ç²ï¼Œå¥³ï¼Œå‡ºç”Ÿæ—¥æœŸ1965/08/20ï¼Œä½å€ï¼šå°åŒ—å¸‚ä¿¡ç¾©å€æ¾ä»è·¯100è™Ÿ5æ¨“",
        "expected_phi": [
            {"text": "ç‹é†«å¸«", "type": "NAME"},
            {"text": "é™³ç¾ç²", "type": "NAME"},
            {"text": "1965/08/20", "type": "DATE"},
            {"text": "å°åŒ—å¸‚ä¿¡ç¾©å€æ¾ä»è·¯100è™Ÿ5æ¨“", "type": "LOCATION"},
        ],
    },
    {
        "id": "case_004",
        "text": "è½‰è¨ºå–®ï¼šå°å¤§é†«é™¢ç¥ç¶“å…§ç§‘æ—å¿—æ˜ä¸»æ²»é†«å¸«ï¼Œç—…æ­·è™ŸM2024001234ï¼Œå¥ä¿å¡è™ŸA123456789",
        "expected_phi": [
            {"text": "å°å¤§é†«é™¢", "type": "HOSPITAL_NAME"},
            {"text": "æ—å¿—æ˜", "type": "NAME"},
            {"text": "M2024001234", "type": "MEDICAL_RECORD_NUMBER"},
            {"text": "A123456789", "type": "INSURANCE_NUMBER"},
        ],
    },
    {
        "id": "case_005",
        "text": "92æ­²é«˜é½¡æ‚£è€…é»ƒè€å…ˆç”Ÿï¼Œå› ç½•è¦‹ç–¾ç—…Hutchinson-Gilfordæ—©è¡°ç—‡å€™ç¾¤å°±è¨º",
        "expected_phi": [
            {"text": "92æ­²", "type": "AGE_OVER_89"},
            {"text": "é»ƒè€å…ˆç”Ÿ", "type": "NAME"},
            {"text": "Hutchinson-Gilfordæ—©è¡°ç—‡å€™ç¾¤", "type": "RARE_DISEASE"},
        ],
    },
]

# PHI åµæ¸¬ Prompt (ä½¿ç”¨ $text$ ä½”ä½ç¬¦é¿å… JSON å¤§æ‹¬è™Ÿè¡çª)
PHI_DETECTION_PROMPT = """ä½ æ˜¯ä¸€ä½é†«ç™‚è³‡æ–™å»è­˜åˆ¥åŒ–å°ˆå®¶ã€‚è«‹è­˜åˆ¥ä»¥ä¸‹é†«ç™‚æ–‡æœ¬ä¸­çš„å€‹äººå¥åº·è³‡è¨Š(PHI)ã€‚

å°æ–¼æ¯å€‹æ‰¾åˆ°çš„ PHIï¼Œè«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
```json
{{
  "phi_entities": [
    {{"text": "æ‰¾åˆ°çš„æ–‡å­—", "type": "PHIé¡å‹"}}
  ]
}}
```

PHI é¡å‹åŒ…æ‹¬ï¼šNAME, DATE, AGE, AGE_OVER_89, LOCATION, PHONE, EMAIL, ID, 
MEDICAL_RECORD_NUMBER, INSURANCE_NUMBER, HOSPITAL_NAME, RARE_DISEASE ç­‰ã€‚

é†«ç™‚æ–‡æœ¬ï¼š
{text}

è«‹åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚"""


@dataclass
class BenchmarkResult:
    """å–®ä¸€æ¨¡å‹çš„æ¸¬è©¦çµæœ"""
    model: str
    total_cases: int = 0
    passed_cases: int = 0
    
    # æ•ˆèƒ½æŒ‡æ¨™
    total_time_sec: float = 0.0
    avg_latency_sec: float = 0.0
    avg_tokens_per_sec: float = 0.0
    
    # æ­£ç¢ºç‡æŒ‡æ¨™
    true_positives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    
    # è©³ç´°çµæœ
    case_results: list = field(default_factory=list)
    errors: list = field(default_factory=list)
    
    def calculate_metrics(self):
        """è¨ˆç®— Precision/Recall/F1"""
        if self.true_positives + self.false_positives > 0:
            self.precision = self.true_positives / (self.true_positives + self.false_positives)
        if self.true_positives + self.false_negatives > 0:
            self.recall = self.true_positives / (self.true_positives + self.false_negatives)
        if self.precision + self.recall > 0:
            self.f1_score = 2 * (self.precision * self.recall) / (self.precision + self.recall)


class OllamaBenchmark:
    """Ollama æ¨¡å‹æ•ˆèƒ½æ¸¬è©¦å™¨"""
    
    def __init__(self, base_url: str, timeout: float = 120.0):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.client = httpx.Client(timeout=timeout)
    
    def get_available_models(self) -> list[str]:
        """å–å¾—å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            resp = self.client.get(f"{self.base_url}/api/tags")
            resp.raise_for_status()
            data = resp.json()
            return [m["name"] for m in data.get("models", [])]
        except Exception as e:
            print(f"âŒ ç„¡æ³•å–å¾—æ¨¡å‹åˆ—è¡¨: {e}")
            return []
    
    def generate(self, model: str, prompt: str) -> tuple[str, float, dict]:
        """
        å‘¼å«æ¨¡å‹ç”Ÿæˆ
        
        Returns:
            (response_text, latency_sec, usage_info)
        """
        start = time.perf_counter()
        
        try:
            resp = self.client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.0,
                        "num_predict": 1024,
                    }
                }
            )
            resp.raise_for_status()
            data = resp.json()
            
            latency = time.perf_counter() - start
            
            usage = {
                "total_duration_ns": data.get("total_duration", 0),
                "load_duration_ns": data.get("load_duration", 0),
                "prompt_eval_count": data.get("prompt_eval_count", 0),
                "eval_count": data.get("eval_count", 0),
                "eval_duration_ns": data.get("eval_duration", 0),
            }
            
            return data.get("response", ""), latency, usage
            
        except Exception as e:
            return f"ERROR: {e}", time.perf_counter() - start, {}
    
    def parse_phi_response(self, response: str) -> list[dict]:
        """è§£ææ¨¡å‹å›å‚³çš„ PHI JSON"""
        import re
        
        # æ¸…ç†å›æ‡‰
        response = response.strip()
        
        # å˜—è©¦ç›´æ¥è§£æ
        try:
            data = json.loads(response)
            if isinstance(data, dict):
                return data.get("phi_entities", [])
            return []
        except json.JSONDecodeError:
            pass
        
        # å˜—è©¦æ‰¾å‡º JSON å€å¡Š (åŒ…å« code block)
        # ç§»é™¤ markdown code block
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # å˜—è©¦æ‰¾åˆ°æœ€å¤–å±¤çš„ { }
        try:
            # æ‰¾åˆ°ç¬¬ä¸€å€‹ { å’Œæœ€å¾Œä¸€å€‹ }
            start = response.find('{')
            end = response.rfind('}')
            if start != -1 and end != -1 and end > start:
                json_str = response[start:end+1]
                data = json.loads(json_str)
                if isinstance(data, dict):
                    return data.get("phi_entities", [])
        except json.JSONDecodeError:
            pass
        
        # å˜—è©¦æ‰¾ phi_entities é™£åˆ—
        match = re.search(r'"phi_entities"\s*:\s*\[(.*?)\]', response, re.DOTALL)
        if match:
            try:
                entities_str = '[' + match.group(1) + ']'
                return json.loads(entities_str)
            except:
                pass
        
        return []
    
    def compare_phi(self, expected: list[dict], detected: list[dict]) -> tuple[int, int, int]:
        """
        æ¯”è¼ƒé æœŸå’Œåµæ¸¬åˆ°çš„ PHI
        
        Returns:
            (true_positives, false_positives, false_negatives)
        """
        expected_texts = {e["text"].lower() for e in expected}
        detected_texts = {d["text"].lower() for d in detected}
        
        tp = len(expected_texts & detected_texts)
        fp = len(detected_texts - expected_texts)
        fn = len(expected_texts - detected_texts)
        
        return tp, fp, fn
    
    def benchmark_model(
        self, 
        model: str, 
        test_cases: list[dict] | None = None
    ) -> BenchmarkResult:
        """æ¸¬è©¦å–®ä¸€æ¨¡å‹"""
        if test_cases is None:
            test_cases = TEST_CASES
        
        result = BenchmarkResult(model=model)
        result.total_cases = len(test_cases)
        
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹: {model}")
        print(f"{'='*60}")
        
        for case in test_cases:
            case_id = case["id"]
            text = case["text"]
            expected = case["expected_phi"]
            
            print(f"\n  ğŸ“ {case_id}: {text[:50]}...")
            
            # ç”Ÿæˆ prompt
            prompt = PHI_DETECTION_PROMPT.format(text=text)
            
            # å‘¼å«æ¨¡å‹
            response, latency, usage = self.generate(model, prompt)
            
            if response.startswith("ERROR:"):
                result.errors.append({"case_id": case_id, "error": response})
                print(f"     âŒ éŒ¯èª¤: {response}")
                continue
            
            # è§£æå›æ‡‰
            detected = self.parse_phi_response(response)
            
            # è¨ˆç®—æ­£ç¢ºç‡
            tp, fp, fn = self.compare_phi(expected, detected)
            result.true_positives += tp
            result.false_positives += fp
            result.false_negatives += fn
            
            # è¨ˆç®—æ•ˆèƒ½
            result.total_time_sec += latency
            tokens_per_sec = 0
            if usage.get("eval_duration_ns", 0) > 0:
                tokens_per_sec = usage.get("eval_count", 0) / (usage["eval_duration_ns"] / 1e9)
            
            case_result = {
                "case_id": case_id,
                "latency_sec": latency,
                "tokens_per_sec": tokens_per_sec,
                "expected_count": len(expected),
                "detected_count": len(detected),
                "tp": tp, "fp": fp, "fn": fn,
            }
            result.case_results.append(case_result)
            
            # é¡¯ç¤ºçµæœ
            status = "âœ…" if fn == 0 and fp == 0 else "âš ï¸" if fn == 0 else "âŒ"
            print(f"     {status} TP={tp} FP={fp} FN={fn} | {latency:.2f}s | {tokens_per_sec:.1f} tok/s")
            
            if fp > 0 or fn > 0:
                expected_texts = {e["text"] for e in expected}
                detected_texts = {d["text"] for d in detected}
                if fn > 0:
                    print(f"        æ¼åµ: {expected_texts - detected_texts}")
                if fp > 0:
                    print(f"        èª¤å ±: {detected_texts - expected_texts}")
        
        # è¨ˆç®—ç¸½çµæŒ‡æ¨™
        result.calculate_metrics()
        if result.total_cases > 0:
            result.avg_latency_sec = result.total_time_sec / result.total_cases
        
        valid_cases = [c for c in result.case_results if c["tokens_per_sec"] > 0]
        if valid_cases:
            result.avg_tokens_per_sec = sum(c["tokens_per_sec"] for c in valid_cases) / len(valid_cases)
        
        return result
    
    def run_benchmark(
        self, 
        models: list[str] | None = None,
        test_cases: list[dict] | None = None
    ) -> list[BenchmarkResult]:
        """åŸ·è¡Œå®Œæ•´ benchmark"""
        if models is None:
            models = self.get_available_models()
            if not models:
                print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹")
                return []
        
        if test_cases is None:
            test_cases = TEST_CASES
        
        print(f"\nğŸš€ é–‹å§‹æ¸¬è©¦ {len(models)} å€‹æ¨¡å‹...")
        print(f"ğŸ“Š æ¸¬è©¦æ¡ˆä¾‹æ•¸: {len(test_cases)}")
        
        results = []
        for model in models:
            try:
                result = self.benchmark_model(model, test_cases)
                results.append(result)
            except Exception as e:
                import traceback
                print(f"âŒ æ¸¬è©¦ {model} å¤±æ•—: {e}")
                traceback.print_exc()
        
        return results
    
    def print_summary(self, results: list[BenchmarkResult]):
        """å°å‡ºæ¸¬è©¦æ‘˜è¦"""
        print("\n")
        print("=" * 80)
        print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
        print("=" * 80)
        
        # è¡¨é ­
        print(f"\n{'æ¨¡å‹':<25} {'F1':>8} {'Prec':>8} {'Recall':>8} {'å»¶é²(s)':>10} {'tok/s':>10}")
        print("-" * 80)
        
        # æŒ‰ F1 æ’åº
        sorted_results = sorted(results, key=lambda r: r.f1_score, reverse=True)
        
        for r in sorted_results:
            print(f"{r.model:<25} {r.f1_score:>8.1%} {r.precision:>8.1%} {r.recall:>8.1%} "
                  f"{r.avg_latency_sec:>10.2f} {r.avg_tokens_per_sec:>10.1f}")
        
        print("-" * 80)
        
        # æœ€ä½³æ¨¡å‹æ¨è–¦
        if sorted_results:
            best_f1 = sorted_results[0]
            fastest = min(results, key=lambda r: r.avg_latency_sec if r.avg_latency_sec > 0 else float('inf'))
            
            print(f"\nğŸ† æœ€é«˜æ­£ç¢ºç‡: {best_f1.model} (F1={best_f1.f1_score:.1%})")
            print(f"âš¡ æœ€å¿«é€Ÿåº¦: {fastest.model} ({fastest.avg_latency_sec:.2f}s, {fastest.avg_tokens_per_sec:.1f} tok/s)")
            
            # æ•ˆèƒ½/æ­£ç¢ºç‡å¹³è¡¡æ¨è–¦
            balanced = max(results, key=lambda r: r.f1_score * 0.7 + (1 / max(r.avg_latency_sec, 0.1)) * 0.3)
            print(f"âš–ï¸ æœ€ä½³å¹³è¡¡: {balanced.model} (F1={balanced.f1_score:.1%}, {balanced.avg_latency_sec:.2f}s)")
    
    def save_results(self, results: list[BenchmarkResult], output_path: str):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        data = []
        for r in results:
            data.append({
                "model": r.model,
                "f1_score": r.f1_score,
                "precision": r.precision,
                "recall": r.recall,
                "avg_latency_sec": r.avg_latency_sec,
                "avg_tokens_per_sec": r.avg_tokens_per_sec,
                "total_cases": r.total_cases,
                "true_positives": r.true_positives,
                "false_positives": r.false_positives,
                "false_negatives": r.false_negatives,
                "case_results": r.case_results,
                "errors": r.errors,
            })
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="LLM PHI Detection Benchmark")
    parser.add_argument("--base-url", default=os.getenv("OLLAMA_BASE_URL", "http://192.168.1.2:30133"),
                        help="Ollama API URL")
    parser.add_argument("--models", type=str, default=None,
                        help="è¦æ¸¬è©¦çš„æ¨¡å‹ï¼Œé€—è™Ÿåˆ†éš” (é è¨­ï¼šæ¸¬è©¦æ‰€æœ‰å¯ç”¨æ¨¡å‹)")
    parser.add_argument("--output", type=str, default="data/output/benchmark_results.json",
                        help="çµæœè¼¸å‡ºè·¯å¾‘")
    parser.add_argument("--timeout", type=float, default=120.0,
                        help="è«‹æ±‚è¶…æ™‚ç§’æ•¸")
    parser.add_argument("--realistic", action="store_true",
                        help="ä½¿ç”¨è¤‡é›œæ“¬çœŸæ¸¬è©¦æ¡ˆä¾‹ (å¾ Excel è®€å– PHI æ¨™è¨˜è³‡æ–™)")
    parser.add_argument("--test-file", type=str, 
                        default="data/test/test_phi_tagged_cases.xlsx",
                        help="PHI æ¨™è¨˜æ¸¬è©¦è³‡æ–™çš„ Excel è·¯å¾‘")
    
    args = parser.parse_args()
    
    print(f"ğŸ”— é€£æ¥ Ollama API: {args.base_url}")
    
    benchmark = OllamaBenchmark(base_url=args.base_url, timeout=args.timeout)
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    models = None
    if args.models:
        models = [m.strip() for m in args.models.split(",")]
    
    # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
    test_cases = None
    if args.realistic:
        print(f"ğŸ“‚ è¼‰å…¥æ“¬çœŸæ¸¬è©¦è³‡æ–™: {args.test_file}")
        test_cases = load_realistic_test_cases(args.test_file)
        if test_cases:
            total_phi = sum(len(c["expected_phi"]) for c in test_cases)
            print(f"   âœ… è¼‰å…¥ {len(test_cases)} å€‹æ¡ˆä¾‹ï¼Œå…± {total_phi} å€‹ PHI æ¨™è¨˜")
        else:
            print("   âŒ è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­ç°¡å–®æ¡ˆä¾‹")
            test_cases = None
    
    # åŸ·è¡Œæ¸¬è©¦
    results = benchmark.run_benchmark(models, test_cases)
    
    if results:
        # å°å‡ºæ‘˜è¦
        benchmark.print_summary(results)
        
        # å„²å­˜çµæœ
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        benchmark.save_results(results, args.output)


if __name__ == "__main__":
    main()
