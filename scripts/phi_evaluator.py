#!/usr/bin/env python3
"""
PHI Detection Evaluation Tool
PHI æª¢å‡ºç‡è©•ä¼°å·¥å…·

è©•ä¼° PHI æª¢æ¸¬ç³»çµ±çš„æ•ˆèƒ½ï¼Œè¨ˆç®— Precisionã€Recallã€F1 Score
"""

import re
from collections import Counter, defaultdict

import pandas as pd


class PHIEvaluator:
    """PHI æª¢æ¸¬æ•ˆèƒ½è©•ä¼°å™¨"""

    def __init__(self):
        self.tag_pattern = r'ã€PHI:(\w+):?(\w*)ã€‘([^ã€]+?)ã€/PHIã€‘'

    def parse_ground_truth(self, text: str) -> list[dict]:
        """
        å¾å¸¶æ¨™è¨˜çš„æ–‡æœ¬ä¸­æå–æ¨™æº–ç­”æ¡ˆ PHI
        
        Returns:
            List of dict: [{'type': 'NAME', 'id': 'P001', 'content': 'é™³è€å…ˆç”Ÿ', 'start': 0, 'end': 10}, ...]
        """
        phi_list = []

        for match in re.finditer(self.tag_pattern, text):
            phi_type = match.group(1)
            phi_id = match.group(2) if match.group(2) else None
            content = match.group(3)

            # è¨ˆç®—åœ¨åŸå§‹æ–‡æœ¬ï¼ˆç§»é™¤æ¨™è¨˜å¾Œï¼‰ä¸­çš„ä½ç½®
            # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦æ›´ç²¾ç¢ºçš„ä½ç½®è¨ˆç®—
            phi_list.append({
                'type': phi_type,
                'id': phi_id,
                'content': content.strip(),
                'start': match.start(),
                'end': match.end()
            })

        return phi_list

    def remove_tags(self, text: str) -> str:
        """ç§»é™¤ PHI æ¨™è¨˜ï¼Œè¿”å›åŸå§‹æ–‡æœ¬"""
        return re.sub(self.tag_pattern, r'\3', text)

    def calculate_metrics(
        self,
        ground_truth: list[dict],
        detected: list[dict],
        match_mode: str = 'exact'  # 'exact', 'partial', 'type_only'
    ) -> dict:
        """
        è¨ˆç®—æª¢æ¸¬æŒ‡æ¨™
        
        Args:
            ground_truth: æ¨™æº–ç­”æ¡ˆ PHI åˆ—è¡¨
            detected: ç³»çµ±æª¢æ¸¬çš„ PHI åˆ—è¡¨
            match_mode: åŒ¹é…æ¨¡å¼
                - 'exact': å…§å®¹å’Œé¡å‹éƒ½è¦å®Œå…¨åŒ¹é…
                - 'partial': å…§å®¹éƒ¨åˆ†åŒ¹é…å³å¯
                - 'type_only': åªè¦é¡å‹æ­£ç¢ºå³å¯
        
        Returns:
            æŒ‡æ¨™å­—å…¸: {TP, FP, FN, Precision, Recall, F1, detailed_results}
        """
        if match_mode == 'exact':
            gt_set = {(phi['type'], phi['content'].lower()) for phi in ground_truth}
            det_set = {(phi['type'], phi['content'].lower()) for phi in detected}

            tp_items = gt_set & det_set
            tp = len(tp_items)
            fp = len(det_set - gt_set)
            fn = len(gt_set - det_set)

        elif match_mode == 'partial':
            # éƒ¨åˆ†åŒ¹é…ï¼šæª¢æ¸¬çš„å…§å®¹åŒ…å«åœ¨æ¨™æº–ç­”æ¡ˆä¸­ï¼Œæˆ–æ¨™æº–ç­”æ¡ˆåŒ…å«åœ¨æª¢æ¸¬ä¸­
            tp = 0
            matched_gt = set()
            matched_det = set()

            for i, det_phi in enumerate(detected):
                for j, gt_phi in enumerate(ground_truth):
                    if det_phi['type'] == gt_phi['type']:
                        det_content = det_phi['content'].lower().strip()
                        gt_content = gt_phi['content'].lower().strip()

                        if det_content in gt_content or gt_content in det_content:
                            tp += 1
                            matched_gt.add(j)
                            matched_det.add(i)
                            break

            fp = len(detected) - len(matched_det)
            fn = len(ground_truth) - len(matched_gt)

        else:  # type_only
            gt_types = Counter(phi['type'] for phi in ground_truth)
            det_types = Counter(phi['type'] for phi in detected)

            tp = sum((gt_types & det_types).values())
            fp = sum((det_types - gt_types).values())
            fn = sum((gt_types - det_types).values())

        # è¨ˆç®—æŒ‡æ¨™
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # è¨ˆç®—å„é¡å‹çš„æŒ‡æ¨™
        type_metrics = self._calculate_type_metrics(ground_truth, detected, match_mode)

        return {
            'TP': tp,
            'FP': fp,
            'FN': fn,
            'Precision': precision,
            'Recall': recall,
            'F1': f1,
            'Accuracy': tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0,
            'by_type': type_metrics
        }

    def _calculate_type_metrics(self, ground_truth: list[dict], detected: list[dict], match_mode: str) -> dict:
        """è¨ˆç®—å„ PHI é¡å‹çš„æŒ‡æ¨™"""
        # æŒ‰é¡å‹åˆ†çµ„
        gt_by_type = defaultdict(list)
        det_by_type = defaultdict(list)

        for phi in ground_truth:
            gt_by_type[phi['type']].append(phi)

        for phi in detected:
            det_by_type[phi['type']].append(phi)

        # è¨ˆç®—å„é¡å‹æŒ‡æ¨™
        type_metrics = {}
        all_types = set(gt_by_type.keys()) | set(det_by_type.keys())

        for phi_type in all_types:
            gt_list = gt_by_type[phi_type]
            det_list = det_by_type[phi_type]

            if match_mode == 'exact':
                gt_set = {phi['content'].lower() for phi in gt_list}
                det_set = {phi['content'].lower() for phi in det_list}
                tp = len(gt_set & det_set)
                fp = len(det_set - gt_set)
                fn = len(gt_set - det_set)
            else:
                # ç°¡åŒ–è¨ˆç®—
                tp = min(len(gt_list), len(det_list))
                fp = max(0, len(det_list) - len(gt_list))
                fn = max(0, len(gt_list) - len(det_list))

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            type_metrics[phi_type] = {
                'TP': tp,
                'FP': fp,
                'FN': fn,
                'Precision': precision,
                'Recall': recall,
                'F1': f1,
                'Ground_Truth_Count': len(gt_list),
                'Detected_Count': len(det_list)
            }

        return type_metrics

    def evaluate_from_excel(self, excel_path: str, detected_phi_dict: dict[str, list[dict]]) -> dict:
        """
        å¾ Excel æ–‡ä»¶è©•ä¼°æª¢æ¸¬çµæœ
        
        Args:
            excel_path: å¸¶ PHI æ¨™è¨˜çš„æ¸¬è©¦æ–‡ä»¶è·¯å¾‘
            detected_phi_dict: ç³»çµ±æª¢æ¸¬çµæœï¼Œæ ¼å¼ï¼š{case_id: [phi_list]}
        
        Returns:
            è©•ä¼°çµæœ
        """
        df = pd.read_excel(excel_path)

        overall_metrics = {
            'TP': 0, 'FP': 0, 'FN': 0,
            'by_case': {}
        }

        for idx, row in df.iterrows():
            case_id = row['Case ID']

            # åˆä½µæ‰€æœ‰æ–‡æœ¬åˆ—
            text_columns = [col for col in df.columns[1:5]]  # æ’é™¤ Case ID å’Œ PHI Count
            full_text = ' '.join([str(row[col]) for col in text_columns if pd.notna(row[col])])

            # æå–æ¨™æº–ç­”æ¡ˆ
            ground_truth = self.parse_ground_truth(full_text)

            # ç²å–ç³»çµ±æª¢æ¸¬çµæœ
            detected = detected_phi_dict.get(case_id, [])

            # è¨ˆç®—æŒ‡æ¨™
            metrics = self.calculate_metrics(ground_truth, detected, match_mode='partial')

            overall_metrics['TP'] += metrics['TP']
            overall_metrics['FP'] += metrics['FP']
            overall_metrics['FN'] += metrics['FN']
            overall_metrics['by_case'][case_id] = metrics

        # è¨ˆç®—ç¸½é«”æŒ‡æ¨™
        tp = overall_metrics['TP']
        fp = overall_metrics['FP']
        fn = overall_metrics['FN']

        overall_metrics['Precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0
        overall_metrics['Recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0
        overall_metrics['F1'] = (
            2 * overall_metrics['Precision'] * overall_metrics['Recall'] /
            (overall_metrics['Precision'] + overall_metrics['Recall'])
            if (overall_metrics['Precision'] + overall_metrics['Recall']) > 0 else 0
        )

        return overall_metrics

    def print_evaluation_report(self, metrics: dict, title: str = "PHI Detection Evaluation Report"):
        """æ‰“å°è©•ä¼°å ±å‘Š"""
        print("\n" + "="*70)
        print(title)
        print("="*70)

        print("\nğŸ“Š Overall Metrics:")
        print(f"  True Positives (TP):  {metrics['TP']:4d}")
        print(f"  False Positives (FP): {metrics['FP']:4d}")
        print(f"  False Negatives (FN): {metrics['FN']:4d}")
        print(f"\n  Precision: {metrics['Precision']:.2%}")
        print(f"  Recall:    {metrics['Recall']:.2%}")
        print(f"  F1 Score:  {metrics['F1']:.2%}")

        if 'by_case' in metrics:
            print("\nğŸ“‹ By Case:")
            for case_id, case_metrics in metrics['by_case'].items():
                print(f"\n  {case_id}:")
                print(f"    TP={case_metrics['TP']}, FP={case_metrics['FP']}, FN={case_metrics['FN']}")
                print(f"    Precision={case_metrics['Precision']:.2%}, "
                      f"Recall={case_metrics['Recall']:.2%}, "
                      f"F1={case_metrics['F1']:.2%}")

        if 'by_type' in metrics:
            print("\nğŸ“Œ By PHI Type:")
            for phi_type, type_metrics in sorted(metrics['by_type'].items()):
                print(f"\n  {phi_type}:")
                print(f"    Ground Truth: {type_metrics['Ground_Truth_Count']}, "
                      f"Detected: {type_metrics['Detected_Count']}")
                print(f"    TP={type_metrics['TP']}, FP={type_metrics['FP']}, FN={type_metrics['FN']}")
                print(f"    Precision={type_metrics['Precision']:.2%}, "
                      f"Recall={type_metrics['Recall']:.2%}, "
                      f"F1={type_metrics['F1']:.2%}")


if __name__ == "__main__":
    # ä½¿ç”¨ç¯„ä¾‹
    evaluator = PHIEvaluator()

    # æ¸¬è©¦æ¨™è¨˜è§£æ
    test_text = """
    Patient ã€PHI:NAME:P001ã€‘é™³è€å…ˆç”Ÿã€/PHIã€‘, age ã€PHI:AGE_OVER_89:A001ã€‘94ã€/PHIã€‘, 
    contact ã€PHI:PHONE:T001ã€‘02-2758-9999ã€/PHIã€‘
    """

    ground_truth = evaluator.parse_ground_truth(test_text)
    print("âœ… Parsed Ground Truth PHI:")
    for phi in ground_truth:
        print(f"  - {phi['type']}: {phi['content']} (ID: {phi['id']})")

    # æ¨¡æ“¬ç³»çµ±æª¢æ¸¬çµæœ
    detected = [
        {'type': 'NAME', 'content': 'é™³è€å…ˆç”Ÿ'},
        {'type': 'AGE_OVER_89', 'content': '94'},
        # ç¼ºå°‘ PHONE (False Negative)
        {'type': 'DATE', 'content': '2024'},  # å¤šæª¢æ¸¬äº†ä¸€å€‹ (False Positive)
    ]

    # è¨ˆç®—æŒ‡æ¨™
    metrics = evaluator.calculate_metrics(ground_truth, detected, match_mode='exact')

    evaluator.print_evaluation_report(metrics, "Demo Evaluation")

    print("\n\n" + "="*70)
    print("ğŸ’¡ ä½¿ç”¨èªªæ˜:")
    print("="*70)
    print("\n1. è¼‰å…¥å¸¶æ¨™è¨˜çš„æ¸¬è©¦æ–‡ä»¶:")
    print("   df = pd.read_excel('data/test/test_phi_tagged_cases.xlsx')")
    print("\n2. åŸ·è¡Œ PHI æª¢æ¸¬ç³»çµ±ï¼Œç²å¾—æª¢æ¸¬çµæœ:")
    print("   detected_phi_dict = {")
    print("       'CASE-001': [{'type': 'NAME', 'content': 'é™³è€å…ˆç”Ÿ'}, ...],")
    print("       'CASE-002': [...],")
    print("       ...")
    print("   }")
    print("\n3. è©•ä¼°æª¢æ¸¬æ•ˆèƒ½:")
    print("   evaluator = PHIEvaluator()")
    print("   results = evaluator.evaluate_from_excel(")
    print("       'data/test/test_phi_tagged_cases.xlsx',")
    print("       detected_phi_dict")
    print("   )")
    print("   evaluator.print_evaluation_report(results)")
