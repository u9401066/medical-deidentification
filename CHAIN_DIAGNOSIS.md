# PHI Identification Chain è¨ºæ–·å ±å‘Š

## ğŸ” å•é¡Œè¨ºæ–·

### 1. **æ²’æœ‰ä½¿ç”¨ LangChain çš„ Chain æ©Ÿåˆ¶**

**ç¾æ³ï¼š**
```python
# phi_identification_chain.py
class PHIIdentificationChain:  # âŒ åªæ˜¯æ™®é€š Python class
    def __init__(self, ...):
        self.llm = create_llm(...)  # ç›´æ¥æŒæœ‰ LLM
    
    def identify_phi(self, text):
        prompt = template.format(context=context, text=text)  # âŒ å­—ä¸²æ‹¼æ¥
        response = self.llm.invoke(prompt)  # âŒ ç›´æ¥èª¿ç”¨
```

**å•é¡Œï¼š**
- æ²’æœ‰ç¹¼æ‰¿ `Chain` æˆ–ä½¿ç”¨ LCEL (LangChain Expression Language)
- æ²’æœ‰ä½¿ç”¨ `PromptTemplate` æˆ– `ChatPromptTemplate`
- æ²’æœ‰ token é™åˆ¶æˆ–è‡ªå‹• truncation
- æ²’æœ‰ä½¿ç”¨ LangChain çš„ document processing chains

### 2. **Context å†—é¤˜å•é¡Œ**

**ç¾æ³ï¼š**
```python
# å³ä½¿ retrieve_regulation_context=False
context = DEFAULT_HIPAA_SAFE_HARBOR_RULES  # 800 å­—å…ƒ

# æ¯æ¬¡è«‹æ±‚éƒ½ç™¼é€ï¼š
prompt = f"""
Regulations:
{context}  # 800 å­—å…ƒ

Medical Text:
{text}  # 1400+ å­—å…ƒ

Instructions: ...  # 200+ å­—å…ƒ
"""
# ç¸½è¨ˆ: 2000+ å­—å…ƒ (~1500 tokens)
```

**å¯¦éš›æ¸¬è©¦çµæœï¼š**
- ç°¡å–® promptï¼ˆ253 å­—å…ƒï¼‰ï¼šâœ… 6.5 ç§’
- å¯¦éš› promptï¼ˆ1579 å­—å…ƒï¼‰ï¼šâŒ >150 ç§’ï¼ˆè¶…æ™‚ï¼‰

### 3. **åˆ†å¡Šç­–ç•¥å•é¡Œ**

**ç¾æ³ï¼š**
```python
def _identify_phi_chunked(self, text, ...):
    # ç²å– context ä¸€æ¬¡
    context = DEFAULT_HIPAA_SAFE_HARBOR_RULES  # 800 å­—å…ƒ
    
    # ä½†æ¯å€‹ chunk éƒ½è¦å¸¶è‘—å®Œæ•´ context
    for chunk in chunks:
        entities = self._identify_phi_structured(
            text=chunk,      # ä¾‹å¦‚ 300 å­—å…ƒ
            context=context, # âŒ æ¯æ¬¡éƒ½æ˜¯ 800 å­—å…ƒ
            language=language
        )
        # æ¯æ¬¡è«‹æ±‚ = 800 (context) + 300 (chunk) + 200 (instructions)
        #           = 1300+ å­—å…ƒ
```

**å•é¡Œï¼š**
- åˆ†å¡Šç„¡æ³•æ¸›å°‘ prompt é•·åº¦ï¼ˆcontext ä»ç„¶å¾ˆé•·ï¼‰
- æ²’æœ‰ä½¿ç”¨ LangChain çš„ MapReduce æˆ– Refine æ¨¡å¼
- ç„¡æ³•åˆ©ç”¨ LangChain çš„æ–‡æª”å£“ç¸®æˆ–æ‘˜è¦åŠŸèƒ½

## âœ… è§£æ±ºæ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šç°¡åŒ– Contextï¼ˆç«‹å³ä¿®å¾©ï¼‰

**ä¿®æ”¹ `phi_identification_chain.py`ï¼š**

```python
# ç¬¬ 215-216 è¡Œ
else:
    # ä½¿ç”¨æ¥µç°¡ context
    context = "Identify PHI: names, dates, IDs, locations, ages >89."
    # å¾ 800 å­—å…ƒ â†’ 60 å­—å…ƒï¼ˆæ¸›å°‘ ~92%ï¼‰
```

**é æœŸæ•ˆæœï¼š**
- Prompt å¾ 1579 å­—å…ƒ â†’ ~839 å­—å…ƒ
- è™•ç†æ™‚é–“å¾ >150 ç§’ â†’ ~10-15 ç§’

### æ–¹æ¡ˆ 2ï¼šé‡æ§‹ç‚º LangChain LCEL Chainï¼ˆä¸­æœŸå„ªåŒ–ï¼‰

**å»ºè­°å¯¦ç¾ï¼š**

```python
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

class PHIIdentificationChain:
    def __init__(self, ...):
        # ä½¿ç”¨ ChatPromptTemplate
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a PHI identification expert."),
            ("user", "Identify PHI in: {text}")  # ç°¡åŒ– prompt
        ])
        
        # ä½¿ç”¨ LCEL chain
        self.chain = (
            RunnablePassthrough.assign(
                context=lambda x: self._get_context(x["text"])  # å‹•æ…‹ context
            )
            | self.prompt
            | self.llm.with_structured_output(PHIDetectionResponse)
        )
    
    def identify_phi(self, text):
        return self.chain.invoke({"text": text})
```

### æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ LangChain Document Chainsï¼ˆé•·æœŸå„ªåŒ–ï¼‰

**æ­£ç¢ºçš„ MapReduce æµç¨‹ï¼š**

```
è¼¸å…¥: é•·æ–‡æœ¬ (ä¾‹å¦‚ 10000 å­—å…ƒ)
  â†“
åˆ†æ®µ: [Chunk1, Chunk2, Chunk3, ...]
  â†“
Map éšæ®µ: æ¯å€‹ chunk â†’ PHI å¯¦é«”åˆ—è¡¨
  Chunk1 â†’ [PHI1, PHI2, PHI3]          # åªè¼¸å‡º PHIï¼Œä¸åŒ…å«åŸæ–‡
  Chunk2 â†’ [PHI4, PHI5]
  Chunk3 â†’ [PHI6]
  â†“
Reduce éšæ®µ: å½™æ•´æ‰€æœ‰ PHI
  åˆä½µ â†’ [PHI1, PHI2, PHI3, PHI4, PHI5, PHI6]
  å»é‡ â†’ ç§»é™¤é‡è¤‡å¯¦é«”
  èª¿æ•´ä½ç½® â†’ ä¿®æ­£ start_pos/end_pos ç‚ºåŸæ–‡ä¸­çš„çµ•å°ä½ç½®
  â†“
è¼¸å‡º: å®Œæ•´ PHI å¯¦é«”åˆ—è¡¨
```

**å¯¦ç¾ä»£ç¢¼ï¼š**

```python
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from typing import List

class PHIIdentificationChain:
    def _build_map_chain(self):
        """Map: å¾å–®å€‹ chunk è­˜åˆ¥ PHIï¼ˆåªè¼¸å‡º PHI å¯¦é«”ï¼‰"""
        map_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a PHI identification expert. Extract ONLY PHI entities."),
            ("user", """Identify all PHI in this text section:

{page_content}

Return ONLY the PHI entities found, not the full text.""")
        ])
        
        # Map chain: Document â†’ PHIDetectionResponse
        return (
            map_prompt 
            | self.llm.with_structured_output(PHIDetectionResponse)
        )
    
    def _merge_phi_results(
        self, 
        chunk_results: List[Tuple[PHIDetectionResponse, int, str]]
    ) -> List[PHIEntity]:
        """Reduce: å½™æ•´æ‰€æœ‰ chunk çš„ PHI å¯¦é«”"""
        all_entities = []
        
        for detection_response, chunk_start_pos, chunk_text in chunk_results:
            for result in detection_response.entities:
                # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å¯¦é«”çš„çµ•å°ä½ç½®
                entity_start = chunk_text.find(result.entity_text, 0)
                if entity_start != -1:
                    absolute_start = chunk_start_pos + entity_start
                    absolute_end = absolute_start + len(result.entity_text)
                    
                    # å‰µå»ºèª¿æ•´ä½ç½®å¾Œçš„å¯¦é«”
                    entity = result.to_phi_entity()
                    adjusted_entity = replace(
                        entity,
                        start_pos=absolute_start,
                        end_pos=absolute_end
                    )
                    all_entities.append(adjusted_entity)
        
        # å»é‡
        unique_entities = self._deduplicate_entities(all_entities)
        return unique_entities
    
    def _identify_phi_with_map_reduce(
        self,
        text: str,
        language: Optional[str] = None
    ) -> List[PHIEntity]:
        """ä½¿ç”¨ MapReduce æ¨¡å¼è™•ç†é•·æ–‡æœ¬"""
        
        # 1. åˆ†æ®µ
        chunks = self.medical_retriever._split_text(text)
        logger.info(f"Split into {len(chunks)} chunks for MapReduce processing")
        
        # 2. ç²å– contextï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
        context = self._get_minimal_context()
        
        # 3. Map: è™•ç†æ¯å€‹ chunk
        map_chain = self._build_map_chain()
        chunk_results = []
        current_pos = 0
        
        for i, chunk in enumerate(chunks):
            logger.debug(f"Map: Processing chunk {i+1}/{len(chunks)}")
            
            # å‰µå»º Documentï¼ˆLangChain æ¨™æº–æ ¼å¼ï¼‰
            doc = Document(
                page_content=chunk,
                metadata={"chunk_index": i, "start_position": current_pos}
            )
            
            # åŸ·è¡Œ map chainï¼ˆåªè¼¸å‡º PHIï¼‰
            detection_response = map_chain.invoke({"page_content": chunk})
            chunk_results.append((detection_response, current_pos, chunk))
            
            current_pos += len(chunk)
        
        # 4. Reduce: å½™æ•´çµæœ
        logger.debug(f"Reduce: Merging {len(chunk_results)} chunk results")
        unique_entities = self._merge_phi_results(chunk_results)
        
        logger.success(f"MapReduce complete: {len(unique_entities)} unique PHI entities")
        return unique_entities
```

**é—œéµæ”¹é€²ï¼š**

1. âœ… **Map éšæ®µåªè¼¸å‡º PHI**ï¼šæ¯å€‹ chunk çš„è™•ç†çµæœæ˜¯ `PHIDetectionResponse`ï¼ˆPHI åˆ—è¡¨ï¼‰ï¼Œä¸åŒ…å«åŸæ–‡
2. âœ… **æ¸›å°‘ prompt é•·åº¦**ï¼šæ¯æ¬¡åªè™•ç†ä¸€å€‹ chunkï¼ˆ~500-2000 å­—å…ƒï¼‰ï¼Œä¸éœ€è¦å®Œæ•´æ–‡æª”
3. âœ… **Reduce éšæ®µç´”æ•¸æ“šè™•ç†**ï¼šåˆä½µ PHI åˆ—è¡¨ï¼Œå»é‡ï¼Œèª¿æ•´ä½ç½®ï¼ˆä¸éœ€è¦ LLMï¼‰
4. âœ… **ä½ç½®ä¿®æ­£**ï¼šè¿½è¹¤æ¯å€‹ chunk çš„èµ·å§‹ä½ç½®ï¼Œå°‡ç›¸å°ä½ç½®è½‰æ›ç‚ºçµ•å°ä½ç½®

## ğŸ“Š æ•ˆèƒ½å°æ¯”

| æ–¹æ¡ˆ | Prompt é•·åº¦ | é æœŸæ™‚é–“ | å¯¦ç¾é›£åº¦ | æ¨è–¦åº¦ |
|------|------------|----------|---------|--------|
| æ–¹æ¡ˆ 1: ç°¡åŒ– Context | ~839 å­—å…ƒ | ~10-15 ç§’ | â­ ç°¡å–® | â­â­â­â­â­ |
| æ–¹æ¡ˆ 2: LCEL Chain | ~500 å­—å…ƒ | ~5-8 ç§’ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ |
| æ–¹æ¡ˆ 3: MapReduce | ~300 å­—å…ƒ/chunk | ~8-12 ç§’ | â­â­â­â­â­ è¤‡é›œ | â­â­â­ |

## ğŸ¯ å»ºè­°åŸ·è¡Œé †åº

### ç«‹å³åŸ·è¡Œï¼ˆæ–¹æ¡ˆ 1ï¼‰

**ä¿®æ”¹ 2 è™•ï¼š**

1. `phi_identification_chain.py:216`ï¼ˆ`_identify_phi_direct`ï¼‰
2. `phi_identification_chain.py:297`ï¼ˆ`_identify_phi_chunked`ï¼‰

```python
context = "Identify PHI: names, dates, IDs, locations, ages >89."
```

**æ¸¬è©¦ï¼š**
```bash
python examples/simple_batch_test.py
```

### å¾ŒçºŒå„ªåŒ–ï¼ˆæ–¹æ¡ˆ 2ï¼‰

1. é‡æ§‹ç‚º LCEL chain
2. ä½¿ç”¨ `ChatPromptTemplate`
3. æ·»åŠ  token counting å’Œ truncation
4. ä½¿ç”¨ LangChain çš„ callbacks é€²è¡Œç›£æ§

### æœªä¾†å¢å¼·ï¼ˆæ–¹æ¡ˆ 3ï¼‰

1. å¯¦ç¾ MapReduce pattern for long documents
2. æ·»åŠ  intermediate result caching
3. æ”¯æ´ streaming output
4. ä½¿ç”¨ LangSmith é€²è¡Œ tracing

## ğŸ”§ ç«‹å³ä¿®å¾©ä»£ç¢¼

**æª”æ¡ˆï¼š** `medical_deidentification/infrastructure/rag/phi_identification_chain.py`

**ä½ç½® 1ï¼ˆç¬¬ 186-216 è¡Œï¼‰ï¼š**
```python
def _identify_phi_direct(self, text, language, return_source, return_entities):
    regulation_docs = []
    context = ""
    
    if self.config.retrieve_regulation_context and self.regulation_chain:
        # ... åŸæœ‰é‚è¼¯ ...
        context = "\n\n".join([...])
    else:
        # âœ… ä¿®æ”¹é€™è£¡
        context = "Identify PHI according to HIPAA Safe Harbor: names, dates, geographic locations, phone numbers, email addresses, SSNs, medical record numbers, account numbers, certificate numbers, vehicle/device IDs, URLs, IP addresses, biometric identifiers, photos, unique identifying codes, and ages over 89 years."
```

**ä½ç½® 2ï¼ˆç¬¬ 276-297 è¡Œï¼‰ï¼š**
```python
def _identify_phi_chunked(self, text, language, return_source, return_entities):
    regulation_docs = []
    context = ""
    
    if self.config.retrieve_regulation_context and self.regulation_chain:
        # ... åŸæœ‰é‚è¼¯ ...
        context = "\n\n".join([...])
    else:
        # âœ… ä¿®æ”¹é€™è£¡
        context = "Identify PHI according to HIPAA Safe Harbor: names, dates, geographic locations, phone numbers, email addresses, SSNs, medical record numbers, account numbers, certificate numbers, vehicle/device IDs, URLs, IP addresses, biometric identifiers, photos, unique identifying codes, and ages over 89 years."
```

## ğŸ“ˆ é æœŸæ”¹å–„

**ä¿®æ”¹å‰ï¼š**
- Prompt: 1579 å­—å…ƒ
- æ™‚é–“: >150 ç§’ï¼ˆè¶…æ™‚ï¼‰
- æˆåŠŸç‡: 0%

**ä¿®æ”¹å¾Œï¼š**
- Prompt: ~800-900 å­—å…ƒï¼ˆæ¸›å°‘ 45%ï¼‰
- æ™‚é–“: ~10-20 ç§’
- æˆåŠŸç‡: ~90%+

## ğŸš€ åŸ·è¡Œä¿®å¾©

```bash
# 1. æ‡‰ç”¨ä¿®å¾©
# ç·¨è¼¯ phi_identification_chain.pyï¼ˆè¦‹ä¸Šæ–¹ä»£ç¢¼ï¼‰

# 2. æ¸¬è©¦
python test_batch_scenario.py  # æ‡‰è©²åœ¨ 10-20 ç§’å…§å®Œæˆ

# 3. åŸ·è¡Œå¯¦éš›æ‰¹æ¬¡è™•ç†
python examples/simple_batch_test.py  # æ‡‰è©²èƒ½æˆåŠŸå®Œæˆ

# 4. æäº¤ä¿®å¾©
git add medical_deidentification/infrastructure/rag/phi_identification_chain.py
git commit -m "fix: Reduce context length to fix Ollama timeout issue"
```
