"""
Medical De-identification Web API
FastAPI å¾Œç«¯æœå‹™
"""
import json

# ç¢ºä¿å¯ä»¥ import ä¸»å°ˆæ¡ˆæ¨¡çµ„
import sys
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import Any

from fastapi import BackgroundTasks, FastAPI, File, HTTPException, Query, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from loguru import logger

# Import PHIType for type handling
try:
    from core.domain import PHIType
except ImportError:
    PHIType = None  # Fallback if not available

# è¨­å®šè³‡æ–™ç›®éŒ„
DATA_DIR = Path(__file__).parent / "data"
UPLOAD_DIR = DATA_DIR / "uploads"
RESULTS_DIR = DATA_DIR / "results"
REPORTS_DIR = DATA_DIR / "reports"
REGULATIONS_DIR = DATA_DIR / "regulations"

for d in [UPLOAD_DIR, RESULTS_DIR, REPORTS_DIR, REGULATIONS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# ä»»å‹™ç‹€æ…‹å­˜å„²
tasks_db: dict[str, dict[str, Any]] = {}


def format_time(seconds: float | None) -> str:
    """æ ¼å¼åŒ–æ™‚é–“ç‚ºäººé¡å¯è®€æ ¼å¼"""
    if seconds is None or seconds < 0:
        return "è¨ˆç®—ä¸­..."
    if seconds < 60:
        return f"{seconds:.0f} ç§’"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins} åˆ† {secs} ç§’"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        return f"{hours} å°æ™‚ {mins} åˆ†"


# ============================================================
# Pydantic Models
# ============================================================

class PHIConfig(BaseModel):
    """PHI è™•ç†é…ç½®"""
    masking_type: str = Field(default="redact", description="redact, hash, pseudonymize")
    phi_types: list[str] = Field(default_factory=lambda: [
        "NAME", "DATE", "PHONE", "EMAIL", "ADDRESS", "ID_NUMBER", "MEDICAL_RECORD"
    ])
    preserve_format: bool = Field(default=True)
    custom_patterns: dict[str, str] | None = None


class ProcessRequest(BaseModel):
    """è™•ç†è«‹æ±‚"""
    file_ids: list[str]
    config: PHIConfig | None = None
    job_name: str | None = None


class TaskStatus(BaseModel):
    """ä»»å‹™ç‹€æ…‹"""
    task_id: str
    status: str  # pending, processing, completed, failed
    progress: float = 0.0
    message: str = ""
    created_at: datetime
    completed_at: datetime | None = None
    result_file: str | None = None
    report_file: str | None = None
    # è¨ˆæ™‚ç›¸é—œ
    started_at: datetime | None = None
    elapsed_seconds: float | None = None
    estimated_remaining_seconds: float | None = None
    processing_speed: float | None = None  # chars per second
    total_chars: int | None = None
    processed_chars: int | None = None


class RegulationRule(BaseModel):
    """æ³•è¦è¦å‰‡"""
    id: str
    name: str
    description: str
    phi_types: list[str]
    source: str  # hipaa, taiwan_pdpa, custom
    enabled: bool = True


class UploadedFile(BaseModel):
    """ä¸Šå‚³çš„æª”æ¡ˆè³‡è¨Š"""
    file_id: str
    filename: str
    size: int
    upload_time: datetime
    file_type: str
    preview_available: bool = True


# ============================================================
# Application Setup
# ============================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç†"""
    logger.info("ğŸš€ Medical De-identification Web API starting...")
    # è¼‰å…¥å·²æœ‰çš„ä»»å‹™ç‹€æ…‹
    tasks_file = DATA_DIR / "tasks.json"
    if tasks_file.exists():
        try:
            with open(tasks_file) as f:
                saved_tasks = json.load(f)
                for task_id, task_data in saved_tasks.items():
                    task_data["created_at"] = datetime.fromisoformat(task_data["created_at"])
                    if task_data.get("completed_at"):
                        task_data["completed_at"] = datetime.fromisoformat(task_data["completed_at"])
                    tasks_db[task_id] = task_data
            logger.info(f"Loaded {len(tasks_db)} existing tasks")
        except Exception as e:
            logger.warning(f"Could not load tasks: {e}")

    yield

    # ä¿å­˜ä»»å‹™ç‹€æ…‹
    logger.info("ğŸ’¾ Saving tasks state...")
    try:
        serializable = {}
        for task_id, task in tasks_db.items():
            serializable[task_id] = {
                **task,
                "created_at": task["created_at"].isoformat(),
                "completed_at": task["completed_at"].isoformat() if task.get("completed_at") else None
            }
        with open(tasks_file, "w") as f:
            json.dump(serializable, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Could not save tasks: {e}")


app = FastAPI(
    title="Medical De-identification API",
    description="é†«ç™‚æ–‡æœ¬å»è­˜åˆ¥åŒ– Web API",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# File Upload & Download APIs
# ============================================================

@app.post("/api/upload", response_model=UploadedFile)
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šå‚³æª”æ¡ˆ"""
    file_id = str(uuid.uuid4())[:8]
    file_ext = Path(file.filename).suffix.lower()

    # æ”¯æ´çš„æª”æ¡ˆé¡å‹
    supported_types = {".csv", ".xlsx", ".xls", ".txt", ".json", ".docx", ".pdf"}
    if file_ext not in supported_types:
        raise HTTPException(400, f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_ext}. æ”¯æ´: {supported_types}")

    # å„²å­˜æª”æ¡ˆ
    save_path = UPLOAD_DIR / f"{file_id}{file_ext}"
    content = await file.read()

    with open(save_path, "wb") as f:
        f.write(content)

    # å„²å­˜å…ƒæ•¸æ“š
    metadata = {
        "file_id": file_id,
        "filename": file.filename,
        "size": len(content),
        "upload_time": datetime.now().isoformat(),
        "file_type": file_ext[1:],
        "path": str(save_path),
    }

    with open(UPLOAD_DIR / f"{file_id}.meta.json", "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    logger.info(f"ğŸ“ Uploaded file: {file.filename} -> {file_id}")

    return UploadedFile(
        file_id=file_id,
        filename=file.filename,
        size=len(content),
        upload_time=datetime.now(),
        file_type=file_ext[1:],
    )


@app.get("/api/files", response_model=list[UploadedFile])
async def list_files():
    """åˆ—å‡ºæ‰€æœ‰ä¸Šå‚³çš„æª”æ¡ˆ"""
    files = []
    for meta_file in UPLOAD_DIR.glob("*.meta.json"):
        with open(meta_file) as f:
            meta = json.load(f)
            files.append(UploadedFile(
                file_id=meta["file_id"],
                filename=meta["filename"],
                size=meta["size"],
                upload_time=datetime.fromisoformat(meta["upload_time"]),
                file_type=meta["file_type"],
            ))
    return sorted(files, key=lambda x: x.upload_time, reverse=True)


@app.delete("/api/files/{file_id}")
async def delete_file(file_id: str):
    """åˆªé™¤æª”æ¡ˆ"""
    meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
    if not meta_file.exists():
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    with open(meta_file) as f:
        meta = json.load(f)

    # åˆªé™¤æª”æ¡ˆå’Œå…ƒæ•¸æ“š
    Path(meta["path"]).unlink(missing_ok=True)
    meta_file.unlink()

    logger.info(f"ğŸ—‘ï¸ Deleted file: {file_id}")
    return {"message": "å·²åˆªé™¤"}


@app.get("/api/download/{file_id}")
async def download_result(file_id: str, file_type: str = Query("result", enum=["result", "report"])):
    """ä¸‹è¼‰è™•ç†çµæœæˆ–å ±å‘Š"""
    if file_type == "result":
        search_dir = RESULTS_DIR
    else:
        search_dir = REPORTS_DIR

    # æ‰¾åˆ°å°æ‡‰çš„æª”æ¡ˆ
    matching_files = list(search_dir.glob(f"{file_id}*"))
    if not matching_files:
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    file_path = matching_files[0]
    return FileResponse(
        file_path,
        filename=file_path.name,
        media_type="application/octet-stream"
    )


# ============================================================
# Data Preview APIs
# ============================================================

@app.get("/api/preview/{file_id}")
async def preview_file(
    file_id: str,
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100)
):
    """é è¦½ä¸Šå‚³çš„æª”æ¡ˆå…§å®¹"""
    meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
    if not meta_file.exists():
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    with open(meta_file) as f:
        meta = json.load(f)

    file_path = Path(meta["path"])
    file_type = meta["file_type"]

    try:
        if file_type in ["csv", "xlsx", "xls"]:
            return await _preview_tabular(file_path, file_type, page, page_size)
        elif file_type == "txt":
            return await _preview_text(file_path, page, page_size)
        elif file_type == "json":
            return await _preview_json(file_path, page, page_size)
        else:
            return {"message": f"é è¦½ä¸æ”¯æ´ {file_type} æ ¼å¼", "preview_available": False}
    except Exception as e:
        logger.error(f"Preview error: {e}")
        raise HTTPException(500, f"é è¦½å¤±æ•—: {e!s}")


async def _preview_tabular(file_path: Path, file_type: str, page: int, page_size: int):
    """é è¦½è¡¨æ ¼è³‡æ–™"""
    import pandas as pd

    if file_type == "csv":
        df = pd.read_csv(file_path, nrows=page * page_size + page_size)
    else:
        df = pd.read_excel(file_path, nrows=page * page_size + page_size)

    total_rows = len(df)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size

    page_df = df.iloc[start_idx:end_idx]

    return {
        "type": "tabular",
        "columns": list(df.columns),
        "data": page_df.fillna("").to_dict(orient="records"),
        "total_rows": total_rows,
        "page": page,
        "page_size": page_size,
        "has_more": end_idx < total_rows,
    }


async def _preview_text(file_path: Path, page: int, page_size: int):
    """é è¦½æ–‡å­—æª”"""
    with open(file_path, encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    total_lines = len(lines)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size

    return {
        "type": "text",
        "lines": [line.rstrip() for line in lines[start_idx:end_idx]],
        "total_lines": total_lines,
        "page": page,
        "page_size": page_size,
        "has_more": end_idx < total_lines,
    }


async def _preview_json(file_path: Path, page: int, page_size: int):
    """é è¦½ JSON æª”æ¡ˆ"""
    with open(file_path, encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, list):
        total_items = len(data)
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        return {
            "type": "json_array",
            "data": data[start_idx:end_idx],
            "total_items": total_items,
            "page": page,
            "page_size": page_size,
            "has_more": end_idx < total_items,
        }
    else:
        return {
            "type": "json_object",
            "data": data,
            "total_items": 1,
            "page": 1,
            "page_size": 1,
            "has_more": False,
        }


# ============================================================
# PHI Processing APIs
# ============================================================

@app.post("/api/process", response_model=TaskStatus)
async def start_processing(request: ProcessRequest, background_tasks: BackgroundTasks):
    """é–‹å§‹ PHI è™•ç†ä»»å‹™"""
    task_id = str(uuid.uuid4())[:8]

    # é©—è­‰æª”æ¡ˆå­˜åœ¨
    for file_id in request.file_ids:
        meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
        if not meta_file.exists():
            raise HTTPException(404, f"æª”æ¡ˆä¸å­˜åœ¨: {file_id}")

    # å»ºç«‹ä»»å‹™
    task = {
        "task_id": task_id,
        "status": "pending",
        "progress": 0.0,
        "message": "ä»»å‹™å·²å»ºç«‹ï¼Œç­‰å¾…è™•ç†...",
        "created_at": datetime.now(),
        "completed_at": None,
        "result_file": None,
        "report_file": None,
        "file_ids": request.file_ids,
        "config": request.config.model_dump() if request.config else PHIConfig().model_dump(),
        "job_name": request.job_name or f"job-{task_id}",
    }
    tasks_db[task_id] = task

    # èƒŒæ™¯åŸ·è¡Œè™•ç†
    background_tasks.add_task(process_phi_task, task_id)

    logger.info(f"ğŸš€ Created task: {task_id} for files: {request.file_ids}")

    return TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})


# è™•ç†é€Ÿåº¦çµ±è¨ˆï¼ˆç”¨æ–¼é ä¼°æ™‚é–“ï¼‰
processing_stats = {
    "total_chars_processed": 0,
    "total_time_seconds": 0.0,
    "task_count": 0,
    "avg_chars_per_second": 50.0,  # åˆå§‹ä¼°è¨ˆå€¼ï¼ˆåŸºæ–¼ LLM æ¨ç†é€Ÿåº¦ï¼‰
}


def estimate_remaining_time(total_chars: int, processed_chars: int, elapsed: float) -> float | None:
    """ä¼°è¨ˆå‰©é¤˜æ™‚é–“"""
    if processed_chars <= 0 or elapsed <= 0:
        # ä½¿ç”¨æ­·å²å¹³å‡å€¼ä¼°è¨ˆ
        if processing_stats["avg_chars_per_second"] > 0:
            return (total_chars - processed_chars) / processing_stats["avg_chars_per_second"]
        return None

    # åŸºæ–¼ç•¶å‰é€Ÿåº¦ä¼°è¨ˆ
    current_speed = processed_chars / elapsed
    remaining_chars = total_chars - processed_chars
    return remaining_chars / current_speed if current_speed > 0 else None


def update_processing_stats(chars_processed: int, time_seconds: float):
    """æ›´æ–°è™•ç†é€Ÿåº¦çµ±è¨ˆ"""
    global processing_stats
    if chars_processed > 0 and time_seconds > 0:
        processing_stats["total_chars_processed"] += chars_processed
        processing_stats["total_time_seconds"] += time_seconds
        processing_stats["task_count"] += 1
        processing_stats["avg_chars_per_second"] = (
            processing_stats["total_chars_processed"] /
            processing_stats["total_time_seconds"]
        )
        logger.info(f"ğŸ“Š Updated processing stats: avg speed = {processing_stats['avg_chars_per_second']:.2f} chars/sec")


async def process_phi_task(task_id: str):
    """èƒŒæ™¯åŸ·è¡Œ PHI è™•ç†"""
    task = tasks_db[task_id]
    task["status"] = "processing"
    task["message"] = "æ­£åœ¨è™•ç†..."
    task["started_at"] = datetime.now()
    task["elapsed_seconds"] = 0.0

    try:
        # è¼‰å…¥è™•ç†å¼•æ“ï¼ˆå¿…é ˆæˆåŠŸï¼‰
        from core.application.processing.engine import DeidentificationEngine, EngineConfig
        logger.info("âœ… DeidentificationEngine loaded successfully")

        file_ids = task["file_ids"]
        config = PHIConfig(**task["config"])
        results = []

        # è¨ˆç®—ç¸½å­—ç¬¦æ•¸ç”¨æ–¼é ä¼°æ™‚é–“
        total_chars = 0
        file_chars = {}
        for file_id in file_ids:
            meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
            if meta_file.exists():
                with open(meta_file) as f:
                    meta = json.load(f)
                file_path = Path(meta["path"])
                if file_path.exists():
                    try:
                        content = file_path.read_text(encoding="utf-8", errors="ignore")
                        chars = len(content)
                        file_chars[file_id] = chars
                        total_chars += chars
                    except:
                        file_chars[file_id] = meta.get("size", 1000)
                        total_chars += file_chars[file_id]

        task["total_chars"] = total_chars
        task["processed_chars"] = 0

        # åˆå§‹é ä¼°æ™‚é–“
        if processing_stats["avg_chars_per_second"] > 0:
            task["estimated_remaining_seconds"] = total_chars / processing_stats["avg_chars_per_second"]
            task["message"] = f"é è¨ˆéœ€è¦ {format_time(task['estimated_remaining_seconds'])}"

        for i, file_id in enumerate(file_ids):
            file_start_time = datetime.now()
            task["progress"] = (i / len(file_ids)) * 100

            # æ›´æ–°è¨ˆæ™‚è³‡è¨Š
            elapsed = (datetime.now() - task["started_at"]).total_seconds()
            task["elapsed_seconds"] = elapsed

            # è¨ˆç®—é ä¼°å‰©é¤˜æ™‚é–“
            remaining = estimate_remaining_time(
                total_chars,
                task["processed_chars"],
                elapsed
            )
            task["estimated_remaining_seconds"] = remaining

            # æ›´æ–°è¨Šæ¯
            elapsed_str = format_time(elapsed)
            remaining_str = format_time(remaining) if remaining else "è¨ˆç®—ä¸­..."
            task["message"] = f"è™•ç†æª”æ¡ˆ {i+1}/{len(file_ids)}... (å·²ç”¨æ™‚ {elapsed_str}, é è¨ˆå‰©é¤˜ {remaining_str})"

            # è®€å–æª”æ¡ˆ
            meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
            with open(meta_file) as f:
                meta = json.load(f)

            file_path = Path(meta["path"])

            # ä½¿ç”¨çœŸæ­£çš„è™•ç†å¼•æ“
            engine_config = EngineConfig(
                llm_provider="ollama",
                llm_model="qwen2.5:1.5b",
                use_rag=False,
            )
            engine = DeidentificationEngine(engine_config)
            result = engine.process_file(file_path)

            # å¾æ”¹é€²å¾Œçš„ ProcessingResult æå– PHI è©³ç´°è³‡è¨Š
            phi_count = result.total_phi_entities
            doc_info = result.documents[0] if result.documents else {}

            # ç›´æ¥å¾ documents[0] ç²å– PHI è©³ç´°åˆ—è¡¨ (æ–°çµæ§‹)
            phi_entities = []
            doc_phi_list = doc_info.get("phi_entities", [])
            for entity in doc_phi_list:
                phi_entities.append({
                    "type": entity.get("type", "UNKNOWN"),
                    "value": entity.get("text", ""),
                    "masked_value": "[MASKED]",  # é®ç½©å¾Œçš„å€¼éœ€è¦å¾ masked_content è§£æ
                    "field": None,  # æ¬„ä½è³‡è¨Šéœ€è¦é¡å¤–è™•ç†
                    "row": None,
                    "confidence": entity.get("confidence", 0.9),
                    "start_pos": entity.get("start_pos"),
                    "end_pos": entity.get("end_pos"),
                    "reason": entity.get("reason", ""),
                })

            # ä¹Ÿæª¢æŸ¥ summary.phi_entities (å‚™ç”¨)
            if not phi_entities:
                summary_phi = result.summary.get("phi_entities", [])
                for entity in summary_phi:
                    phi_entities.append({
                        "type": entity.get("type", "UNKNOWN"),
                        "value": entity.get("text", ""),
                        "masked_value": "[MASKED]",
                        "field": None,
                        "row": None,
                        "confidence": entity.get("confidence", 0.9),
                    })

            # å–å¾—åŸå§‹å’Œé®ç½©å¾Œçš„å…§å®¹ (æ–°çµæ§‹ç›´æ¥æä¾›)
            original_content = doc_info.get("original_content", "")
            masked_content = doc_info.get("masked_content", "")
            output_path = doc_info.get("output_path", "")

            # è®€å–åŸå§‹å’Œè™•ç†å¾Œçš„è³‡æ–™ç”¨æ–¼ diff é¡¯ç¤º
            original_data = None
            masked_data = None
            import pandas as pd
            try:
                if meta["file_type"] == "csv":
                    original_df = pd.read_csv(file_path)
                    original_data = original_df.head(100).to_dict(orient='records')
                elif meta["file_type"] == "xlsx":
                    original_df = pd.read_excel(file_path)
                    original_data = original_df.head(100).to_dict(orient='records')

                # å„ªå…ˆä½¿ç”¨å¼•æ“è¿”å›çš„è¼¸å‡ºè·¯å¾‘
                if output_path and Path(output_path).exists():
                    out_path = Path(output_path)
                    if out_path.suffix == ".csv":
                        masked_df = pd.read_csv(out_path)
                    else:
                        masked_df = pd.read_excel(out_path)
                    masked_data = masked_df.head(100).to_dict(orient='records')
                    logger.info(f"ä½¿ç”¨å¼•æ“è¼¸å‡ºè·¯å¾‘: {output_path}")
                else:
                    # å‚™ç”¨ï¼šæŸ¥æ‰¾è™•ç†å¾Œçš„è¼¸å‡ºæª”æ¡ˆ
                    output_dir = Path("data/output/results")
                    if output_dir.exists():
                        original_stem = file_path.stem
                        for output_file in sorted(output_dir.glob(f"*{original_stem}*"), key=lambda x: x.stat().st_mtime, reverse=True):
                            if output_file.suffix in [".csv", ".xlsx"]:
                                if output_file.suffix == ".csv":
                                    masked_df = pd.read_csv(output_file)
                                else:
                                    masked_df = pd.read_excel(output_file)
                                masked_data = masked_df.head(100).to_dict(orient='records')
                                logger.info(f"æ‰¾åˆ°è™•ç†å¾Œæª”æ¡ˆ: {output_file}")
                                break
            except Exception as read_err:
                logger.warning(f"ç„¡æ³•è®€å–åŸå§‹/è™•ç†å¾Œè³‡æ–™: {read_err}")

            # å–å¾—æŒ‰é¡å‹çµ±è¨ˆ
            phi_by_type = result.summary.get("phi_by_type", {})

            results.append({
                "file_id": file_id,
                "filename": meta["filename"],
                "phi_found": phi_count,
                "phi_by_type": phi_by_type,  # æ–°å¢ï¼šæŒ‰é¡å‹çµ±è¨ˆ
                "rows_processed": result.processed_documents,
                "status": "completed",
                "phi_entities": phi_entities,
                "original_data": original_data,
                "masked_data": masked_data,
                "original_content": original_content[:5000] if original_content else None,  # é™åˆ¶å¤§å°
                "masked_content": masked_content[:5000] if masked_content else None,
                "output_path": output_path,
            })
            logger.info(f"Engine processed {meta['filename']}: found {phi_count} PHI, types: {phi_by_type}")

            # æ›´æ–°å·²è™•ç†å­—ç¬¦æ•¸
            task["processed_chars"] = task.get("processed_chars", 0) + file_chars.get(file_id, 0)

            # è¨˜éŒ„å–®æª”è™•ç†æ™‚é–“
            file_elapsed = (datetime.now() - file_start_time).total_seconds()
            logger.info(f"â±ï¸ File {meta['filename']} processed in {file_elapsed:.2f}s")

        # å„²å­˜çµæœ
        result_id = task_id
        result_file = RESULTS_DIR / f"{result_id}_results.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump({
                "task_id": task_id,
                "job_name": task["job_name"],
                "config": task["config"],
                "results": results,
                "processed_at": datetime.now().isoformat(),
            }, f, indent=2, ensure_ascii=False)

        # ç”¢ç”Ÿå ±å‘Š
        report_file = REPORTS_DIR / f"{result_id}_report.json"
        total_phi = sum(r.get("phi_found", 0) for r in results)

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_time = (datetime.now() - task["started_at"]).total_seconds()
        processing_speed = total_chars / total_time if total_time > 0 else 0

        report = {
            "task_id": task_id,
            "job_name": task["job_name"],
            "summary": {
                "files_processed": len(results),
                "total_phi_found": total_phi,
                "processing_time_seconds": round(total_time, 2),
                "total_chars": total_chars,
                "processing_speed_chars_per_sec": round(processing_speed, 2),
            },
            "file_details": results,
            "generated_at": datetime.now().isoformat(),
        }
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # æ›´æ–°è™•ç†é€Ÿåº¦çµ±è¨ˆ
        update_processing_stats(total_chars, total_time)

        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        task["status"] = "completed"
        task["progress"] = 100.0
        task["elapsed_seconds"] = total_time
        task["estimated_remaining_seconds"] = 0
        task["processing_speed"] = processing_speed
        task["message"] = f"è™•ç†å®Œæˆï¼æ‰¾åˆ° {total_phi} å€‹ PHI (è€—æ™‚ {format_time(total_time)}, é€Ÿåº¦ {processing_speed:.1f} å­—å…ƒ/ç§’)"
        task["completed_at"] = datetime.now()
        task["result_file"] = str(result_file.name)
        task["report_file"] = str(report_file.name)

        logger.info(f"âœ… Task completed: {task_id}, PHI found: {total_phi}")

    except Exception as e:
        task["status"] = "failed"
        task["message"] = f"è™•ç†å¤±æ•—: {e!s}"
        task["completed_at"] = datetime.now()
        logger.error(f"âŒ Task failed: {task_id}, error: {e}")


@app.get("/api/tasks", response_model=list[TaskStatus])
async def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»å‹™"""
    return [
        TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})
        for task in sorted(tasks_db.values(), key=lambda x: x["created_at"], reverse=True)
    ]


@app.get("/api/tasks/{task_id}", response_model=TaskStatus)
async def get_task(task_id: str):
    """å–å¾—ä»»å‹™ç‹€æ…‹"""
    if task_id not in tasks_db:
        raise HTTPException(404, "ä»»å‹™ä¸å­˜åœ¨")
    task = tasks_db[task_id]

    # å¦‚æœä»»å‹™æ­£åœ¨è™•ç†ä¸­ï¼Œå³æ™‚æ›´æ–°è¨ˆæ™‚è³‡è¨Š
    if task["status"] == "processing" and task.get("started_at"):
        elapsed = (datetime.now() - task["started_at"]).total_seconds()
        task["elapsed_seconds"] = elapsed

        # æ›´æ–°é ä¼°å‰©é¤˜æ™‚é–“
        total_chars = task.get("total_chars", 0)
        processed_chars = task.get("processed_chars", 0)
        if total_chars > 0 and processed_chars > 0 and elapsed > 0:
            current_speed = processed_chars / elapsed
            remaining_chars = total_chars - processed_chars
            task["estimated_remaining_seconds"] = remaining_chars / current_speed
            task["processing_speed"] = current_speed

    return TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})


@app.get("/api/stats/processing")
async def get_processing_stats():
    """å–å¾—è™•ç†é€Ÿåº¦çµ±è¨ˆ"""
    return {
        "avg_chars_per_second": processing_stats["avg_chars_per_second"],
        "total_chars_processed": processing_stats["total_chars_processed"],
        "total_time_seconds": processing_stats["total_time_seconds"],
        "task_count": processing_stats["task_count"],
    }


# ============================================================
# Results & Reports APIs
# ============================================================

@app.get("/api/results")
async def list_results():
    """åˆ—å‡ºæ‰€æœ‰è™•ç†çµæœ"""
    results = []
    for result_file in RESULTS_DIR.glob("*_results.json"):
        try:
            with open(result_file, encoding="utf-8") as f:
                data = json.load(f)
                task_id = result_file.stem.replace("_results", "")

                # è¨ˆç®—ç¸½ PHI æ•¸é‡
                phi_count = 0
                file_results = data.get("results", [])
                for fr in file_results:
                    phi_count += fr.get("phi_found", 0)

                # å–å¾—æª”æ¡ˆåç¨±
                filenames = [fr.get("filename", "Unknown") for fr in file_results]

                results.append({
                    "task_id": task_id,
                    "filename": ", ".join(filenames) if filenames else "Unknown",
                    "phi_count": phi_count,
                    "files_processed": len(file_results),
                    "status": "completed",
                    "created_at": data.get("processed_at"),
                })
        except Exception as e:
            logger.warning(f"Failed to read result file {result_file}: {e}")

    # æŒ‰æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    results.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return results


@app.get("/api/results/{task_id}")
async def get_results(task_id: str):
    """å–å¾—è™•ç†çµæœ"""
    result_file = RESULTS_DIR / f"{task_id}_results.json"
    if not result_file.exists():
        raise HTTPException(404, "çµæœä¸å­˜åœ¨")

    with open(result_file, encoding="utf-8") as f:
        return json.load(f)


@app.get("/api/reports/{task_id}")
async def get_report(task_id: str):
    """å–å¾—å ±å‘Š"""
    report_file = REPORTS_DIR / f"{task_id}_report.json"
    if not report_file.exists():
        raise HTTPException(404, "å ±å‘Šä¸å­˜åœ¨")

    with open(report_file, encoding="utf-8") as f:
        return json.load(f)


@app.get("/api/reports")
async def list_reports():
    """åˆ—å‡ºæ‰€æœ‰å ±å‘Š"""
    reports = []
    for report_file in REPORTS_DIR.glob("*_report.json"):
        with open(report_file, encoding="utf-8") as f:
            report = json.load(f)
            reports.append({
                "task_id": report["task_id"],
                "job_name": report.get("job_name", ""),
                "files_processed": report["summary"]["files_processed"],
                "total_phi_found": report["summary"]["total_phi_found"],
                "generated_at": report["generated_at"],
            })
    return sorted(reports, key=lambda x: x["generated_at"], reverse=True)


# ============================================================
# Settings & Regulations APIs
# ============================================================

@app.get("/api/settings/phi-types")
async def get_phi_types():
    """å–å¾—å¯ç”¨çš„ PHI é¡å‹"""
    return {
        "phi_types": [
            {"id": "NAME", "name": "å§“å", "description": "æ‚£è€…æˆ–ç›¸é—œäººå“¡å§“å", "category": "identifier"},
            {"id": "DATE", "name": "æ—¥æœŸ", "description": "å‡ºç”Ÿæ—¥æœŸã€å°±è¨ºæ—¥æœŸç­‰", "category": "temporal"},
            {"id": "PHONE", "name": "é›»è©±", "description": "é›»è©±è™Ÿç¢¼", "category": "contact"},
            {"id": "EMAIL", "name": "é›»å­éƒµä»¶", "description": "é›»å­éƒµä»¶åœ°å€", "category": "contact"},
            {"id": "ADDRESS", "name": "åœ°å€", "description": "ä½å€ã€å·¥ä½œåœ°å€ç­‰", "category": "geographic"},
            {"id": "ID_NUMBER", "name": "èº«ä»½è­‰è™Ÿ", "description": "èº«åˆ†è­‰å­—è™Ÿã€è­·ç…§è™Ÿç¢¼", "category": "identifier"},
            {"id": "MEDICAL_RECORD", "name": "ç—…æ­·è™Ÿ", "description": "ç—…æ­·è™Ÿç¢¼", "category": "identifier"},
            {"id": "SOCIAL_SECURITY", "name": "ç¤¾æœƒå®‰å…¨ç¢¼", "description": "å¥ä¿å¡è™Ÿç­‰", "category": "identifier"},
            {"id": "ACCOUNT_NUMBER", "name": "å¸³è™Ÿ", "description": "éŠ€è¡Œå¸³è™Ÿç­‰", "category": "financial"},
            {"id": "LICENSE_NUMBER", "name": "åŸ·ç…§è™Ÿç¢¼", "description": "é§•ç…§ã€è­‰ç…§è™Ÿç¢¼", "category": "identifier"},
            {"id": "VEHICLE_ID", "name": "è»Šè¼›è­˜åˆ¥", "description": "è»Šç‰Œè™Ÿç¢¼", "category": "identifier"},
            {"id": "DEVICE_ID", "name": "è¨­å‚™è­˜åˆ¥", "description": "è¨­å‚™åºè™Ÿã€IP ä½å€", "category": "technical"},
            {"id": "URL", "name": "ç¶²å€", "description": "ç¶²ç«™ç¶²å€", "category": "technical"},
            {"id": "BIOMETRIC", "name": "ç”Ÿç‰©è­˜åˆ¥", "description": "æŒ‡ç´‹ã€è²ç´‹ç­‰", "category": "biometric"},
            {"id": "PHOTO", "name": "å½±åƒ", "description": "å…¨è‡‰ç…§ç‰‡ç­‰", "category": "biometric"},
            {"id": "AGE", "name": "å¹´é½¡", "description": "è¶…é 89 æ­²çš„å¹´é½¡", "category": "demographic"},
        ],
        "masking_types": [
            {"id": "redact", "name": "é®è”½", "description": "ä»¥ [REDACTED] å–ä»£"},
            {"id": "hash", "name": "é›œæ¹Š", "description": "ä»¥é›œæ¹Šå€¼å–ä»£ï¼Œå¯è¿½è¹¤åŒä¸€äºº"},
            {"id": "pseudonymize", "name": "å‡ååŒ–", "description": "ä»¥å‡åå–ä»£"},
            {"id": "generalize", "name": "æ³›åŒ–", "description": "ä»¥æ›´å»£æ³›çš„é¡åˆ¥å–ä»£"},
        ],
    }


@app.get("/api/settings/config")
async def get_config():
    """å–å¾—ç›®å‰çš„è™•ç†è¨­å®š"""
    config_file = DATA_DIR / "config.json"
    if config_file.exists():
        with open(config_file, encoding="utf-8") as f:
            return json.load(f)
    return PHIConfig().model_dump()


@app.put("/api/settings/config")
async def update_config(config: PHIConfig):
    """æ›´æ–°è™•ç†è¨­å®š"""
    config_file = DATA_DIR / "config.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config.model_dump(), f, indent=2, ensure_ascii=False)
    logger.info(f"âš™ï¸ Config updated: {config.masking_type}")
    return {"message": "è¨­å®šå·²æ›´æ–°", "config": config.model_dump()}


@app.get("/api/regulations", response_model=list[RegulationRule])
async def list_regulations():
    """åˆ—å‡ºæ‰€æœ‰æ³•è¦è¦å‰‡"""
    # é è¨­è¦å‰‡
    default_rules = [
        {
            "id": "hipaa-safe-harbor",
            "name": "HIPAA Safe Harbor",
            "description": "ç¾åœ‹ HIPAA æ³•è¦çš„ 18 é …è­˜åˆ¥è³‡è¨Š",
            "phi_types": ["NAME", "DATE", "PHONE", "EMAIL", "ADDRESS", "ID_NUMBER",
                         "MEDICAL_RECORD", "SOCIAL_SECURITY", "ACCOUNT_NUMBER"],
            "source": "hipaa",
            "enabled": True,
        },
        {
            "id": "taiwan-pdpa",
            "name": "å°ç£å€‹è³‡æ³•",
            "description": "å°ç£å€‹äººè³‡æ–™ä¿è­·æ³•å®šç¾©çš„å€‹äººè³‡æ–™",
            "phi_types": ["NAME", "DATE", "PHONE", "EMAIL", "ADDRESS", "ID_NUMBER"],
            "source": "taiwan_pdpa",
            "enabled": True,
        },
    ]

    # è¼‰å…¥è‡ªè¨‚è¦å‰‡
    custom_rules_file = REGULATIONS_DIR / "custom_rules.json"
    if custom_rules_file.exists():
        with open(custom_rules_file, encoding="utf-8") as f:
            custom_rules = json.load(f)
            default_rules.extend(custom_rules)

    return [RegulationRule(**r) for r in default_rules]


@app.post("/api/regulations/upload")
async def upload_regulation(file: UploadFile = File(...)):
    """ä¸Šå‚³è‡ªè¨‚æ³•è¦æª”æ¡ˆ"""
    if not file.filename.endswith(".json"):
        raise HTTPException(400, "åƒ…æ”¯æ´ JSON æ ¼å¼")

    content = await file.read()
    try:
        rules = json.loads(content)

        # é©—è­‰æ ¼å¼
        if not isinstance(rules, list):
            rules = [rules]

        for rule in rules:
            required_fields = ["id", "name", "description", "phi_types"]
            for field in required_fields:
                if field not in rule:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
            rule["source"] = "custom"
            rule["enabled"] = rule.get("enabled", True)

        # å„²å­˜
        custom_rules_file = REGULATIONS_DIR / "custom_rules.json"
        existing_rules = []
        if custom_rules_file.exists():
            with open(custom_rules_file, encoding="utf-8") as f:
                existing_rules = json.load(f)

        existing_rules.extend(rules)

        with open(custom_rules_file, "w", encoding="utf-8") as f:
            json.dump(existing_rules, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“œ Uploaded {len(rules)} regulation rules")
        return {"message": f"å·²ä¸Šå‚³ {len(rules)} æ¢è¦å‰‡", "rules": rules}

    except json.JSONDecodeError:
        raise HTTPException(400, "ç„¡æ•ˆçš„ JSON æ ¼å¼")
    except ValueError as e:
        raise HTTPException(400, str(e))


@app.put("/api/regulations/{rule_id}")
async def update_regulation(rule_id: str, enabled: bool):
    """å•Ÿç”¨/åœç”¨æ³•è¦è¦å‰‡"""
    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²æ›´æ–°è³‡æ–™åº«
    logger.info(f"ğŸ“œ Rule {rule_id} {'enabled' if enabled else 'disabled'}")
    return {"message": "è¦å‰‡å·²æ›´æ–°", "rule_id": rule_id, "enabled": enabled}


# ============================================================
# Health Check
# ============================================================

@app.get("/api/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ï¼ŒåŒ…å« LLM ç‹€æ…‹"""
    import subprocess

    # æª¢æŸ¥ Ollama LLM ç‹€æ…‹
    llm_status = "offline"
    llm_model = None
    try:
        result = subprocess.run(
            ["curl", "-s", "http://localhost:11434/api/tags"],
            check=False, capture_output=True,
            text=True,
            timeout=3
        )
        if result.returncode == 0:
            import json as json_lib
            data = json_lib.loads(result.stdout)
            models = [m.get("name") for m in data.get("models", [])]
            if models:
                llm_status = "online"
                llm_model = models[0] if len(models) == 1 else f"{len(models)} models"
    except Exception:
        pass

    # æª¢æŸ¥ PHI Engine æ˜¯å¦å¯ç”¨
    engine_available = False
    try:
        from core.application.processing.engine import DeidentificationEngine
        engine_available = True
    except ImportError:
        pass

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "llm": {
            "status": llm_status,
            "model": llm_model,
            "provider": "ollama"
        },
        "engine_available": engine_available
    }


# ============================================================
# Run Server
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )
