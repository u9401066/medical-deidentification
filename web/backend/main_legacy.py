"""
Medical De-identification Web API
FastAPI å¾Œç«¯æœå‹™
"""

import json
import os

# ç¢ºä¿å¯ä»¥ import ä¸»å°ˆæ¡ˆæ¨¡çµ„
import sys
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import Any

from fastapi import BackgroundTasks, FastAPI, File, HTTPException, Query, UploadFile

# LLM é…ç½® (æ”¯æ´é ç«¯ Ollama API)
# å¿…é ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OLLAMA_BASE_URLï¼Œæˆ–ä½¿ç”¨æœ¬åœ°é è¨­å€¼
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from loguru import logger

# Import PHIType for type handling
try:
    from core.domain import PHIType
except ImportError:
    PHIType = None  # Fallback if not available

# è¨­å®šè³‡æ–™ç›®éŒ„
DATA_DIR = Path(__file__).parent / "data"
UPLOAD_DIR = DATA_DIR / "uploads"
RESULTS_DIR = DATA_DIR / "results"
REPORTS_DIR = DATA_DIR / "reports"
REGULATIONS_DIR = DATA_DIR / "regulations"

for d in [UPLOAD_DIR, RESULTS_DIR, REPORTS_DIR, REGULATIONS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# ä»»å‹™ç‹€æ…‹å­˜å„²
tasks_db: dict[str, dict[str, Any]] = {}


def format_time(seconds: float | None) -> str:
    """æ ¼å¼åŒ–æ™‚é–“ç‚ºäººé¡å¯è®€æ ¼å¼"""
    if seconds is None or seconds < 0:
        return "è¨ˆç®—ä¸­..."
    if seconds < 60:
        return f"{seconds:.0f} ç§’"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins} åˆ† {secs} ç§’"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        return f"{hours} å°æ™‚ {mins} åˆ†"


# ============================================================
# Pydantic Models
# ============================================================


class PHITypeConfig(BaseModel):
    """å–®ä¸€ PHI é¡å‹é…ç½®"""

    enabled: bool = True
    masking: str = "mask"  # mask, hash, replace, delete, keep
    replace_with: str | None = None  # è‡ªè¨‚æ›¿æ›è©ï¼Œç•¶ masking ç‚º 'replace' æ™‚ä½¿ç”¨


class PHIConfig(BaseModel):
    """PHI è™•ç†é…ç½®"""

    masking_type: str = Field(default="redact", description="redact, hash, pseudonymize")
    phi_types: list[str] | dict[str, PHITypeConfig] = Field(
        default_factory=lambda: [
            "NAME",
            "DATE",
            "PHONE",
            "EMAIL",
            "ADDRESS",
            "ID_NUMBER",
            "MEDICAL_RECORD",
        ]
    )
    preserve_format: bool = Field(default=True)
    custom_patterns: dict[str, str] | None = None

    def get_enabled_types(self) -> list[str]:
        """å–å¾—å•Ÿç”¨çš„ PHI é¡å‹åˆ—è¡¨"""
        if isinstance(self.phi_types, list):
            return self.phi_types
        return [k for k, v in self.phi_types.items() if v.enabled]

    def get_replace_text(self, phi_type: str) -> str | None:
        """å–å¾—æŒ‡å®š PHI é¡å‹çš„æ›¿æ›è©"""
        if isinstance(self.phi_types, dict):
            config = self.phi_types.get(phi_type)
            if config and config.masking == "replace":
                return config.replace_with or f"[{phi_type}]"
        return None


class ProcessRequest(BaseModel):
    """è™•ç†è«‹æ±‚"""

    file_ids: list[str]
    config: PHIConfig | None = None
    job_name: str | None = None


class TaskStatus(BaseModel):
    """ä»»å‹™ç‹€æ…‹"""

    task_id: str
    status: str  # pending, processing, completed, failed
    progress: float = 0.0
    message: str = ""
    created_at: datetime
    completed_at: datetime | None = None
    result_file: str | None = None
    report_file: str | None = None
    # è¨ˆæ™‚ç›¸é—œ
    started_at: datetime | None = None
    elapsed_seconds: float | None = None
    estimated_remaining_seconds: float | None = None
    processing_speed: float | None = None  # chars per second
    total_chars: int | None = None
    processed_chars: int | None = None


class RegulationRule(BaseModel):
    """æ³•è¦è¦å‰‡"""

    id: str
    name: str
    description: str
    phi_types: list[str]
    source: str  # hipaa, taiwan_pdpa, custom
    enabled: bool = True


class UploadedFile(BaseModel):
    """ä¸Šå‚³çš„æª”æ¡ˆè³‡è¨Š"""

    file_id: str
    filename: str
    size: int
    upload_time: datetime
    file_type: str
    preview_available: bool = True
    status: str = "pending"  # pending, processing, completed, error
    task_id: str | None = None  # é—œè¯çš„è™•ç†ä»»å‹™ ID


# ============================================================
# Application Setup
# ============================================================


@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç†"""
    logger.info("ğŸš€ Medical De-identification Web API starting...")
    # è¼‰å…¥å·²æœ‰çš„ä»»å‹™ç‹€æ…‹
    tasks_file = DATA_DIR / "tasks.json"
    if tasks_file.exists():
        try:
            with open(tasks_file) as f:
                saved_tasks = json.load(f)
                for task_id, task_data in saved_tasks.items():
                    task_data["created_at"] = datetime.fromisoformat(task_data["created_at"])
                    if task_data.get("completed_at"):
                        task_data["completed_at"] = datetime.fromisoformat(
                            task_data["completed_at"]
                        )
                    tasks_db[task_id] = task_data
            logger.info(f"Loaded {len(tasks_db)} existing tasks")
        except Exception as e:
            logger.warning(f"Could not load tasks: {e}")

    yield

    # ä¿å­˜ä»»å‹™ç‹€æ…‹
    logger.info("ğŸ’¾ Saving tasks state...")
    try:
        serializable = {}
        for task_id, task in tasks_db.items():
            serializable[task_id] = {
                **task,
                "created_at": task["created_at"].isoformat(),
                "completed_at": task["completed_at"].isoformat()
                if task.get("completed_at")
                else None,
            }
        with open(tasks_file, "w") as f:
            json.dump(serializable, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Could not save tasks: {e}")


app = FastAPI(
    title="Medical De-identification API",
    description="é†«ç™‚æ–‡æœ¬å»è­˜åˆ¥åŒ– Web API",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# File Upload & Download APIs
# ============================================================

# æª”æ¡ˆå¤§å°é™åˆ¶ (50MB)
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB in bytes


def _sanitize_path(file_id: str) -> bool:
    """é©—è­‰ file_id æ ¼å¼ï¼Œé˜²æ­¢è·¯å¾‘ç©¿è¶Šæ”»æ“Š"""
    import re

    # file_id åªå…è¨±è‹±æ•¸å­—å’Œé€£å­—è™Ÿ
    return bool(re.match(r"^[a-zA-Z0-9-]+$", file_id))


@app.post("/api/upload", response_model=UploadedFile)
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šå‚³æª”æ¡ˆ"""
    file_id = str(uuid.uuid4())[:8]
    file_ext = Path(file.filename).suffix.lower()

    # æ”¯æ´çš„æª”æ¡ˆé¡å‹
    supported_types = {".csv", ".xlsx", ".xls", ".txt", ".json", ".docx", ".pdf"}
    if file_ext not in supported_types:
        raise HTTPException(400, f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_ext}. æ”¯æ´: {supported_types}")

    # å„²å­˜æª”æ¡ˆ
    save_path = UPLOAD_DIR / f"{file_id}{file_ext}"
    content = await file.read()

    # M2: æª”æ¡ˆå¤§å°é™åˆ¶
    if len(content) > MAX_FILE_SIZE:
        raise HTTPException(413, f"æª”æ¡ˆéå¤§ï¼Œæœ€å¤§å…è¨± {MAX_FILE_SIZE // (1024 * 1024)}MB")

    with open(save_path, "wb") as f:
        f.write(content)

    # å„²å­˜å…ƒæ•¸æ“š
    metadata = {
        "file_id": file_id,
        "filename": file.filename,
        "size": len(content),
        "upload_time": datetime.now().isoformat(),
        "file_type": file_ext[1:],
        "path": str(save_path),
    }

    with open(UPLOAD_DIR / f"{file_id}.meta.json", "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    # M4: æ—¥èªŒè„«æ• - åªè¨˜éŒ„ file_idï¼Œä¸è¨˜éŒ„å¯èƒ½å« PHI çš„åŸå§‹æª”å
    logger.info(f"ğŸ“ Uploaded file: [REDACTED] -> {file_id} ({file_ext}, {len(content)} bytes)")

    return UploadedFile(
        file_id=file_id,
        filename=file.filename,
        size=len(content),
        upload_time=datetime.now(),
        file_type=file_ext[1:],
    )


@app.get("/api/files", response_model=list[UploadedFile])
async def list_files():
    """åˆ—å‡ºæ‰€æœ‰ä¸Šå‚³çš„æª”æ¡ˆï¼ˆå«è™•ç†ç‹€æ…‹ï¼‰"""
    files = []

    # å»ºç«‹æª”æ¡ˆ ID -> ä»»å‹™ç‹€æ…‹çš„æ˜ å°„
    file_task_map: dict[str, dict] = {}
    for task in tasks_db.values():
        for file_id in task.get("file_ids", []):
            # å–æœ€æ–°çš„ä»»å‹™
            if (
                file_id not in file_task_map
                or task["created_at"] > file_task_map[file_id]["created_at"]
            ):
                file_task_map[file_id] = task

    for meta_file in UPLOAD_DIR.glob("*.meta.json"):
        with open(meta_file) as f:
            meta = json.load(f)
            file_id = meta["file_id"]

            # åˆ¤æ–·æª”æ¡ˆç‹€æ…‹
            status = "pending"
            task_id = None
            if file_id in file_task_map:
                task = file_task_map[file_id]
                task_id = task["task_id"]
                task_status = task.get("status", "pending")
                if task_status == "completed":
                    status = "completed"
                elif task_status == "processing":
                    status = "processing"
                elif task_status == "failed":
                    status = "error"

            files.append(
                UploadedFile(
                    file_id=file_id,
                    filename=meta["filename"],
                    size=meta["size"],
                    upload_time=datetime.fromisoformat(meta["upload_time"]),
                    file_type=meta["file_type"],
                    status=status,
                    task_id=task_id,
                )
            )
    return sorted(files, key=lambda x: x.upload_time, reverse=True)


@app.delete("/api/files/{file_id}")
async def delete_file(file_id: str):
    """åˆªé™¤æª”æ¡ˆ"""
    # H2: è·¯å¾‘ç©¿è¶Šé˜²è­· - é©—è­‰ file_id æ ¼å¼
    if not _sanitize_path(file_id):
        raise HTTPException(400, "ç„¡æ•ˆçš„æª”æ¡ˆ ID")

    meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
    if not meta_file.exists():
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    with open(meta_file) as f:
        meta = json.load(f)

    # H2: è·¯å¾‘ç©¿è¶Šé˜²è­· - ç¢ºä¿è·¯å¾‘åœ¨å…è¨±ç›®éŒ„å…§
    target_path = Path(meta["path"]).resolve()
    allowed_dir = UPLOAD_DIR.resolve()
    if not str(target_path).startswith(str(allowed_dir)):
        logger.warning(f"âš ï¸ Path traversal attempt blocked: {file_id}")
        raise HTTPException(403, "ç¦æ­¢çš„æ“ä½œ")

    # åˆªé™¤æª”æ¡ˆå’Œå…ƒæ•¸æ“š
    target_path.unlink(missing_ok=True)
    meta_file.unlink()

    logger.info(f"ğŸ—‘ï¸ Deleted file: {file_id}")
    return {"message": "å·²åˆªé™¤"}


@app.get("/api/download/{file_id}")
async def download_result(
    file_id: str,
    file_type: str = Query("result", enum=["result", "report"]),
    format: str = Query("xlsx", enum=["xlsx", "csv", "json"]),
):
    """ä¸‹è¼‰è™•ç†çµæœæˆ–å ±å‘Š

    Args:
        file_id: ä»»å‹™ ID
        file_type: result (è™•ç†çµæœ) æˆ– report (å ±å‘Š)
        format: xlsx, csv, json
    """
    from io import BytesIO

    import pandas as pd

    if file_type == "result":
        search_dir = RESULTS_DIR
    else:
        search_dir = REPORTS_DIR

    # æ‰¾åˆ°å°æ‡‰çš„ JSON æª”æ¡ˆ
    matching_files = list(search_dir.glob(f"{file_id}*"))
    if not matching_files:
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    json_path = matching_files[0]

    # å¦‚æœè¦æ±‚ JSON æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if format == "json":
        return FileResponse(
            json_path, filename=f"{file_id}_{file_type}.json", media_type="application/json"
        )

    # è®€å– JSON
    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)

    # è¼¸å‡º mask å¾Œçš„å®Œæ•´è³‡æ–™
    output = BytesIO()

    if file_type == "result":
        # å„ªå…ˆä½¿ç”¨ masked_data (è¡¨æ ¼è³‡æ–™)
        all_masked_data = []
        for file_result in data.get("results", []):
            masked_data = file_result.get("masked_data")
            if masked_data and isinstance(masked_data, list):
                all_masked_data.extend(masked_data)
            elif file_result.get("masked_content"):
                # ç´”æ–‡å­—å…§å®¹ï¼ŒåŒ…è£æˆè¡¨æ ¼
                all_masked_data.append(
                    {
                        "æª”æ¡ˆ": file_result.get("filename", ""),
                        "å…§å®¹": file_result.get("masked_content", ""),
                    }
                )

        if all_masked_data:
            df = pd.DataFrame(all_masked_data)
        else:
            # fallback: è¼¸å‡º PHI æ‘˜è¦
            df = pd.DataFrame(
                [
                    {
                        "è¨Šæ¯": "æ²’æœ‰å¯è¼¸å‡ºçš„è³‡æ–™",
                        "ä»»å‹™ ID": file_id,
                    }
                ]
            )
    else:
        # å ±å‘Šæ ¼å¼ï¼šè¼¸å‡º PHI åˆ—è¡¨
        phi_records = []
        for file_detail in data.get("file_details", []):
            filename = file_detail.get("filename", "unknown")
            for phi in file_detail.get("phi_entities", []):
                phi_records.append(
                    {
                        "æª”æ¡ˆ": filename,
                        "PHI é¡å‹": phi.get("type", ""),
                        "åŸå§‹å€¼": phi.get("value", ""),
                        "é®ç½©å€¼": phi.get("masked_value", "[MASKED]"),
                        "ä¿¡å¿ƒåº¦": phi.get("confidence", ""),
                    }
                )

        if phi_records:
            df = pd.DataFrame(phi_records)
        else:
            df = pd.DataFrame(
                [
                    {
                        "è¨Šæ¯": "æ²’æœ‰ç™¼ç¾ PHI",
                        "ä»»å‹™ ID": file_id,
                    }
                ]
            )

    # ç”¢ç”Ÿæª”æ¡ˆ
    if format == "xlsx":
        df.to_excel(output, index=False, engine="openpyxl")
        media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        filename = f"{file_id}_{file_type}.xlsx"
    else:  # csv
        df.to_csv(output, index=False, encoding="utf-8-sig")  # BOM for Excel
        media_type = "text/csv"
        filename = f"{file_id}_{file_type}.csv"

    output.seek(0)

    from starlette.responses import StreamingResponse

    return StreamingResponse(
        output,
        media_type=media_type,
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


# ============================================================
# Data Preview APIs
# ============================================================


@app.get("/api/preview/{file_id}")
async def preview_file(
    file_id: str, page: int = Query(1, ge=1), page_size: int = Query(20, ge=1, le=100)
):
    """é è¦½ä¸Šå‚³çš„æª”æ¡ˆå…§å®¹"""
    meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
    if not meta_file.exists():
        raise HTTPException(404, "æª”æ¡ˆä¸å­˜åœ¨")

    with open(meta_file) as f:
        meta = json.load(f)

    file_path = Path(meta["path"])
    file_type = meta["file_type"]

    try:
        if file_type in ["csv", "xlsx", "xls"]:
            return await _preview_tabular(file_path, file_type, page, page_size)
        elif file_type == "txt":
            return await _preview_text(file_path, page, page_size)
        elif file_type == "json":
            return await _preview_json(file_path, page, page_size)
        else:
            return {"message": f"é è¦½ä¸æ”¯æ´ {file_type} æ ¼å¼", "preview_available": False}
    except Exception as e:
        logger.error(f"Preview error: {e}")
        raise HTTPException(500, f"é è¦½å¤±æ•—: {e!s}")


async def _preview_tabular(file_path: Path, file_type: str, page: int, page_size: int):
    """é è¦½è¡¨æ ¼è³‡æ–™"""
    import pandas as pd

    if file_type == "csv":
        df = pd.read_csv(file_path, nrows=page * page_size + page_size)
    else:
        df = pd.read_excel(file_path, nrows=page * page_size + page_size)

    total_rows = len(df)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size

    page_df = df.iloc[start_idx:end_idx]

    return {
        "type": "tabular",
        "columns": list(df.columns),
        "data": page_df.fillna("").to_dict(orient="records"),
        "total_rows": total_rows,
        "page": page,
        "page_size": page_size,
        "has_more": end_idx < total_rows,
    }


async def _preview_text(file_path: Path, page: int, page_size: int):
    """é è¦½æ–‡å­—æª”"""
    with open(file_path, encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    total_lines = len(lines)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size

    return {
        "type": "text",
        "lines": [line.rstrip() for line in lines[start_idx:end_idx]],
        "total_lines": total_lines,
        "page": page,
        "page_size": page_size,
        "has_more": end_idx < total_lines,
    }


async def _preview_json(file_path: Path, page: int, page_size: int):
    """é è¦½ JSON æª”æ¡ˆ"""
    with open(file_path, encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, list):
        total_items = len(data)
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        return {
            "type": "json_array",
            "data": data[start_idx:end_idx],
            "total_items": total_items,
            "page": page,
            "page_size": page_size,
            "has_more": end_idx < total_items,
        }
    else:
        return {
            "type": "json_object",
            "data": data,
            "total_items": 1,
            "page": 1,
            "page_size": 1,
            "has_more": False,
        }


# ============================================================
# PHI Processing APIs
# ============================================================


@app.post("/api/process", response_model=TaskStatus)
async def start_processing(request: ProcessRequest, background_tasks: BackgroundTasks):
    """é–‹å§‹ PHI è™•ç†ä»»å‹™"""
    task_id = str(uuid.uuid4())[:8]

    # é©—è­‰æª”æ¡ˆå­˜åœ¨
    for file_id in request.file_ids:
        meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
        if not meta_file.exists():
            raise HTTPException(404, f"æª”æ¡ˆä¸å­˜åœ¨: {file_id}")

    # å»ºç«‹ä»»å‹™
    task = {
        "task_id": task_id,
        "status": "pending",
        "progress": 0.0,
        "message": "ä»»å‹™å·²å»ºç«‹ï¼Œç­‰å¾…è™•ç†...",
        "created_at": datetime.now(),
        "completed_at": None,
        "result_file": None,
        "report_file": None,
        "file_ids": request.file_ids,
        "config": request.config.model_dump() if request.config else PHIConfig().model_dump(),
        "job_name": request.job_name or f"job-{task_id}",
    }
    tasks_db[task_id] = task

    # èƒŒæ™¯åŸ·è¡Œè™•ç†
    background_tasks.add_task(process_phi_task, task_id)

    logger.info(f"ğŸš€ Created task: {task_id} for files: {request.file_ids}")

    return TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})


# è™•ç†é€Ÿåº¦çµ±è¨ˆï¼ˆç”¨æ–¼é ä¼°æ™‚é–“ï¼‰
processing_stats = {
    "total_chars_processed": 0,
    "total_time_seconds": 0.0,
    "task_count": 0,
    "avg_chars_per_second": 50.0,  # åˆå§‹ä¼°è¨ˆå€¼ï¼ˆåŸºæ–¼ LLM æ¨ç†é€Ÿåº¦ï¼‰
}


def estimate_remaining_time(total_chars: int, processed_chars: int, elapsed: float) -> float | None:
    """ä¼°è¨ˆå‰©é¤˜æ™‚é–“"""
    if processed_chars <= 0 or elapsed <= 0:
        # ä½¿ç”¨æ­·å²å¹³å‡å€¼ä¼°è¨ˆ
        if processing_stats["avg_chars_per_second"] > 0:
            return (total_chars - processed_chars) / processing_stats["avg_chars_per_second"]
        return None

    # åŸºæ–¼ç•¶å‰é€Ÿåº¦ä¼°è¨ˆ
    current_speed = processed_chars / elapsed
    remaining_chars = total_chars - processed_chars
    return remaining_chars / current_speed if current_speed > 0 else None


def update_processing_stats(chars_processed: int, time_seconds: float):
    """æ›´æ–°è™•ç†é€Ÿåº¦çµ±è¨ˆ"""
    global processing_stats
    if chars_processed > 0 and time_seconds > 0:
        processing_stats["total_chars_processed"] += chars_processed
        processing_stats["total_time_seconds"] += time_seconds
        processing_stats["task_count"] += 1
        processing_stats["avg_chars_per_second"] = (
            processing_stats["total_chars_processed"] / processing_stats["total_time_seconds"]
        )
        logger.info(
            f"ğŸ“Š Updated processing stats: avg speed = {processing_stats['avg_chars_per_second']:.2f} chars/sec"
        )


async def process_phi_task(task_id: str):
    """èƒŒæ™¯åŸ·è¡Œ PHI è™•ç†"""
    task = tasks_db[task_id]
    task["status"] = "processing"
    task["message"] = "æ­£åœ¨è™•ç†..."
    task["started_at"] = datetime.now()
    task["elapsed_seconds"] = 0.0

    try:
        # è¼‰å…¥è™•ç†å¼•æ“ï¼ˆå¿…é ˆæˆåŠŸï¼‰
        from core.application.processing.engine import DeidentificationEngine, EngineConfig

        logger.info("âœ… DeidentificationEngine loaded successfully")

        file_ids = task["file_ids"]
        config = PHIConfig(**task["config"])
        results = []

        # è¨ˆç®—ç¸½å­—ç¬¦æ•¸ç”¨æ–¼é ä¼°æ™‚é–“
        total_chars = 0
        file_chars = {}
        for file_id in file_ids:
            meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
            if meta_file.exists():
                with open(meta_file) as f:
                    meta = json.load(f)
                file_path = Path(meta["path"])
                if file_path.exists():
                    try:
                        content = file_path.read_text(encoding="utf-8", errors="ignore")
                        chars = len(content)
                        file_chars[file_id] = chars
                        total_chars += chars
                    except:
                        file_chars[file_id] = meta.get("size", 1000)
                        total_chars += file_chars[file_id]

        task["total_chars"] = total_chars
        task["processed_chars"] = 0

        # åˆå§‹é ä¼°æ™‚é–“
        if processing_stats["avg_chars_per_second"] > 0:
            task["estimated_remaining_seconds"] = (
                total_chars / processing_stats["avg_chars_per_second"]
            )
            task["message"] = f"é è¨ˆéœ€è¦ {format_time(task['estimated_remaining_seconds'])}"

        for i, file_id in enumerate(file_ids):
            file_start_time = datetime.now()
            task["progress"] = (i / len(file_ids)) * 100

            # æ›´æ–°è¨ˆæ™‚è³‡è¨Š
            elapsed = (datetime.now() - task["started_at"]).total_seconds()
            task["elapsed_seconds"] = elapsed

            # è¨ˆç®—é ä¼°å‰©é¤˜æ™‚é–“
            remaining = estimate_remaining_time(total_chars, task["processed_chars"], elapsed)
            task["estimated_remaining_seconds"] = remaining

            # æ›´æ–°è¨Šæ¯
            elapsed_str = format_time(elapsed)
            remaining_str = format_time(remaining) if remaining else "è¨ˆç®—ä¸­..."
            task["message"] = (
                f"è™•ç†æª”æ¡ˆ {i + 1}/{len(file_ids)}... (å·²ç”¨æ™‚ {elapsed_str}, é è¨ˆå‰©é¤˜ {remaining_str})"
            )

            # è®€å–æª”æ¡ˆ
            meta_file = UPLOAD_DIR / f"{file_id}.meta.json"
            with open(meta_file) as f:
                meta = json.load(f)

            file_path = Path(meta["path"])

            # ä½¿ç”¨çœŸæ­£çš„è™•ç†å¼•æ“
            engine_config = EngineConfig(
                llm_provider="ollama",
                llm_model="gemma3:27b",  # é ç«¯ Ollama ä½¿ç”¨ gemma3
                llm_base_url=OLLAMA_BASE_URL,  # å‚³å…¥é ç«¯ Ollama URL
                use_rag=False,
            )
            engine = DeidentificationEngine(engine_config)
            result = engine.process_file(file_path)

            # å¾æ”¹é€²å¾Œçš„ ProcessingResult æå– PHI è©³ç´°è³‡è¨Š
            phi_count = result.total_phi_entities
            doc_info = result.documents[0] if result.documents else {}

            # ç›´æ¥å¾ documents[0] ç²å– PHI è©³ç´°åˆ—è¡¨ (æ–°çµæ§‹)
            phi_entities = []
            doc_phi_list = doc_info.get("phi_entities", [])
            for entity in doc_phi_list:
                phi_entities.append(
                    {
                        "type": entity.get("type", "UNKNOWN"),
                        "value": entity.get("text", ""),
                        "masked_value": "[MASKED]",  # é®ç½©å¾Œçš„å€¼éœ€è¦å¾ masked_content è§£æ
                        "field": None,  # æ¬„ä½è³‡è¨Šéœ€è¦é¡å¤–è™•ç†
                        "row": None,
                        "confidence": entity.get("confidence", 0.9),
                        "start_pos": entity.get("start_pos"),
                        "end_pos": entity.get("end_pos"),
                        "reason": entity.get("reason", ""),
                    }
                )

            # ä¹Ÿæª¢æŸ¥ summary.phi_entities (å‚™ç”¨)
            if not phi_entities:
                summary_phi = result.summary.get("phi_entities", [])
                for entity in summary_phi:
                    phi_entities.append(
                        {
                            "type": entity.get("type", "UNKNOWN"),
                            "value": entity.get("text", ""),
                            "masked_value": "[MASKED]",
                            "field": None,
                            "row": None,
                            "confidence": entity.get("confidence", 0.9),
                        }
                    )

            # å–å¾—åŸå§‹å’Œé®ç½©å¾Œçš„å…§å®¹ (æ–°çµæ§‹ç›´æ¥æä¾›)
            original_content = doc_info.get("original_content", "")
            masked_content = doc_info.get("masked_content", "")
            output_path = doc_info.get("output_path", "")

            # è®€å–åŸå§‹å’Œè™•ç†å¾Œçš„è³‡æ–™ç”¨æ–¼ diff é¡¯ç¤º
            original_data = None
            masked_data = None
            import pandas as pd

            try:
                if meta["file_type"] == "csv":
                    original_df = pd.read_csv(file_path)
                    original_data = original_df.head(100).to_dict(orient="records")
                elif meta["file_type"] == "xlsx":
                    original_df = pd.read_excel(file_path)
                    original_data = original_df.head(100).to_dict(orient="records")

                # å„ªå…ˆä½¿ç”¨å¼•æ“è¿”å›çš„è¼¸å‡ºè·¯å¾‘
                if output_path and Path(output_path).exists():
                    out_path = Path(output_path)
                    if out_path.suffix == ".csv":
                        masked_df = pd.read_csv(out_path)
                    else:
                        masked_df = pd.read_excel(out_path)
                    masked_data = masked_df.head(100).to_dict(orient="records")
                    logger.info(f"ä½¿ç”¨å¼•æ“è¼¸å‡ºè·¯å¾‘: {output_path}")
                else:
                    # å‚™ç”¨ï¼šæŸ¥æ‰¾è™•ç†å¾Œçš„è¼¸å‡ºæª”æ¡ˆ
                    output_dir = Path("data/output/results")
                    if output_dir.exists():
                        original_stem = file_path.stem
                        for output_file in sorted(
                            output_dir.glob(f"*{original_stem}*"),
                            key=lambda x: x.stat().st_mtime,
                            reverse=True,
                        ):
                            if output_file.suffix in [".csv", ".xlsx"]:
                                if output_file.suffix == ".csv":
                                    masked_df = pd.read_csv(output_file)
                                else:
                                    masked_df = pd.read_excel(output_file)
                                masked_data = masked_df.head(100).to_dict(orient="records")
                                logger.info(f"æ‰¾åˆ°è™•ç†å¾Œæª”æ¡ˆ: {output_file}")
                                break
            except Exception as read_err:
                logger.warning(f"ç„¡æ³•è®€å–åŸå§‹/è™•ç†å¾Œè³‡æ–™: {read_err}")

            # å–å¾—æŒ‰é¡å‹çµ±è¨ˆ
            phi_by_type = result.summary.get("phi_by_type", {})

            results.append(
                {
                    "file_id": file_id,
                    "filename": meta["filename"],
                    "phi_found": phi_count,
                    "phi_by_type": phi_by_type,  # æ–°å¢ï¼šæŒ‰é¡å‹çµ±è¨ˆ
                    "rows_processed": result.processed_documents,
                    "status": "completed",
                    "phi_entities": phi_entities,
                    "original_data": original_data,
                    "masked_data": masked_data,
                    "original_content": original_content[:5000]
                    if original_content
                    else None,  # é™åˆ¶å¤§å°
                    "masked_content": masked_content[:5000] if masked_content else None,
                    "output_path": output_path,
                }
            )
            logger.info(
                f"Engine processed {meta['filename']}: found {phi_count} PHI, types: {phi_by_type}"
            )

            # æ›´æ–°å·²è™•ç†å­—ç¬¦æ•¸
            task["processed_chars"] = task.get("processed_chars", 0) + file_chars.get(file_id, 0)

            # è¨˜éŒ„å–®æª”è™•ç†æ™‚é–“
            file_elapsed = (datetime.now() - file_start_time).total_seconds()
            logger.info(f"â±ï¸ File {meta['filename']} processed in {file_elapsed:.2f}s")

        # å„²å­˜çµæœ
        result_id = task_id
        result_file = RESULTS_DIR / f"{result_id}_results.json"
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "task_id": task_id,
                    "job_name": task["job_name"],
                    "config": task["config"],
                    "results": results,
                    "processed_at": datetime.now().isoformat(),
                },
                f,
                indent=2,
                ensure_ascii=False,
            )

        # ç”¢ç”Ÿå ±å‘Š
        report_file = REPORTS_DIR / f"{result_id}_report.json"
        total_phi = sum(r.get("phi_found", 0) for r in results)

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_time = (datetime.now() - task["started_at"]).total_seconds()
        processing_speed = total_chars / total_time if total_time > 0 else 0

        report = {
            "task_id": task_id,
            "job_name": task["job_name"],
            "summary": {
                "files_processed": len(results),
                "total_phi_found": total_phi,
                "processing_time_seconds": round(total_time, 2),
                "total_chars": total_chars,
                "processing_speed_chars_per_sec": round(processing_speed, 2),
            },
            "file_details": results,
            "generated_at": datetime.now().isoformat(),
        }
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # æ›´æ–°è™•ç†é€Ÿåº¦çµ±è¨ˆ
        update_processing_stats(total_chars, total_time)

        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        task["status"] = "completed"
        task["progress"] = 100.0
        task["elapsed_seconds"] = total_time
        task["estimated_remaining_seconds"] = 0
        task["processing_speed"] = processing_speed
        task["message"] = (
            f"è™•ç†å®Œæˆï¼æ‰¾åˆ° {total_phi} å€‹ PHI (è€—æ™‚ {format_time(total_time)}, é€Ÿåº¦ {processing_speed:.1f} å­—å…ƒ/ç§’)"
        )
        task["completed_at"] = datetime.now()
        task["result_file"] = str(result_file.name)
        task["report_file"] = str(report_file.name)

        logger.info(f"âœ… Task completed: {task_id}, PHI found: {total_phi}")

    except Exception as e:
        task["status"] = "failed"
        task["message"] = f"è™•ç†å¤±æ•—: {e!s}"
        task["completed_at"] = datetime.now()
        logger.error(f"âŒ Task failed: {task_id}, error: {e}")


@app.get("/api/tasks", response_model=list[TaskStatus])
async def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»å‹™"""
    return [
        TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})
        for task in sorted(tasks_db.values(), key=lambda x: x["created_at"], reverse=True)
    ]


@app.get("/api/tasks/{task_id}", response_model=TaskStatus)
async def get_task(task_id: str):
    """å–å¾—ä»»å‹™ç‹€æ…‹"""
    if task_id not in tasks_db:
        raise HTTPException(404, "ä»»å‹™ä¸å­˜åœ¨")
    task = tasks_db[task_id]

    # å¦‚æœä»»å‹™æ­£åœ¨è™•ç†ä¸­ï¼Œå³æ™‚æ›´æ–°è¨ˆæ™‚è³‡è¨Š
    if task["status"] == "processing" and task.get("started_at"):
        elapsed = (datetime.now() - task["started_at"]).total_seconds()
        task["elapsed_seconds"] = elapsed

        # æ›´æ–°é ä¼°å‰©é¤˜æ™‚é–“
        total_chars = task.get("total_chars", 0)
        processed_chars = task.get("processed_chars", 0)
        if total_chars > 0 and processed_chars > 0 and elapsed > 0:
            current_speed = processed_chars / elapsed
            remaining_chars = total_chars - processed_chars
            task["estimated_remaining_seconds"] = remaining_chars / current_speed
            task["processing_speed"] = current_speed

    return TaskStatus(**{k: v for k, v in task.items() if k in TaskStatus.model_fields})


@app.get("/api/stats/processing")
async def get_processing_stats():
    """å–å¾—è™•ç†é€Ÿåº¦çµ±è¨ˆ"""
    return {
        "avg_chars_per_second": processing_stats["avg_chars_per_second"],
        "total_chars_processed": processing_stats["total_chars_processed"],
        "total_time_seconds": processing_stats["total_time_seconds"],
        "task_count": processing_stats["task_count"],
    }


# ============================================================
# Results & Reports APIs
# ============================================================


@app.get("/api/results")
async def list_results():
    """åˆ—å‡ºæ‰€æœ‰è™•ç†çµæœ"""
    results = []
    for result_file in RESULTS_DIR.glob("*_results.json"):
        try:
            with open(result_file, encoding="utf-8") as f:
                data = json.load(f)
                task_id = result_file.stem.replace("_results", "")

                # è¨ˆç®—ç¸½ PHI æ•¸é‡
                phi_count = 0
                file_results = data.get("results", [])
                for fr in file_results:
                    phi_count += fr.get("phi_found", 0)

                # å–å¾—æª”æ¡ˆåç¨±
                filenames = [fr.get("filename", "Unknown") for fr in file_results]

                results.append(
                    {
                        "task_id": task_id,
                        "filename": ", ".join(filenames) if filenames else "Unknown",
                        "phi_count": phi_count,
                        "files_processed": len(file_results),
                        "status": "completed",
                        "created_at": data.get("processed_at"),
                    }
                )
        except Exception as e:
            logger.warning(f"Failed to read result file {result_file}: {e}")

    # æŒ‰æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    results.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return results


@app.get("/api/results/{task_id}")
async def get_results(task_id: str):
    """å–å¾—è™•ç†çµæœ"""
    result_file = RESULTS_DIR / f"{task_id}_results.json"
    if not result_file.exists():
        raise HTTPException(404, "çµæœä¸å­˜åœ¨")

    with open(result_file, encoding="utf-8") as f:
        return json.load(f)


@app.get("/api/reports/{task_id}")
async def get_report(task_id: str):
    """å–å¾—å ±å‘Š"""
    report_file = REPORTS_DIR / f"{task_id}_report.json"
    if not report_file.exists():
        raise HTTPException(404, "å ±å‘Šä¸å­˜åœ¨")

    with open(report_file, encoding="utf-8") as f:
        return json.load(f)


@app.get("/api/reports")
async def list_reports():
    """åˆ—å‡ºæ‰€æœ‰å ±å‘Š"""
    reports = []
    for report_file in REPORTS_DIR.glob("*_report.json"):
        with open(report_file, encoding="utf-8") as f:
            report = json.load(f)
            task_id = report["task_id"]
            reports.append(
                {
                    "id": task_id,  # å‰ç«¯éœ€è¦ id æ¬„ä½
                    "task_id": task_id,
                    "filename": report.get("filename", report_file.name),  # åŠ å…¥ filename
                    "job_name": report.get("job_name", ""),
                    "files_processed": report["summary"]["files_processed"],
                    "total_phi_found": report["summary"]["total_phi_found"],
                    "created_at": report["generated_at"],  # å‰ç«¯éœ€è¦ created_at
                    "generated_at": report["generated_at"],
                }
            )
    return sorted(reports, key=lambda x: x["generated_at"], reverse=True)


# ============================================================
# Settings & Regulations APIs
# ============================================================


@app.get("/api/settings/phi-types")
async def get_phi_types():
    """å–å¾—å¯ç”¨çš„ PHI é¡å‹"""
    return {
        "phi_types": [
            {
                "id": "NAME",
                "name": "å§“å",
                "description": "æ‚£è€…æˆ–ç›¸é—œäººå“¡å§“å",
                "category": "identifier",
            },
            {
                "id": "DATE",
                "name": "æ—¥æœŸ",
                "description": "å‡ºç”Ÿæ—¥æœŸã€å°±è¨ºæ—¥æœŸç­‰",
                "category": "temporal",
            },
            {"id": "PHONE", "name": "é›»è©±", "description": "é›»è©±è™Ÿç¢¼", "category": "contact"},
            {
                "id": "EMAIL",
                "name": "é›»å­éƒµä»¶",
                "description": "é›»å­éƒµä»¶åœ°å€",
                "category": "contact",
            },
            {
                "id": "ADDRESS",
                "name": "åœ°å€",
                "description": "ä½å€ã€å·¥ä½œåœ°å€ç­‰",
                "category": "geographic",
            },
            {
                "id": "ID_NUMBER",
                "name": "èº«ä»½è­‰è™Ÿ",
                "description": "èº«åˆ†è­‰å­—è™Ÿã€è­·ç…§è™Ÿç¢¼",
                "category": "identifier",
            },
            {
                "id": "MEDICAL_RECORD",
                "name": "ç—…æ­·è™Ÿ",
                "description": "ç—…æ­·è™Ÿç¢¼",
                "category": "identifier",
            },
            {
                "id": "SOCIAL_SECURITY",
                "name": "ç¤¾æœƒå®‰å…¨ç¢¼",
                "description": "å¥ä¿å¡è™Ÿç­‰",
                "category": "identifier",
            },
            {
                "id": "ACCOUNT_NUMBER",
                "name": "å¸³è™Ÿ",
                "description": "éŠ€è¡Œå¸³è™Ÿç­‰",
                "category": "financial",
            },
            {
                "id": "LICENSE_NUMBER",
                "name": "åŸ·ç…§è™Ÿç¢¼",
                "description": "é§•ç…§ã€è­‰ç…§è™Ÿç¢¼",
                "category": "identifier",
            },
            {
                "id": "VEHICLE_ID",
                "name": "è»Šè¼›è­˜åˆ¥",
                "description": "è»Šç‰Œè™Ÿç¢¼",
                "category": "identifier",
            },
            {
                "id": "DEVICE_ID",
                "name": "è¨­å‚™è­˜åˆ¥",
                "description": "è¨­å‚™åºè™Ÿã€IP ä½å€",
                "category": "technical",
            },
            {"id": "URL", "name": "ç¶²å€", "description": "ç¶²ç«™ç¶²å€", "category": "technical"},
            {
                "id": "BIOMETRIC",
                "name": "ç”Ÿç‰©è­˜åˆ¥",
                "description": "æŒ‡ç´‹ã€è²ç´‹ç­‰",
                "category": "biometric",
            },
            {"id": "PHOTO", "name": "å½±åƒ", "description": "å…¨è‡‰ç…§ç‰‡ç­‰", "category": "biometric"},
            {
                "id": "AGE",
                "name": "å¹´é½¡",
                "description": "è¶…é 89 æ­²çš„å¹´é½¡",
                "category": "demographic",
            },
        ],
        "masking_types": [
            {"id": "redact", "name": "é®è”½", "description": "ä»¥ [REDACTED] å–ä»£"},
            {"id": "hash", "name": "é›œæ¹Š", "description": "ä»¥é›œæ¹Šå€¼å–ä»£ï¼Œå¯è¿½è¹¤åŒä¸€äºº"},
            {"id": "pseudonymize", "name": "å‡ååŒ–", "description": "ä»¥å‡åå–ä»£"},
            {"id": "generalize", "name": "æ³›åŒ–", "description": "ä»¥æ›´å»£æ³›çš„é¡åˆ¥å–ä»£"},
        ],
    }


@app.get("/api/settings/config")
async def get_config():
    """å–å¾—ç›®å‰çš„è™•ç†è¨­å®š"""
    config_file = DATA_DIR / "config.json"
    if config_file.exists():
        with open(config_file, encoding="utf-8") as f:
            return json.load(f)
    return PHIConfig().model_dump()


@app.put("/api/settings/config")
async def update_config(config: PHIConfig):
    """æ›´æ–°è™•ç†è¨­å®š"""
    config_file = DATA_DIR / "config.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config.model_dump(), f, indent=2, ensure_ascii=False)
    logger.info(f"âš™ï¸ Config updated: {config.masking_type}")
    return {"message": "è¨­å®šå·²æ›´æ–°", "config": config.model_dump()}


class RegulationContent(BaseModel):
    """æ³•è¦å®Œæ•´å…§å®¹"""

    id: str
    name: str
    content: str
    source_file: str | None = None


@app.get("/api/regulations/{rule_id}/content")
async def get_regulation_content(rule_id: str):
    """å–å¾—æ³•è¦çš„å®Œæ•´å…§å®¹"""
    # æ³•è¦ä¾†æºæª”æ¡ˆå°ç…§
    source_files = {
        "hipaa-safe-harbor": "hipaa_safe_harbor.md",
        "hipaa-phi": "hipaa_phi_definition.md",
        "taiwan-pdpa": "taiwan_pdpa.md",
    }

    # å…ˆæ‰¾å°ˆæ¡ˆæ ¹ç›®éŒ„çš„ regulations
    project_root = Path(__file__).parent.parent.parent
    regulations_source = project_root / "regulations" / "source_documents"

    source_file = source_files.get(rule_id)
    if source_file:
        file_path = regulations_source / source_file
        if file_path.exists():
            with open(file_path, encoding="utf-8") as f:
                content = f.read()
            return {
                "id": rule_id,
                "name": rule_id.replace("-", " ").title(),
                "content": content,
                "source_file": source_file,
            }

    # æ‰¾è‡ªè¨‚æ³•è¦
    custom_rules_file = REGULATIONS_DIR / "custom_rules.json"
    if custom_rules_file.exists():
        with open(custom_rules_file, encoding="utf-8") as f:
            custom_rules = json.load(f)
            for rule in custom_rules:
                if rule.get("id") == rule_id and rule.get("content"):
                    return {
                        "id": rule_id,
                        "name": rule.get("name", rule_id),
                        "content": rule.get("content", ""),
                        "source_file": None,
                    }

    raise HTTPException(404, f"æ‰¾ä¸åˆ°æ³•è¦å…§å®¹: {rule_id}")


@app.get("/api/regulations", response_model=list[RegulationRule])
async def list_regulations():
    """åˆ—å‡ºæ‰€æœ‰æ³•è¦è¦å‰‡"""
    # é è¨­è¦å‰‡
    default_rules = [
        {
            "id": "hipaa-safe-harbor",
            "name": "HIPAA Safe Harbor",
            "description": "ç¾åœ‹ HIPAA æ³•è¦çš„ 18 é …è­˜åˆ¥è³‡è¨Š",
            "phi_types": [
                "NAME",
                "DATE",
                "PHONE",
                "EMAIL",
                "ADDRESS",
                "ID_NUMBER",
                "MEDICAL_RECORD",
                "SOCIAL_SECURITY",
                "ACCOUNT_NUMBER",
            ],
            "source": "hipaa",
            "enabled": True,
        },
        {
            "id": "taiwan-pdpa",
            "name": "å°ç£å€‹è³‡æ³•",
            "description": "å°ç£å€‹äººè³‡æ–™ä¿è­·æ³•å®šç¾©çš„å€‹äººè³‡æ–™",
            "phi_types": ["NAME", "DATE", "PHONE", "EMAIL", "ADDRESS", "ID_NUMBER"],
            "source": "taiwan_pdpa",
            "enabled": True,
        },
    ]

    # è¼‰å…¥è‡ªè¨‚è¦å‰‡
    custom_rules_file = REGULATIONS_DIR / "custom_rules.json"
    if custom_rules_file.exists():
        with open(custom_rules_file, encoding="utf-8") as f:
            custom_rules = json.load(f)
            default_rules.extend(custom_rules)

    return [RegulationRule(**r) for r in default_rules]


@app.post("/api/regulations/upload")
async def upload_regulation(file: UploadFile = File(...)):
    """ä¸Šå‚³è‡ªè¨‚æ³•è¦æª”æ¡ˆ"""
    if not file.filename.endswith(".json"):
        raise HTTPException(400, "åƒ…æ”¯æ´ JSON æ ¼å¼")

    content = await file.read()
    try:
        rules = json.loads(content)

        # é©—è­‰æ ¼å¼
        if not isinstance(rules, list):
            rules = [rules]

        for rule in rules:
            required_fields = ["id", "name", "description", "phi_types"]
            for field in required_fields:
                if field not in rule:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
            rule["source"] = "custom"
            rule["enabled"] = rule.get("enabled", True)

        # å„²å­˜
        custom_rules_file = REGULATIONS_DIR / "custom_rules.json"
        existing_rules = []
        if custom_rules_file.exists():
            with open(custom_rules_file, encoding="utf-8") as f:
                existing_rules = json.load(f)

        existing_rules.extend(rules)

        with open(custom_rules_file, "w", encoding="utf-8") as f:
            json.dump(existing_rules, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“œ Uploaded {len(rules)} regulation rules")
        return {"message": f"å·²ä¸Šå‚³ {len(rules)} æ¢è¦å‰‡", "rules": rules}

    except json.JSONDecodeError:
        raise HTTPException(400, "ç„¡æ•ˆçš„ JSON æ ¼å¼")
    except ValueError as e:
        raise HTTPException(400, str(e))


@app.put("/api/regulations/{rule_id}")
async def update_regulation(rule_id: str, enabled: bool):
    """å•Ÿç”¨/åœç”¨æ³•è¦è¦å‰‡"""
    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²æ›´æ–°è³‡æ–™åº«
    logger.info(f"ğŸ“œ Rule {rule_id} {'enabled' if enabled else 'disabled'}")
    return {"message": "è¦å‰‡å·²æ›´æ–°", "rule_id": rule_id, "enabled": enabled}


# ============================================================
# Health Check
# ============================================================


@app.get("/api/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ï¼ŒåŒ…å« LLM ç‹€æ…‹"""
    import subprocess

    # æª¢æŸ¥ Ollama LLM ç‹€æ…‹ (æ”¯æ´é ç«¯ API)
    llm_status = "offline"
    llm_model = None
    ollama_url = OLLAMA_BASE_URL.rstrip("/")
    try:
        result = subprocess.run(
            ["curl", "-s", f"{ollama_url}/api/tags"],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            import json as json_lib

            data = json_lib.loads(result.stdout)
            models = [m.get("name") for m in data.get("models", [])]
            if models:
                llm_status = "online"
                llm_model = models[0] if len(models) == 1 else f"{len(models)} models"
    except Exception:
        pass

    # æª¢æŸ¥ PHI Engine æ˜¯å¦å¯ç”¨
    engine_available = False
    try:
        from core.application.processing.engine import DeidentificationEngine

        engine_available = True
    except ImportError:
        pass

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "llm": {
            "status": llm_status,
            "model": llm_model,
            "provider": "ollama",
            "endpoint": ollama_url,
        },
        "engine_available": engine_available,
    }


# ============================================================
# Run Server
# ============================================================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )
